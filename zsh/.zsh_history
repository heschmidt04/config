: 1463669081:0;ls -l shipit_layer
: 1463669104:0;rm main.tf
: 1463669145:0;git rm shipit_stack_uniform/shipit_auto_scale_all_in_one.tf
: 1463669837:0;cp sampleapp.tf sampleapp.tf.orig
: 1463671263:0;cat shipit_layer.tf | grep avail
: 1463671336:0;more ../../staging/variables.tf
: 1463671359:0;cat shipit_layer.tf | grep vpc
: 1463671370:0;cat shipit_layer.tf | grep image
: 1463671453:0;cat shipit_layer.tf | grep instance
: 1463671514:0;vi hurby-server.tf
: 1463671658:0;grep ssl_cert variables.tf
: 1463671998:0;cat shipit_layer.tf
: 1463672094:0;grep ssl_certificate *.tf
: 1463672283:0;grep var.vpc_id *.tf
: 1463672291:0;grep vpc_id *.tf
: 1463672334:0;grep environment_name *.tf
: 1463672454:0;grep layer_name *.tf
: 1463672553:0;cd shipit_
: 1463672555:0;cd shipit_layer
: 1463672597:0;grep private_subnet *.tf
: 1463672673:0;grep -i private_subnet */*
: 1463672854:0;vi vpc.tf
: 1463672873:0;grep vpc-network.tf
: 1463672881:0;vi vpc-network.tf
: 1463672981:0;grep nat_clients *.tf
: 1463673031:0;grep nat_clients
: 1463673035:0;grep nat_clients *
: 1463673100:0;grep layer_auto_scale *.tf
: 1463681337:0;terraform plan -out=test.plan --target=aws_elb.sampleapp
: 1463681386:0;terraform plan -out=test.plan --target=module.product-combo-module.aws_launch_configuration.layer_launch_configuration
: 1463681923:0;vi wbt.tf
: 1463682232:0;terraform plan -out=test.plan --target=module.product-combo-module
: 1463682628:0;grep myh_bootstrap *.tf
: 1463682634:0;grep bootstrap *.tf
: 1463682641:0;vi hello200.tf
: 1463684275:0;grep chef_recip shipit_layer.tf
: 1463684365:0;vi shipit_layer.tf
: 1463686073:0;terraform taint --target=module.product-jobs-module.aws_launch_configuration.layer_launch_configuration
: 1463686088:0;terraform taint --target=module.product-jobs-module
: 1463686101:0;terraform help 
: 1463686109:0;terraform taint help
: 1463686118:0;terraform help taint
: 1463686146:0;more sampleapp.tf
: 1463686171:0;terraform taint module.product-jobs-module
: 1463686759:0;mkdir shipit_elb_layer
: 1463686791:0;mv shipit_layer/shipit_layer.tf shipit_elb_layer/shipit_elb_layer.tf
: 1463686816:0;cp shipit_elb_layer/shipit_elb_layer.tf shipit_layer/shipit_no_elb_layer.tf
: 1463686830:0;mv shipit_layer shipit_no_elb_layer
: 1463686893:0;cd shipit_no_elb_layer
: 1463687070:0;terraform plan -out=test.plan --target=module.product-jobs-module.template_file.layer_userdata
: 1463687094:0;terraform plan -out=test.plan --target=module.product-jobs-module.aws_launch_configuration.layer_launch_configuration
: 1463687246:0;git add sampleapp.tf 
: 1463687258:0;git diff variables.tf
: 1463687281:0;git diff hurby-server.tf
: 1463687305:0;git checkout hurby-server.tf
: 1463687327:0;git add ../modules/shipit_elb_layer/
: 1463687351:0;ls -l ../shipit_elb_layer
: 1463687381:0;git add ../modules/shipit_no_elb_layer/
: 1463687394:0;git add ../modules/shipit_no_elb_layer/shipit_no_elb_layer.tf
: 1463688009:0;vi shipit_no_elb_layer.tf
: 1463688164:0;grep deployment shipit_no_elb_layer.tf
: 1463688177:0;grep deploy variables.tf
: 1463749778:0;grep deployment variables.tf
: 1463751374:0;git diff ../modules/shipit_no_elb_layer/shipit_no_elb_layer.tf
: 1463751388:0;git diff ../modules/shipit_layer/main.tf
: 1463751397:0;cd ../modules/shipit_layer/
: 1463751404:0;git diff main.tf
: 1463751429:0;git diff terraform/modules/shipit_no_elb_layer/shipit_no_elb_layer.tf
: 1463751441:0;git diff terraform/staging/sampleapp.tf
: 1463751453:0;git diff terraform/staging/variables.tf
: 1463751466:0;git diff terraform/modules/shipit_layer/main.tf
: 1463751492:0;git add terraform/modules/shipit_no_elb_layer/shipit_no_elb_layer.tf
: 1463751497:0;git add terraform/staging/sampleapp.tf
: 1463751503:0;git add terraform/staging/variables.tf
: 1463754241:0;find .tpl
: 1463754245:0;which .tpl
: 1463754263:0;more ./terraform/production/cloud-init/shipit-instance.tpl
: 1463754274:0;find . -name "*.tpl" -print
: 1463754289:0;diff -u ./terraform/production/cloud-init/shipit-instance.tpl ./terraform/staging/cloud-init/shipit-instance.tpl
: 1463756036:0;terraform plan -out=test.plan --target=module.product-jobs-module.aws_autoscaling_group.layer_auto_scale
: 1463767498:0;cd terraform/modules
: 1463767528:0;diff -u shipit_elb_layer/shipit_elb_layer.tf shipit_no_elb_layer/shipit_no_elb_layer.tf
: 1463767575:0;diff -u shipit_elb_layer/shipit_elb_layer.tf shipit_no_elb_layer/shipit_no_elb_layer.tf | grep "^+"
: 1463767590:0;vi shipit_elb_layer/shipit_elb_layer.tf
: 1463767665:0;vi sampleapp.tf
: 1463768828:0;terraform plan -out=test.plan --target=module.product-combo-module.aws_autoscaling_group.layer_auto_scale
: 1463769624:0;git add ../modules/shipit_elb_layer/shipit_elb_layer.tf
: 1463769635:0;git add sampleapp.tf
: 1463769698:0;history | grep mysql | grep 127
: 1463769749:0;mysql -u meyouhealth -p -h 127.0.0.1 -P 9997 
: 1463769758:0;ssh -A -L 9997:127.0.0.1:9997 ubuntu@52.86.252.208 ssh -L 9997:staging-shared-mysql.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N ubuntu@10.145.46.88
: 1464012076:0;ls -l shipit*
: 1464012091:0;more shipit-instance.tpl
: 1464012101:0;more shipit_elb_layer.tf
: 1464012157:0;ls -ltra shipit_elb_layer
: 1464013361:0;cd terraform
: 1464013386:0;cp wilder.tf ~/Documents/git/myh-terraform/environments/wilder/wilder.tf
: 1464013403:0;ls -l hello2
: 1464013413:0;ls -l staging
: 1464013419:0;mv wilder staging/
: 1464013427:0;more hello200
: 1464013430:0;ls -l hello200
: 1464013476:0;touch wilder/ec2.tf 
: 1464013485:0;touch wilder/elasticache.tf
: 1464013492:0;touch wilder/iam.tf 
: 1464013500:0;touch wilder/provider.tf
: 1464013508:0;touch wilder/rds.tf
: 1464013524:0;touch wilder/shipit.tf
: 1464013540:0;cp hello200/variables.tf wilder/variables.tf
: 1464013592:0;grep wilder variables.tf
: 1464013600:0;grep wilder terraform.tfvars
: 1464013645:0;cp terraform.tfvars ~/Documents/git/myh-terraform/environments/staging/wilder/terraform.tfvars
: 1464013664:0;vi wilder.tf 
: 1464013689:0;cd ~/Documents/git/myh-terraform/environments/staging/hello200
: 1464013874:0;grep rds *.tf
: 1464013918:0;grep module *.tf
: 1464013982:0;ls -l ~/Documents/git/myh-terraform/
: 1464013985:0;ls -l ~/Documents/git/myh-terraform/modules
: 1464013998:0;ls -l ~/Documents/git/myh-terraform/modules/shipit_no_elb_layer
: 1464014083:0;ls -ltra /Users/heidischmidt/Documents/git/myh-terraform/environments/staging/modules/shipit_no_elb_layer
: 1464014089:0;ls -ltra /Users/heidischmidt/Documents/git/myh-terraform/environments/
: 1464014093:0;ls -ltra /Users/heidischmidt/Documents/git/myh-terraform/
: 1464014099:0;ls -ltra /Users/heidischmidt/Documents/git/myh-terraform/modules
: 1464014106:0;ls -ltra /Users/heidischmidt/Documents/git/myh-terraform/modules/shipit_no_elb_layer
: 1464014616:0;cp provider.tf ../wilder/provider.tf
: 1464014959:0;cp terraform.tfstate ~/Documents/git/myh-terraform/environments/staging/wilder/terraform.tfstate
: 1464015735:0;cp terraform.tfstate ~/Documents/git/myh-terraform/environments/staging/wilder/terraform.tfstate.ref
: 1464017207:0;mv terraform.tfstate terraform.tfstate.newbie
: 1464017220:0;cp terraform.tfstate.ref terraform.tfstate
: 1464017267:0;more terraform.tfstate.newbie 
: 1464017315:0;more terraform.tfstate
: 1464017384:0;cd staging/wilder
: 1464018617:0;cp terraform.tfstate
: 1464018646:0;cp terraform.tfstate terraform.tfstate.working
: 1464022861:0;terraform plan -out=test.plan --target=module.wilder-staging-in-one.aws_instance.web_instance
: 1464022936:0;terraform plan -out=test.plan --target=module.wilder-staging-web.aws_autoscaling_group.layer_auto_scale
: 1464023215:0;terraform plan -out=test.plan --target=module.wilder-staging-in-one.aws_launch_configuration.layer_launch_configuration
: 1464023953:0;shipit ssh hello200-staging
: 1464023990:0;vi .ssh/config
: 1464024109:0;terraform plan -out=test.plan --target=module.wilder-staging-in-one.aws_autoscaling_group.layer_auto_scale
: 1464024549:0;terraform plan -out=test.plan --target=module.wilder-staging-in-one.aws_instance.all_in_one_instance
: 1464024598:0;history | grep --console-host
: 1464024625:0;shipit environment update wilder-staging --console-host=10.0.17.16
: 1464024711:0;terraform plan -out=test.plan --target=module.wilder-staging-in-one.aws_elb.web
: 1464024943:0;knife search node "chef_environment:wilder-staging"
: 1464025004:0;knife search node "chef_environment:wilder-staging" > ~/Documents/wilder/knife-wilder-staging-b4-cleanup.txt
: 1464027992:0;vi ~/Documents/wilder/knife-wilder-staging-b4-cleanup.txt
: 1464028227:0;cat ~/Documents/wilder/knife-wilder-staging-b4-cleanup.txt
: 1464028253:0;knife client delete -y i-120d7295
: 1464028261:0;knife node delete -y i-120d7295
: 1464028299:0;knife client delete -y i-420d72c5
: 1464028314:0;knife node delete -y i-420d72c5
: 1464028327:0;knife search node "chef_environment:wilder-staging" > ~/Documents/wilder/knife-wilder-staging-after-cleanup.txt
: 1464028418:0;more elb.tf
: 1464028456:0;cd terraform/staging
: 1464028498:0;rm augh
: 1464028630:0;cd ../../cd ../
: 1464028645:0;more shipit_elb_layer/shipit_elb_layer.tf
: 1464028657:0;more shipit_elb_layer/shipit_elb_layer.tf | grep elb
: 1464028723:0;ls -l quitnet
: 1464028772:0;cd shipit_stack_all_in_one
: 1464028777:0;vi main.tf
: 1464028801:0;tail -50 main.tf
: 1464028925:0;grep split *.tf
: 1464029060:0;grep elb *.tf
: 1464029122:0;find . -name "*.tf" -print -exec grep -i aws_elb {} \;
: 1464029253:0;grep "[" */*.tf
: 1464029270:0;find . -name "*.tf" -print -exec grep -i "["  {} \;
: 1464029289:0;grep [ *.tf
: 1464029345:0;grep "\[" *.tf
: 1464029499:0;cd modules
: 1464029505:0;cd shipit_elb_layer
: 1464029513:0;vi shipit_elb_layer.tf
: 1464029751:0;cd environments/staging
: 1464030390:0;grep split *
: 1464030404:0;vi note-for-subnets-needs-splitting.txt
: 1464030427:0;mv "\~" note-for-subnets-needs-splitting-in-terraform.txt
: 1464030439:0;cat "\~"
: 1464030442:0;more ~
: 1464030468:0;vi note-for-subnets-needs-splitting-in-terraform.txt
: 1464030553:0;rm -i "*" 
: 1464030567:0;rm -i * 
: 1464030589:0;cp note-for-subnets-needs-splitting-in-terraform.txt ~/Documents/wilder
: 1464030615:0;rm "\~\"
: 1464030618:0;rm "\~"
: 1464030642:0;rm terraform.tfstate.working
: 1464030647:0;rm terraform.tfstate.ref
: 1464030654:0;more *new*
: 1464030663:0;rm terraform.tfstate.newbie
: 1464030838:0;shipit deployment create wilder-staging abd5636bb8b6f1769fae283d5ded2929b22e48c1
: 1464031175:0;shipit deployment logs 3329
: 1464031266:0;shipit deployment logs 3329 | more
: 1464031289:0;grep migrations *.tf
: 1464031346:0;mkdir keep/
: 1464031351:0;mv *.tf keep/
: 1464031361:0;mv note-for-subnets-needs-splitting-in-terraform.txt keep/
: 1464031371:0;mv terraform.tfvars keep/
: 1464031387:0;mv terraform.tfstate* keep/
: 1464031400:0;rm *
: 1464031413:0;cd keep 
: 1464031417:0;mv * ../
: 1464031425:0;rm -rf keep
: 1464031456:0;git add terraform.tfvars
: 1464031584:0;git push 
: 1464031622:0;vim .git
: 1464031627:0;vim .git/config
: 1464031666:0;cd .git
: 1464031921:0;rm sampleapp.tf.orig
: 1464031943:0;ls *.tf
: 1464032012:0;git rm mysql-db-template.tf
: 1464032020:0;more mysql-db-template.tf
: 1464032076:0;git checkout ../modules/shipit_layer/main.tf
: 1464032091:0;rm ../production/test.plan
: 1464032948:0;grep desired *
: 1464033035:0;find . -name "shipit.tf" -print -exec grep "module" {} \;
: 1464100990:0;more insight.tf
: 1464101004:0;mkdir insight
: 1464101012:0;ls -lrta *
: 1464101042:0;cp concierge/*.tf insight/
: 1464101072:0;cp terraform.tfvars /Users/heidischmidt/Documents/git/myh-terraform/environments/production/insight/
: 1464101106:0;more variables.tf| grep insight
: 1464101788:0;ssh -A -L 9990:127.0.0.1:9990 ubuntu@54.152.155.24 ssh -L 9990:insight-production.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:5432 -N ubuntu@10.0.5.227
: 1464101840:0;sql -h 127.0.0.1 -p 9990 meyouhealth --dbname=insight_production 
: 1464101963:0;grep insight *
: 1464101972:0;ls -l insig*
: 1464102050:0;psql -h 127.0.0.1 -p 9990 meyouhealth --dbname=insight_production 
: 1464102188:0;psql -h 127.0.0.1 -p 9990 reporting_production_app --dbname=insight_production 
: 1464103220:0;vi insight.tf
: 1464103236:0;grep s3 *.tf
: 1464103365:0;ls -l ../staging/*/
: 1464103369:0;ls -l ../staging/*/*.tf
: 1464103382:0;ls -l ../../staging/
: 1464103386:0;ls -l ../../staging/*/
: 1464103421:0;find  ../../staging/*/ -name "*.tf" -print -exec grep -i bucket {} \;
: 1464103435:0;vi ~/Documents/git/chef-repo/terraform/production/insight.tf
: 1464103774:0;cp ~/Documents/git/chef-repo/terraform/production/insight.tf ~/Documents/git/myh-terraform/environments/production/insight/insight.tf
: 1464103788:0;vi iam.tf 
: 1464107755:0;git checkout insight.tf
: 1464108883:0;cp terraform.tfstate terraform.tfstate.insight
: 1464108889:0;vi terraform.tfstate.insight
: 1464109546:0;cp ~/Documents/git/chef-repo/terraform/production/terraform.tfstate.insight ~/Documents/git/myh-terraform/environments/production/insight/terraform.tfstate.insight
: 1464109578:0;head -50 terraform.tfstate.insight
: 1464111251:0;vi terraform.tfstate 
: 1464111444:0;grep aws_elb.insight_web *.tf
: 1464111645:0;cat shipit.tf
: 1464111770:0;cd ../../production/insight
: 1464112046:0;grep aws_elb *.tf
: 1464112364:0;terraform plan -out=test.plan --target=aws_db_instance.insight_production
: 1464112463:0;terraform plan -out=test.plan --target=module.insight-production-web.template_file.layer_userdata
: 1464112493:0;terraform plan -out=test.plan --target=module.insight-production-web.aws_launch_configuration.layer_launch_configuration
: 1464112582:0;terraform plan -out=test.plan --target=module.insight-production-web.aws_autoscaling_group.layer_auto_scale
: 1464113739:0;terraform plan -out=test.plan --target=module.insight-production-all-in-one.template_file.layer_userdata
: 1464113761:0;terraform plan -out=test.plan --target=module.insight-production-all-in-one.aws_launch_configuration.layer_launch_configuration
: 1464113813:0;terraform plan -out=test.plan --target=module.insight-production-all-in-one.aws_autoscaling_group.layer_auto_scale
: 1464113933:0;terraform plan -out=test.plan --target=aws_instance.all_in_one_instance
: 1464115051:0;history | grep knife | tail -10
: 1464115093:0;knife search node "chef_environment:insight-production" > ~/Documents/knife-insight-production-b4-cleanup.txt5
: 1464115105:0;mv ~/Documents/knife-insight-production-b4-cleanup.txt5 ~/Documents/knife-insight-production-b4-cleanup.txt
: 1464115108:0;more ~/Documents/knife-insight-production-b4-cleanup.txt5
: 1464115110:0;more ~/Documents/knife-insight-production-b4-cleanup.txt
: 1464115135:0;knife client delete -y i-77d3f4f0
: 1464115143:0;knife node delete -y i-77d3f4f0
: 1464115160:0;knife client delete -y i-98ae7f02
: 1464115168:0;knife node delete -y i-98ae7f02
: 1464115181:0;knife search node "chef_environment:insight-production" > ~/Documents/knife-insight-production-after-cleanup.txt
: 1464115280:0;more ~/Documents/knife-insight-production-after-cleanup.txt
: 1464115285:0;shipit deployment create insight-production 38bfa3b1f5e5c3b7d2d9cc6c10f02d4bc71d55d0
: 1464115297:0;shipit env show insight-production
: 1464115437:0;shipit environment update insight-production --console-host=10.0.14.195
: 1464115465:0;shipit ssh insight-production
: 1464115504:0;shipit deployment logs 3337
: 1464115593:0;rm *wq*
: 1464115628:0;git add insight/
: 1464115960:0;cd chef-repo/terraform
: 1464115973:0;rm terraform.tfstate.insight
: 1464115980:0;rm "*@*"
: 1464115991:0;rm \@\
: 1464115993:0;ls -ltra
: 1464115999:0;rm test.plan 
: 1464186219:0;mkdir wilder
: 1464186238:0;history | grep cp | grep myh-terraform
: 1464186255:0;cp wilder.tf ~/Documents/git/myh-terraform/environments/production/wilder.tf
: 1464186271:0;cp terraform.tfstate ~/Documents/git/myh-terraform/environments/production/terraform.tfstate.wilder
: 1464186283:0;cp terraform.tfvars ~/Documents/git/myh-terraform/environments/production/terraform.tfvars
: 1464186292:0;cp variables.tf ~/Documents/git/myh-terraform/environments/production/variables.tf
: 1464186497:0;mv *.tf* wilder/
: 1464186510:0;ls -l ../insight
: 1464186544:0;touch ec2.tf elasticache.tf iam.tf insight.tf 
: 1464186557:0;cp ../insight/provider.tf .
: 1464186569:0;touch rds.tf shipit.tf
: 1464186621:0;rm insight.tf
: 1464186631:0;git rm insight.tf
: 1464186640:0;cd ../wilder
: 1464186648:0;cat wilder.tf >> ec2.tf
: 1464186654:0;more ../insight/ec2.tf
: 1464186809:0;cat wilder.tf >> iam.tf
: 1464186895:0;cp ../insight/rds.tf rds.tf
: 1464186967:0;cat wilder.tf
: 1464186999:0;cat wilder.tf >> elasticache.tf
: 1464187046:0;ls -l dailychallenge
: 1464187051:0;more dailychallenge/rds.tf
: 1464187068:0;cd wilder/`
: 1464187069:0;cd wilder/
: 1464188061:0;cp ../insight/shipit.tf shipit.tf
: 1464188091:0;more wilder.tf
: 1464188304:0;vi wilder.tf
: 1464188321:0;rm wilder.tf
: 1464188341:0;vi iam.tf
: 1464188363:0;grep minimal *.tf
: 1464188485:0;grep public_subnets *.tf
: 1464188524:0;grep subnets variables.tf
: 1464188539:0;grep subnets *tf
: 1464188559:0;more variables.tf 
: 1464188576:0;find . -name variables.tf -print
: 1464188583:0;more ./concierge/variables.tf
: 1464188754:0;grep wellbeingtracker *.tf
: 1464189023:0;vi terraform.tfstate.wilder
: 1464189656:0;ls -0ltra
: 1464189707:0;grep all-in-one *.tf
: 1464191539:0;terrraform apply test.plan 
: 1464191546:0;;tfap
: 1464191620:0;grep t2.small
: 1464191624:0;grep t2.small *
: 1464191686:0;terraform plan -out=test.plan --target=module.wilder-production-web.aws_launch_configuration.layer_launch_configuration
: 1464191728:0;terraform plan -out=test.plan --target=module.wilder-production-all-in-one.aws_launch_configuration.layer_launch_configuration
: 1464191756:0;terraform plan -out=test.plan --target=module.wilder-production-web.aws_autoscaling_group.layer_auto_scale
: 1464191778:0;terraform apply test.plan --target=module.wilder-production-all-in-one.aws_autoscaling_group.layer_auto_scale
: 1464191791:0;terraform plan -out=test.plan --target=module.wilder-production-all-in-one.aws_autoscaling_group.layer_auto_scale
: 1464196748:0;terraform plan -out=test.plan --target=aws_instance.web_instance
: 1464197707:0;history | grep knife | tail -5
: 1464197973:0;knife search node "chef_environment:wilder-production" > ~/Documents/knife-wilder-production-b4-cleanup.txt
: 1464197988:0;knife search node "chef_environment:wilder-production" > ~/Documents/wilder/knife-wilder-production-b4-cleanup.txt
: 1464198007:0;knife client delete -y i-2d93f9aa
: 1464198014:0;knife node delete -y i-2d93f9aa
: 1464198029:0;knife client delete -y i-46a1cbc1
: 1464198037:0;knife node delete -y i-46a1cbc1
: 1464198061:0;knife client delete -y i-95fa7e0f
: 1464198081:0;knife node delete -y i-95fa7e0f
: 1464198087:0;knife client delete -y i-c71a0140
: 1464198095:0;knife node delete -y i-c71a0140
: 1464198107:0;knife search node "chef_environment:wilder-production" > ~/Documents/wilder/knife-wilder-production-after-cleanup.txt
: 1464198112:0;more ~/Documents/wilder/knife-wilder-production-after-cleanup.txt
: 1464198149:0;shipit deployment create wilder-production abd5636bb8b6f1769fae283d5ded2929b22e48c1
: 1464198546:0;shipit deployment logs 3346
: 1464198714:0;rm terraform.tfstate.wilder
: 1464198719:0;rm test.plan
: 1464198730:0;git add wilder
: 1464198737:0;git add *.tf
: 1464198756:0;git add ../wilder/
: 1464198853:0;shipit env show wilder-production 
: 1464198859:0;history | grep console-host
: 1464198876:0;shipit environment update wilder-production --console-host=10.0.14.26
: 1464205337:0;git clone git@github.com:rabbitinaction/sourcecode.git
: 1464210042:0;ssh ubuntu@ec2-54-243-134-237.compute-1.amazonaws.com
: 1464210758:0;more ~Documents/dc/dc-prod-new-redis-knife-db-values.050216.txt
: 1464276262:0;brew upgrade ruby-build
: 1464276310:0;more ~/Documents/h200/ruby-2-2-4-install.log
: 1464276329:0;cat ~/Documents/h200/ruby-2-2-4-install.log
: 1464276364:0;rbenv install 2.3.1
: 1464276563:0;echo "11:28 AM" 
: 1464276570:0;rbenv rehash
: 1464277344:0;which aws-info
: 1464277699:0;terraform version
: 1464277801:0;which rbenv
: 1464277810:0;locate rbenv
: 1464277820:0;brew list | grep ruby
: 1464277825:0;brew list | grep -i ruby
: 1464283676:0;env | sort
: 1464283821:0;env | grep heidi
: 1464284354:0;ssh ubuntu@ec2-54-234-118-19.compute-1.amazonaws.com
: 1464284636:0;ssh ubuntu@ec2-54-234-118-19.compute-1.amazonaws.com\\
: 1464285163:0;ssh ubuntu@ec2-54-234-118-19.compute-1.amazonaws.com\

: 1464285955:0;ssh ubuntu@ec2-54-224-55-76.compute-1.amazonaws.com
: 1464287035:0;ssh ubuntu@ec2-54-146-72-167.compute-1.amazonaws.com
: 1464288437:0;history | grpe knife | grep dc
: 1464288442:0;history | grep knife | grep dc
: 1464288723:0;knife search chef_environment:dc-staging > list
: 1464288740:0;vi list 
: 1464288775:0;cat list | grep ec2.54.157
: 1464288784:0;cat list | grep ec2-54-157
: 1464288878:0;head -31 list
: 1464288919:0;knife client delete -y i-3a0897d3
: 1464288926:0;knife node delete -y i-3a0897d3
: 1464288938:0;knife ssh chef_environment:dc-staging "dpkg -l | grep libssl" -x ubuntu 
: 1464289174:0;more list | grep 54.162
: 1464289593:0;knife ssh chef_environment:dc-production "dpkg -l | grep libssl1.0.0" -x ubuntu 
: 1464289701:0;knife search chef_environment:dc-production > list
: 1464289899:0;cat list | grep i-e06f568a
: 1464289932:0;knife client delete -y i-e06f568a
: 1464289939:0;knife node delete -y i-e06f568a
: 1464289996:0;knife client delete -y i-3a5dfe56
: 1464290004:0;knife node delete -y i-3a5dfe56
: 1464290039:0;knife client delete -y i-d238bda2
: 1464290047:0;knife node delete -y i-d238bda2
: 1464290105:0;knife client delete -y i-65e712c6
: 1464290113:0;knife node delete -y i-65e712c6
: 1464290176:0;knife client delete -y i-9065a0fa
: 1464290183:0;knife node delete -y i-9065a0fa
: 1464290339:0;ssh ubuntu@ec2-54-159-59-178.compute-1.amazonaws.com
: 1464290761:0;knife client delete -y i-2c9f3dff
: 1464290768:0;knife node delete -y i-2c9f3dff
: 1464291600:0;ssh ubuntu@ec2-23-22-181-254.compute-1.amazonaws.com
: 1464310953:0;ssh ubuntu@ec2-54-159-58-43.compute-1.amazonaws.com
: 1464313605:0;ssh ubuntu@ec2-54-90-56-219.compute-1.amazonaws.com
: 1464313866:0;knife ssh chef_environment:dc-production "uptime" -x ubuntu
: 1464314682:0;knife ssh chef_environment:dc-production "dpkg -l | grep libssl1.0.0" -x ubuntu
: 1464314984:0;knife ssh chef_environment:walkadoo-production:ec2-54-89-80-148.compute-1.amazonaws.com "date" -x ubuntu
: 1464315002:0;knife ssh ec2-54-89-80-148.compute-1.amazonaws.com "date" -x ubuntu
: 1464315023:0;knife ssh chef_environment:walkadoo-production:ec2-54-89-80-148 "date" -x ubuntu
: 1464315042:0;knife ssh chef_environment:walkadoo-production "sudo apt-get update" -x ubuntu
: 1464315175:0;knife search chef_environment:walkadoo-production 
: 1464315238:0;aws-info i-e342f092
: 1464315282:0;knife ssh chef_environment:walkadoo-production "apt-get upgrade" -x ubuntu
: 1464315337:0;knife ssh chef_environment:walkadoo-production "dpkg -l | grep libssq" -x ubuntu
: 1464315469:0;history | grep knife | grep ssh
: 1464357517:0;knife ssh chef_environment:walkadoo-production "dpkg -l | grep libssl1.0.0" -x ubuntu
: 1464360648:0;knife ssh chef_environment:walkadoo-production > list
: 1464360682:0;knife search chef_environment:walkadoo-production > list
: 1464361017:0;knife ssh chef_environment:walkadoo-production "ps -ef | grep uni" -x ubuntu
: 1464361028:0;knife ssh chef_environment:walkadoo-production "ps -ef | grep unicorn master" -x ubuntu
: 1464361049:0;knife ssh chef_environment:walkadoo-production "ps -ef | grep 'unicorn master' " -x ubuntu
: 1464361065:0;knife ssh chef_environment:walkadoo-production "ps -ef | grep 'unicorn master' | grep -v 'grep' " -x ubuntu
: 1464361122:0;knife ssh chef_environment:walkadoo-production "ps -ef | grep 'nginx' | grep -v 'grep' " -x ubuntu
: 1464361140:0;knife ssh chef_environment:walkadoo-production "ps -ef | grep 'nginx: master' | grep -v 'grep' " -x ubuntu
: 1464361183:0;knife ssh chef_environment:walkadoo-production "ps -ef | grep 'nginx: master' | grep -v 'grep' " -x ubuntu > nginx_wd-list
: 1464361198:0;knife ssh chef_environment:walkadoo-production "ps -ef | grep 'unicorn master' | grep -v 'grep' " -x ubuntu > unicorn_wd_list
: 1464361228:0;knife ssh chef_environment:walkadoo-production "dpkg -l | grep 'libssl'" -x ubuntu
: 1464364504:0;knife ssh chef_environment:walkadoo-production "sudo chef-client" -x ubuntu
: 1464370451:0;knife ssh chef_environment:walkadoo-production "ls -l /var/log/chef/client.log" -x ubuntu
: 1464370514:0;history | grep libssl1
: 1464377373:0;knife ssh chef_environment:walkadoo-staging "rvm -version " -x ubuntu
: 1464377553:0;knife ssh chef_environment:walkadoo-staging "ruby --version" -x ubuntu
: 1464377573:0;knife ssh chef_environment:walkadoo-staging "ruby --version" -x deploy
: 1464394126:0;ssh ubuntu@ec2-54-89-80-148.compute-1.amazonaws.com
: 1464702712:0;knife ssh chef_environment:walkadoo-production "dpkg -l | grep 'libssl1.0.0'" -x ubuntu
: 1464702743:0;knife ssh chef_environment:walkadoo-production "uptime" -x ubuntu
: 1464702908:0;ssh heidischmidt@ec2-54-242-47-84.compute-1.amazonaws.com
: 1464703144:0;history | grep console
: 1464703161:0;shipit environment update walkadoo-production --console-host=10.155.49.39
: 1464703178:0;shipit environment show walkadoo-production
: 1464703293:0;shipit env walkadoo-production config_list
: 1464704230:0;man uname 
: 1464704241:0;uname -s 
: 1464704252:0;man uname
: 1464704270:0;more /etc/*rel*
: 1464704274:0;locate release
: 1464704356:0;ls -l /etc/*version*
: 1464704361:0;ls -l /etc/*rel*
: 1464704364:0;ls -l /etc/
: 1464704406:0;more /proc/version
: 1464704409:0;ls -l /proc
: 1464704414:0;ls -l /etc/*is*
: 1464704745:0;uname -s
: 1464704752:0;which uname 
: 1464704877:0;irb
: 1464706759:0;which easy_setup
: 1464707044:0;python ez_setup.py
: 1464707055:0;which easy_install
: 1464707062:0;ls -ltra /usr/local/bin/easy_install
: 1464707173:0;easy_install pika
: 1464707224:0;mkdir rabbitMQ-chapter-2
: 1464707249:0;rm rabbitMQ-chapter-2
: 1464707254:0;rm -rf rabbitMQ-chapter-2
: 1464707347:0;mkdir book-chapter-2 
: 1464707362:0;find . -name "hello*world*py" -print 
: 1464707371:0;find . -name "hello*world*py" -print  >> heidi-notes-rabbit-mq-sourcecode
: 1464707534:0;brew install rabbitmq
: 1464707698:0;brew services start rabbitmq
: 1464707706:0;brew services list >> heidi-notes-rabbit-mq-sourcecode
: 1464707709:0;vi heidi-notes-rabbit-mq-sourcecode
: 1464707752:0;netstat -an | more
: 1464728258:0;more heidi-notes-rabbit-mq-sourcecode
: 1464728283:0;rm -rf book-chapter-2
: 1464728289:0;ls -l python/chapter-2
: 1464728306:0;ln -s python/chapter-2 chapter-2
: 1464728319:0;more hello_world_producer
: 1464728327:0;more hello_world_producer.py
: 1464728342:0;python hello_world_producer.py
: 1464728362:0;easy_install sys
: 1464728373:0;python 
: 1464728494:0;python hello_world_producer.py Heidi
: 1464787790:0;rabbitmqctl status
: 1464787801:0;rabbitmqctl vhosts
: 1464789849:0;ls -ltra *.py
: 1464789892:0;cd chapter-2
: 1464789928:0;python ./hello_world_producer.py 'Hello New MYH World!'
: 1464789954:0;python ./hello_world_producer.py 'Hello New Day Weds June 1st 2016!'
: 1464789976:0;python ./hello_world_producer.py 'quit the test'
: 1464789985:0;python ./hello_world_producer.py 'quit'
: 1464790660:0;more hello_world_producer_pubconfirm.py
: 1464790717:0;cat ./hello_world_consumer.py
: 1464790754:0;python ./hello_world_consumer.py
: 1464790787:0;python ./hello_world_producer_pubconfirm.py 'Testing new form'
: 1464790846:0;python ./hello_world_producer_pubconfirm.py 
: 1464790856:0;python ./hello_world_producer_pubconfirm.py  'Hello'
: 1464794880:0;history | grep walkadoo-staging | grep ssh
: 1464795002:0;ssh -A -L 9999:walkadoo-staging.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 heidischmidt@ec2-54-90-101-115.compute-1.amazonaws.com -N
: 1464795055:0;mysql -u meyouhealth -p -P 9999 -h 127.0.0.1 
: 1464797089:0;ls -ltra /tmp/
: 1464797144:0;cp /tmp/walkadoo-logging.txt Documents/walkadoo/walkadoo-prod-console-blue-pill-outage-060116-0956AM-sleuthing-retry-11ish.log
: 1464877478:0;ps -ef | grep res
: 1464877493:0;ps -ef | egrep -i "res|uni" 
: 1464877501:0;ps -ef | egrep -i "resq|uni" 
: 1464877508:0;ps -ef | more
: 1464877538:0;ps -ef | egrep -i "[amfid|stackshot]" 
: 1464877546:0;ps -ef | egrep  "[amfid|stackshot]" 
: 1464877563:0;ps -ef | egrep  "['amfid'|'stackshot']" 
: 1464877573:0;ps -ef | grep ssh
: 1464877596:0;ps -ef | egrep  "['ssh-'|'heidischmidt']" 
: 1464877601:0;ps -ef | egrep  "['ssh'|'heidischmidt']" 
: 1464877644:0;ps -ef | egrep  "login|heidischmidt" 
: 1464877655:0;ps -ef | egrep  "loginwindow|heidischmidt" 
: 1464889297:0;ssh heidischmidt@ec2-54-145-6-93.compute-1.amazonaws.com
: 1464890172:0;ssh heidischmidt@ec2-54-237-123-172.compute-1.amazonaws.com
: 1464890424:0;ssh heidischmidt@ec2-54-145-119-245.compute-1.amazonaws.com
: 1464890708:0;ssh heidischmidt@ec2-54-243-134-237.compute-1.amazonaws.com
: 1464897068:0;ssh heidischmidt@ec2-54-83-45-39.compute-1.amazonaws.com
: 1464966294:0;mysql -u meyouhealth -p -P 9998 -h 127.0.0.1 
: 1465221521:0;ssh ubuntu@ec2-54-90-124-73.compute-1.amazonaws.com
: 1465226266:0;histopry | grep continu
: 1465226270:0;histopry | grep contin
: 1465226338:0;shipit console legacy-deploy-dashboard 
: 1465227767:0;ssh ubuntu@ec2-54-90-161-228.compute-1.amazonaws.com
: 1465231233:0;cd environments/production
: 1465233243:0;history | grep knife | grep search 
: 1465233280:0;knife search node "chef_environment:walkadoo-production
: 1465233288:0;history | grep knife | grep "grep"
: 1465233363:0;knife ssh chef_environment:walkadoo-production "grep '\[03\/Jun\/2016:20' /var/log/nginx/access.log.1" -x ubuntu > test
: 1465233370:0;knife ssh chef_environment:walkadoo-production "grep '\[03\/Jun\/2016:20' /var/log/nginx/access.log.1" -x ubuntu  > foople
: 1465233422:0;more foople
: 1465233472:0;mv foople walkadoo-production-june-03-2016-2000-hr-search-all-nodes.log
: 1465233504:0;grep "HTTP/1.1\" 499 0" walkadoo-production-june-03-2016-2000-hr-search-all-nodes.log
: 1465233609:0;grep "HTTP/1.1\" 499 0" walkadoo-production-june-03-2016-2000-hr-search-all-nodes.log | grep "\[03\/Jun\/2016:20:0"
: 1465233664:0;grep "HTTP/1.1\" 499 0" walkadoo-production-june-03-2016-2000-hr-search-all-nodes.log | grep "\[03\/Jun\/2016:20:0" > early_http_409s_06032016-2009.log
: 1465233734:0;mv walkadoo-production-june-03-2016-2000-hr-search-all-nodes.log Documents/
: 1465233740:0;mv early_http_409s_06032016-2009.log Documents
: 1465233821:0;more walkadoo-production-june-03-2016-2000-hr-search-all-nodes.log
: 1465233881:0;more early_http_409s_06032016-2009.log
: 1465234072:0;mv early_http_409s_06032016-2009.log early_http_409s_06032016-2009.txt
: 1465234084:0;mv walkadoo-production-june-03-2016-2000-hr-search-all-nodes.log walkadoo-production-june-03-2016-2000-hr-search-all-nodes.txt
: 1465234214:0;vi walkadoo-production-june-03-2016-2000-hr-search-all-nodes.txt
: 1465234934:0;grep ec2-54-196-178-138.compute-1.amazonaws.com walkadoo-production-june-03-2016-2000-hr-search-all-nodes.txt
: 1465234952:0;grep ec2-54-196-178-138.compute-1.amazonaws.com walkadoo-production-june-03-2016-2000-hr-search-all-nodes.txt | grep 499
: 1465313466:0;grep registrations walkadoo-production-june-03-2016-2000-hr-search-all-nodes.txt 
: 1465313494:0;grep registrations walkadoo-production-june-03-2016-2000-hr-search-all-nodes.txt
: 1465313571:0;Fr0uFr0uF0x3s
: 1465314592:0;cat .my.cnf
: 1465314622:0;mysql -u meyouhealth -P 9998 -h 127.0.0.1 -e "show full processlist"
: 1465314648:0;more pt-osc-users-dry-run-alter-users.sh
: 1465314703:0;cp pt-osc-users-dry-run-alter-users.sh ../walkadoo/
: 1465314707:0;ls -l ../wdoo
: 1465314726:0;cp mysqldump_schema.sh ../walkadoo/
: 1465314742:0;cat mysqldump_schema.sh >> pt-osc-users-dry-run-alter-users.sh
: 1465314760:0;mv pt-osc-users-dry-run-alter-users.sh wd-deploy-action-tracking.sh 
: 1465315535:0;vi show-engine-innodb-status.sql 
: 1465315589:0;mysql -u meyouhealth -P 9998 -h 127.0.0.1 < show-engine-innodb-status.sql > show-engine-innodb-status.log
: 1465315899:0;mysql -help
: 1465315904:0;mysql --help
: 1465315924:0;vi wd-deploy-action-tracking.sh
: 1465316069:0;vi wd-action-tracking-during-deploy.sql
: 1465316147:0;rm wd-deploy-action-tracking.sh-20160607-1465315889.log
: 1465316230:0;date +"%Y%m%d-%s"
: 1465316239:0;touch file-`date +"%Y%m%d-%s"`.log
: 1465316242:0;ls -trla file-20160607-1465316239.log
: 1465316419:0;rm file-20160607-1465316239.log
: 1465316428:0;more wd-deploy-action-tracking.sh-20160607-1465315943.log
: 1465316456:0;more show-engine-innodb-status-20160607-1465316370.log
: 1465318795:0;ssh ubuntu@
: 1465319329:0;more wd-deploy-action-tracking.sh-20160607-1465317025.log
: 1465319375:0;more show-engine-innodb-status-20160607-1465317090.log
: 1465319785:0;more wd-deploy-action-tracking.sh-20160607-1465318887.log
: 1465319923:0;grep "LOCK WAIT" wd-deploy-action-tracking.sh-20*
: 1465320015:0;more wd-deploy-action-tracking.sh-20160607-1465319976.log
: 1465321948:0;more show-engine-innodb-status-20160607-1465321878.log
: 1465322105:0;mysql -u meyouhealth -P 9998 -h 127.0.0.1 -D walkadoo_production -e "SELECT  `users`.* FROM `users`  WHERE `users`.`id` = 45819;" 
: 1465322130:0;mysql -u meyouhealth -P 9998 -h 127.0.0.1 -D walkadoo_production -e "SELECT  * FROM users  WHERE id = 45819;" 
: 1465322147:0;mysql -u meyouhealth -P 9998 -h 127.0.0.1 -D walkadoo_production -e "SELECT  count(*) FROM users  WHERE id = 45819;" 
: 1465322166:0;mysql -u meyouhealth -P 9998 -h 127.0.0.1 -D walkadoo_production -e "explain SELECT  * FROM users  WHERE id = 45819;" 
: 1465322186:0;mysql -u meyouhealth -P 9998 -h 127.0.0.1 -D walkadoo_production -e "show create table users;" 
: 1465322432:0;more wd-deploy-action-tracking.sh-20160607-1465322331.log
: 1465323395:0;mkdir logs-during-deploy
: 1465323401:0;mv *.log logs-during-deploy
: 1465324816:0;history | grep "show engine innodb"
: 1465324820:0;history | grep "show engine "
: 1465324829:0;history | grep "mysql"
: 1465324944:0;ls -ltr logs-during-deploy
: 1465324986:0;more wd-deploy-action-tracking.sh-20160607-1465321726.log
: 1465324992:0;more logs-during-deploy/wd-deploy-action-tracking.sh-20160607-1465321726.log
: 1465328155:0;bash -xv wd-deploy-action-tracking.sh
: 1465328173:0;more wd-deploy-action-tracking.sh-20160607-1465328155.log
: 1465328216:0;grep 45815 *.log
: 1465328228:0;more show-engine-innodb-status-20160607-1465324841.log
: 1465328256:0;more show-engine-innodb-status-20160607-1465328124.log
: 1465328288:0;cd logs-during-deploy
: 1465328306:0;more walkadoo-prod-console-blue-pill-outage-060116-0956AM-sleuthing-retry-11ish.log
: 1465328422:0;grep "mysql tables in use" *.log
: 1465328442:0;more show-engine-innodb-status-20160607-1465318879.log
: 1465329806:0;mkdir logs-after-deploy
: 1465329816:0;mv *.log logs-after-deploy/
: 1465329834:0;mv logs-during-deploy logs-during-deploy-060716
: 1465329845:0;mv logs-after-deploy logs-after-deploy-060716
: 1465329870:0;grep ACTIVE *.log
: 1465329900:0;grep ACTIVE show-engine-innodb-status-20160607-1*.log 
: 1465329946:0;grep ACTIVE show-engine-innodb-status-20160607-1*.log  | awk -F "TRANSACTION" '{print $2 $3}'
: 1465329958:0;grep ACTIVE show-engine-innodb-status-20160607-1*.log  | awk -F "TRANSACTION" '{print $2 $3}' | sort
: 1465329990:0;cd ../logs-after-deploy-060716
: 1465330019:0;ls -ltrqa
: 1465330028:0;more show-engine-innodb-status-20160607-1465327362.log
: 1465330045:0;!
: 1465330058:0;grep ACTIVE show-engine-innodb-status-20160607-1*.log  | awk -F "TRANSACTION" '{print $2 $3}' | sort | uniq -c
: 1465330115:0;grep ACTIVE show-engine-innodb-status-20160607-1*.log  | awk -F "TRANSACTION" '{print $2 $3}' | sort | grep -v "1257826115" | grep -v "1257826361" | uniq -c
: 1465330195:0;ls -l wd-deploy-action-tracking.sh-20160607*.loog
: 1465330199:0;ls -l wd-deploy-action-tracking.sh-20160607*.log
: 1465330202:0;ls -l wd-deploy-action-tracking.sh-20160607*.log | wc -l 
: 1465330234:0;ls -l  show*
: 1465330240:0;ls -l  show* | wc -l 
: 1465330358:0;grep 1322108942 show-engine-innodb-status*.log
: 1465330382:0;ls -ltra show-engine-innodb-status-20160607-1465318879.log
: 1465396558:0;scp heidischmidt@ec2-54-146-91-11.compute-1.amazonaws.com:web1-wd-ip-10-178-222-17-watching.log Documents/walkadoo/
: 1465396594:0;scp heidischmidt@ec2-54-242-222-79.compute-1.amazonaws.com:web2-wd-ip-10-79-177-186-watching.log Documents/walkadoo/web2-wd-ip-10-79-177-186-watching.log
: 1465396620:0;scp heidischmidt@ec2-54-237-58-62.compute-1.amazonaws.com:web3-wd-ip-10-157-155-168-watching.log Documents/walkadoo/web3-wd-ip-10-157-155-168-watching.log
: 1465396657:0;scp heidischmidt@ec2-54-196-178-138.compute-1.amazonaws.com:web4-wd-ip-10-218-134-17-watching.log Documents/walkadoo/web4-wd-ip-10-218-134-17-watching.log
: 1465396672:0;vi web*ip*watching.log
: 1465486564:0;cd logs-during-deploy-060716
: 1465486596:0;more show-engine-innodb-status.log
: 1465486602:0;vi show-engine-innodb-status.log
: 1465486625:0;grep "THIS LOCK" *.log
: 1465486647:0;vi walkadoo-prod-console-blue-pill-outage-060116-0956AM-sleuthing-retry-11ish.log
: 1465486699:0;rec
: 1465486711:0;vi show-engine-innodb-status-20160607-1465323318.log 
: 1465486760:0;ls -lktra
: 1465486781:0;grep -i trx wd-deploy-action-tracking.sh-20160607-1465323057.log
: 1465486787:0;grep -i trx wd-deploy-action-tracking.sh-20160607*.log
: 1465486794:0;vi wd-deploy-action-tracking.sh-20160607-1465323299.log
: 1465486943:0;more wd-deploy-action-tracking.sh-20160607-1465323057.log
: 1465493555:0;ssh -L 55555:localhost:15672 ubuntu@rabbitmq.myhstg.com
: 1465495190:0;ssh ubuntu@ec2-54-242-222-79.compute-1.amazonaws.com
: 1465495199:0;ssh ubuntu@ec2-54-237-58-62.compute-1.amazonaws.com
: 1465495290:0;grep while *watch*
: 1465495414:0;history | grep ssh | grep ec2
: 1465495447:0;ssh -A -L 9998:walkadoo-production.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 heidischmidt@ec2-54-90-17-51.compute-1.amazonaws.com -N
: 1465495498:0;ssh ubuntu@ec2-54-196-178-138.compute-1.amazonaws.com
: 1465495593:0;mkdir web-log
: 1465495603:0;mv web*watching*.log web-log
: 1465495614:0;history | grpe wd-deploy-action-tracking.sh
: 1465495621:0;history | grep wd-deploy-action-tracking.sh
: 1465495629:0;while true ; do echo "$(date)" ;bash -xv wd-deploy-action-tracking.sh ; sleep 60;  done
: 1465496012:0;history | grep show
: 1465496026:0;while true ; do echo "$(date)" ;mysql -u meyouhealth -P 9998 -h 127.0.0.1 < show-engine-innodb-status.sql > show-engine-innodb-status-`date +"%Y%m%d-%s"`.log ; sleep 360;  done
: 1465498292:0;history | grpe contin
: 1465499528:0;ssh -L 55555:127.0.0.1:15672 heidischmidt@rabbitmq.meyouhealth.com
: 1465499565:0;ssh heidischmidt@rabbitmq.meyouhealth.com
: 1465500453:0;history | grpe rabbit
: 1465500456:0;history | grep rabbit
: 1465500480:0;ssh -L 55554:localhost:15672 ubuntu@rabbitmq.myhstg.com
: 1465500488:0;ssh -L 55554:localhost:15672 heidischmidt@rabbitmq.myhstg.com
: 1465500629:0;ssh heidischmidt@rabbitmq.myhstg.com
: 1465500943:0;echo "Find out why rabbitmq.mystg.com didnâ€ƒ¹t go to 1.0.1f-1ubuntu2.19"
: 1465824396:0;shipit env show wellbeingtracker
: 1465824625:0;shipit ssh wellbeingtracker-production 
: 1465826666:0;cd wellbeing
: 1465826733:0;cd ../daily-challenge
: 1465826844:0;more */elasticache.tf
: 1465826913:0;vi daily-challenge/elasticache.tf
: 1465836616:0;shipit env update walkadoo-production --continuous-deployment
: 1465836626:0;shipit env update walkadoo-production --no-continuous-deployment
: 1465837910:0;history | grep knife | grep walkado
: 1465838878:0;knife environment show walkadoo-production --format json 
: 1465838894:0;knife environment show walkadoo-production --format json  | grep -i database_name 
: 1465838927:0;knife environment show walkadoo-production --format json  > wd-prd-env-knife
: 1465838934:0;cat wd-prd-env-knife
: 1465840774:0;ssh ubuntu@-83-45-39.compute-1.amazonaws.com
: 1465840782:0;ssh ubuntu@ec2-83-45-39.compute-1.amazonaws.com
: 1465840796:0;ssh ubuntu@ec2-54-83-45-39.compute-1.amazonaws.com
: 1465843176:0;knife environment edit dc-staging
: 1465845128:0;ssh ubuntu@ec2-54-196-188-86.compute-1.amazonaws.com
: 1465845323:0;ssh ubuntu@ec2-54-162-59-225.compute-1.amazonaws.com
: 1465998248:0;history | grep knife | grep staging
: 1465998304:0;knife environment edit walkadoo-production
: 1465999872:0;history | grep walkadoo-staging
: 1466000823:0;ssh -A -L 9999:walkadoo-staging2.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 heidischmidt@ec2-54-90-101-115.compute-1.amazonaws.com -N
: 1466000890:0;mysql -u walkadoo_stg_app -p -P 9999 -h 127.0.0.1
: 1466000937:0;mysql -u meyouhealth -p -P 9999 -h 127.0.0.1
: 1466002778:0;history | grep knife | walkadoo
: 1466002785:0;history | grep knife | grep walkadoo
: 1466003498:0;vi wd-stg-new-vpc-db
: 1466003753:0;ssh ubuntu@ec2-54-91-115-4.compute-1.amazonaws.com
: 1466010483:0;cd production/walkadoo
: 1466010501:0;cat ../*/rds.tf
: 1466010784:0;ls -ltra terraform.tfvars
: 1466010896:0;more ~/Documents/git/chef-repo/terraform/production/variables.tf| grep walkad
: 1466010900:0;more ~/Documents/git/chef-repo/terraform/production/variables.tf
: 1466011226:0;aws rds describe-db-instances --db-instance-identifier walkadoo-productiom
: 1466011395:0;aws rds describe-db-instances --db-instance-identifier walkadoo-production
: 1466014821:0;history | grep wilder-production
: 1466014922:0;ssh -A -L 7000:walkadoo-production.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 heidischmidt@ec2-54-90-17-51.compute-1.amazonaws.com -N 
: 1466014936:0;history | grep 7000
: 1466014975:0;mysql -u meyouhealth -p -P 7000 -h 127.0.0.1 
: 1466015582:0;ssh -A -L 7005:walkadoo-production2.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 heidischmidt@ec2-54-90-17-51.compute-1.amazonaws.com -N 
: 1466016447:0;mysql -u walkadoo_prd_app -p -P 7005 -h 127.0.0.1 
: 1466017970:0;history | grep knife | grep dc-staging
: 1466017976:0;knife environment show  dc-staging --format json | grep resq
: 1466085707:0;vi walkadoo-production-read-slave-status-06162016.log
: 1466086038:0;mysqldump -vvv --opt --single-transaction --master-data=2 -u meyouhealth -p -P 7001 -h 127.0.0.1 walkadoo_production  
: 1466086299:0;rm wd-deploy-action-tracking.sh*.log
: 1466086313:0;rm show-engine-innodb-status*.log
: 1466086358:0;watch ls -ltra 
: 1466086374:0;watch `ls -ltra walkadoo-prod-read1-full-dump.06162016.sql\
`
: 1466086382:0;watch `ls -ltra walkadoo-prod-read1-full-dump.06162016.sql`\
`
: 1466086389:0;watch `ls -ltra walkadoo-prod-read1-full-dump.06162016.sql`
: 1466086523:0;history | tail -30
: 1466087314:0;head walkadoo-prod-read1-full-dump.06162016.sql
: 1466087326:0;head walkadoo-prod-read1-full-dump.06162016.sql > start_dump_time
: 1466087336:0;tail walkadoo-prod-read1-full-dump.06162016.sql > end_dump_time
: 1466087395:0;vi catching_up_after_slave_restarted_halfway_through_dump.log
: 1466087487:0;more walkadoo-production-read-slave-status-06162016.log | grep master
: 1466087492:0;more walkadoo-production-read-slave-status-06162016.log | grep Position
: 1466087497:0;more walkadoo-production-read-slave-status-06162016.log | grep -i position
: 1466087515:0;more walkadoo-production-read-slave-status-06162016.log | grep -i bin
: 1466087722:0;mysql -u meyouhealth -p -P 7005 -h 127.0.0.1 -D walkadoo_production 
: 1466087899:0;mysql -u meyouhealth -p -P 7005 -h 127.0.0.1 -D walkadoo_production -vvv < walkadoo-prod-read1-full-dump.06162016.sql > walkadoo-prod2-full-import-06162016.log 
: 1466088576:0;ssh -A -L 9005:walkadoo-production.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 heidischmidt@ec2-54-90-17-51.compute-1.amazonaws.com -N
: 1466088611:0;mysql -u meyouhealth -p -P 9005 -h 127.0.0.1
: 1466089432:0;more start_dump_time
: 1466089445:0;cat end_dump_time
: 1466096999:0;ps -ef | grep mysql > start-of-import
: 1466097019:0;date >> current_import_time_logfile
: 1466097030:0;ls -ltra >> current_import_time_logfile
: 1466097034:0;more current_import_time_logfile
: 1466097105:0;more end_dump_time
: 1466097114:0;more catching_up_after_slave_restarted_halfway_through_dump.log
: 1466097411:0;more start-of-import
: 1466097591:0;more walkadoo-production-read-slave-status-06162016.log | grep -i change
: 1466097639:0;more walkadoo-production-read-slave-status-06162016.log | grep -i pos
: 1466097654:0;more walkadoo-production-read-slave-status-06162016.log | grep -i Exec_Master_Log_Pos
: 1466100700:0;history | grep walkadoo-production-read1
: 1466100740:0;mysql -u meyouhealth -p -P 7001 -h 127.0.0.1
: 1466100800:0;more walkadoo-production-read-slave-status-06162016.log
: 1466100967:0;rm walkadoo-prod-read1-full-dump.06162016.sql
: 1466101010:0;mv start_dump_time start_dump_time_run1
: 1466101019:0;mv end_dump_time end_dump_time_run1
: 1466101028:0;mv current_import_time_logfile current_import_time_logfile_run1
: 1466101041:0;mv start-of-import start-of-import-run1
: 1466101064:0;mv walkadoo-production-read-slave-status-06162016.log walkadoo-production-show-slave-status-06162016.log 
: 1466101189:0;mysql -u meyouhealth -p -P 7001 -h 127.0.0.1 walkadoo_production
: 1466101332:0;history | grep mysqldump | grep 7001
: 1466101386:0;mysqldump -vvv --opt --single-transaction -u meyouhealth -p -P 7001 -h 127.0.0.1 walkadoo_production  > walkadoo-prod-read1-full-dump.06162016.sql 
: 1466101470:0;vi walkadoo-prod-read1-master-status-and-slave-status-start-export.log
: 1466101585:0;cat walkadoo-prod-read1-master-status-and-slave-status-start-export.log
: 1466101822:0;ps -ef | grep mysql > start-time-run2
: 1466102007:0;mv start-time-run2 start-dump-time-run2 
: 1466102259:0;head -20 walkadoo-prod-read1-full-dump.06162016.sql
: 1466102267:0;tail walkadoo-prod-read1-full-dump.06162016.sql
: 1466102278:0;head -50 walkadoo-prod-read1-full-dump.06162016.sql | grep Dump
: 1466102297:0;tail walkadoo-prod-read1-full-dump.06162016.sql > dump_completed_run2
: 1466102312:0;cat dump_completed_run2
: 1466102438:0;	mysql -u meyouhealth -p -P 7005 -h 127.0.0.1 -D walkadoo_production -vvv < walkadoo-prod-read1-full-dump.06162016.sql > walkadoo-prod2-full-import-06162016.log
: 1466103145:0;ls -lrtah
: 1466103166:0;head walkadoo-prod2-full-import-06162016.log
: 1466103173:0;head -50 walkadoo-prod2-full-import-06162016.log
: 1466105106:0;more more current_import_time_logfile_run1
: 1466105406:0;cat start-of-import
: 1466105413:0;ls -l start-of-import*
: 1466105780:0;cat start-dump-time-run2
: 1466105808:0;ps -ef | grep mysql > start-of-import-run2
: 1466105814:0;more start-of-import-run2
: 1466105830:0;more current_import_time_logfile_run1
: 1466106159:0;cat start-of-import-run1
: 1466165199:0;tail walkadoo-prod2-full-import-06162016.log
: 1466165209:0;more walkadoo-prod-read1-full-dump.06162016.sql
: 1466165444:0;rm walkadoo-prod2-full-import-06162016.log
: 1466165527:0;mysql -u meyouhealth -p -P 7005 -h 127.0.0.1 -D walkadoo_production -vvv < walkadoo-prod-read1-full-dump.06162016.sql > walkadoo-prod2-full-import-06172016.log 
: 1466165549:0;ps -ef | grep mysql
: 1466165564:0;ps -ef | grep mysql >> start-dump-time-run2
: 1466165570:0;date >> start-dump-time-run2
: 1466167437:0;ls -l deis
: 1466167447:0;ln -fs $PWD/deis /usr/local/bin/deis
: 1466167472:0;curl -sSL https://get.helm.sh | bash
: 1466168030:0;which helmc
: 1466168038:0;ls -l helmc
: 1466168064:0;rm pgadmin.log
: 1466168264:0;ln -fs $PWD/helmc /usr/local/bin/helmc
: 1466169717:0;more start-dump-time-run2
: 1466169727:0;mv start-dump-time-run2 start-of-import-run2
: 1466169751:0;cat start-of-import-run2
: 1466179436:0;mysql -u meyouhealth -p -P 7005 -h 127.0.0.1 -D walkadoo_production
: 1466180130:0;screen 
: 1466180141:0;history | grep mysql | grep vvv
: 1466180162:0;mysql -u meyouhealth -p -P 7005 -h 127.0.0.1 -D walkadoo_production -vvv < walkadoo-prod-read1-full-dump.06162016.sql > walkadoo-prod2-full-import-06172016.log
: 1466180204:0;more walkadoo-prod2-full-import-06172016.log
: 1466180220:0;tail -f walkadoo-prod2-full-import-06172016.log
: 1466211767:0;more walkadoo-prod-read1-master-status-and-slave-status-start-export.log
: 1466211818:0;rm walkadoo-prod2-full-import-06172016.log
: 1466211827:0;rm walkadoo-prod-read1-full-dump.06162016.sql 
: 1466211983:0;vi argh-master-status-again-re-dump
: 1466212021:0;history | grep mysqldump
: 1466212048:0;mysqldump -vvv --opt --single-transaction -u meyouhealth -p -P 7001 -h 127.0.0.1 walkadoo_production  > walkadoo-prod-read1-full-dump.061716-907.sql 
: 1466212111:0;mysql -u meyouhealth -p -P 7001 -h 127.0.0.1 
: 1466212269:0;while true ; do echo "$(date)" ; ls -ltra walkadoo-prod-read1-full-dump.061716-907.sql; sleep 5; done
: 1466212467:0;ls -ltrah walkadoo-prod-read1-full-dump.061716-907.sql
: 1466212497:0;while true ; do echo "$(date)" ; ls -ltra walkadoo-prod-read1-full-dump.061716-907.sql; sleep 15; done
: 1466212642:0;cat start_dump_time_run1 end_dump_time_run1
: 1466249070:0;mysql -u meyouhealth -p -P 7005 -h 127.0.0.1 -D walkadoo_production -vvv < walkadoo-prod-read1-full-dump.06162016.sql > walkadoo-prod2-full-import-06182016.log
: 1466249134:0;mysql -u meyouhealth -p -P 7005 -h 127.0.0.1 -D walkadoo_production -vvv < walkadoo-prod-read1-full-dump.061716-907.sql > walkadoo-prod2-full-import-061816-725AM.log
: 1466343073:0;tail walkadoo-prod2-full-import-061816-725AM.log
: 1466343116:0;more walkadoo-prod2-full-import-061816-725AM.log
: 1466343141:0;tail -1 walkadoo-prod2-full-import-061816-725AM.log
: 1466343209:0;tail -f walkadoo-prod2-full-import-061816-725AM.log
: 1466429564:0;vi export-wd-read1-sp-info.log
: 1466431251:0;cat export-wd-read1-sp-info.log
: 1466431402:0;more argh-master-status-again-re-dump
: 1466431544:0;	ssh -A -L 7001:walkadoo-production-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 heidischmidt@ec2-54-90-17-51.compute-1.amazonaws.com -N
: 1466431845:0;mysql -u meyouhealth -p -P 7005 -h 127.0.0.1 
: 1466433356:0;rm walkadoo-prod2-full-import-061816-725AM.log
: 1466433363:0;rm walkadoo-prod-read1-full-dump.061716-907.sql
: 1466434934:0;history | grep knife | grep walkadoo-prod
: 1466434986:0;knife environment show walkadoo-production --format json  > walkadoo-prod-knife-data-bag-062016.txt
: 1466434997:0;vi walkadoo-prod-knife-data-bag-062016.txt
: 1466435049:0;shipit env config_list walkadoo-production > shipit-wd-prod-current-values-062016.txt
: 1466435063:0;cp shipit-wd-prod-current-values-062016.txt shipit-wd-prod-new-values-062016.txt
: 1466435075:0;cat walkadoo-prod-knife-data-bag-062016.txt >> shipit-wd-prod-new-values-062016.txt
: 1466435081:0;vi shipit-wd-prod-new-values-062016.txt
: 1466444239:0;cat shipit-wd-prod-new-values-062016.txt | grep -v "\|" 
: 1466444276:0;cat shipit-wd-prod-new-values-062016.txt | grep -v "\|"  > changes_for_shipit_wd_prod.062016.txt
: 1466444378:0;cat changes_for_shipit_wd_prod.062016.txt 
: 1466444400:0;sh -xv changes_for_shipit_wd_prod.062016.txt
: 1466444543:0;shipit env config_list walkadoo-production > newest-combo-shipit-wd-prod.062016.txt
: 1466444563:0;vi changes_for_shipit_wd_prod.062016.txt
: 1466444577:0;grep twilio changes_for_shipit_wd_prod.062016.txt
: 1466444582:0;grep -i twilio changes_for_shipit_wd_prod.062016.txt
: 1466444606:0;shipit env config_unset TWILIO_SHORT_CODE_IMPLEMENTATION_DATE
: 1466444619:0;shipit env config_unset walkadoo-production TWILIO_SHORT_CODE_IMPLEMENTATION_DATE
: 1466444627:0;shipit env config_list walkadoo-production 
: 1466444661:0;shipit env config_list walkadoo-production > newest-combo-shipit-wd-prod-wout-twilio-short-date.062016.txt
: 1466446431:0;grep placeholder newest-combo-shipit-wd-prod
: 1466446439:0;grep placeholder newest-combo-shipit-wd-prod-wout-twilio-short-date.062016.txt
: 1466446850:0;vi wd-prod-application-rb-code-list.062016.txt
: 1466446889:0;grep ENV wd-prod-application-rb-code-list.062016.txt > ENV-wd-prod-application-rb-code-list.062016.txt
: 1466446893:0;more ENV-wd-prod-application-rb-code-list.062016.txt
: 1466446898:0;vi ENV-wd-prod-application-rb-code-list.062016.txt
: 1466447025:0;cat ENV-wd-prod-application-rb-code-list.062016.txt | awk -F "ENV[" '{print $1}' 
: 1466447031:0;cat ENV-wd-prod-application-rb-code-list.062016.txt | awk -F "ENV\[" '{print $1}' 
: 1466447037:0;cat ENV-wd-prod-application-rb-code-list.062016.txt | awk -F "ENV" '{print $1}' 
: 1466447043:0;cat ENV-wd-prod-application-rb-code-list.062016.txt | awk -F "ENV" '{print $2}' 
: 1466447050:0;cat ENV-wd-prod-application-rb-code-list.062016.txt | awk -F "ENV" '{print $2}'  > list
: 1466447216:0;cat list | awk '{print $1}' | sort 
: 1466448235:0;shipit env config_unset MEMCACHED_HOST walkadoo-production
: 1466448251:0;shipit env config_list MEMCACHED 
: 1466448257:0;shipit env config_list MEMCACHED walkadoo-production
: 1466448280:0;shipit env config_list walkadoo-production | grep memcache
: 1466448301:0;shipit env config_unset walkadoo-production MEMCACHED_HOST
: 1466448465:0;vi newest-combo-shipit-wd-prod-wout-twilio-short-date.062016.txt
: 1466448898:0;cat newest-combo-shipit-wd-prod-wout-twilio-short-date.062016.txt | grep -v "\|" 
: 1466449179:0;shipit env config_unset walkadoo-production APN_API_CLIENT_ID
: 1466449195:0;shipit env config_unset walkadoo-production DESK_DOT_COM_API_KEY
: 1466449204:0;shipit env config_unset walkadoo-production HIPPO_HOST
: 1466449211:0;shipit env config_unset walkadoo-production HIPPO_ENDPOINT
: 1466449222:0;shipit env config_unset walkadoo-production HIPPO_CLIENT_ID
: 1466449234:0;shipit env config_unset walkadoo-production HIPPO_SECRET
: 1466449262:0;shipit env config_set walkadoo-production TWILIO_SHORT_CODE_IMPLEMENTATION_DATE placeholder-till-datetime-fix-string
: 1466449282:0;shipit env config_set walkadoo-production WBID_OAUTH_AUTHORIZE_URL       "/walkadoo/oauth/authorize"
: 1466449302:0;shipit env config_set walkadoo-production WBID_OAUTH_TOKEN_URL           "/walkadoo/oauth/token"
: 1466449323:0;shipit env config_set walkadoo-production WBID_OAUTH_CLIENT_ID "9137a0a1bf1cfd239c18d59aa2db76b95d9c8a2132aac85200afcbf1cec7af7c"
: 1466449341:0;shipit env config_set walkadoo-production WBID_OAUTH_CLIENT_SECRET "2cd99e5b5e23eed3e54c3e86e8e7e8b74f5b39a5ff4b4c2368c33cb5c50599e4k"
: 1466449362:0;shipit env config_set walkadoo-production WALKADOO_MEMCACHED_HOST           wd-prod-memcache.teaj7o.cfg.use1.cache.amazonaws.com
: 1466449382:0;shipit env config_set walkadoo-production SECRET_KEY_BASE                    0e24fdac7b4d749a21d031f223a92aa897133adc923becf84899e73b16952a827485e30ee20b9b820aabaf79ccc7abe50852f5cbda668432d5fda2f5621633d1
: 1466449407:0;shipit env config_set walkadoo-production SERVICE_CREDENTIAL_ENCRYPTION_KEY  Pa64b0Tg4bE0Q4VzD2VZ3637V064p9V9
: 1466449457:0;vi final-working-wd-prod-shipit-vars.062016.txt
: 1466449625:0;grep -i logger_type *
: 1466449653:0;shipit env config_set walkadoo-production LOGGER_TYPE syslog
: 1466449735:0;more shipit-wd-prd-from-github-chef-walkadoo-recipes-application-rb-vars.txt
: 1466449769:0;shipit env config_set walkadoo-production RAILS_SERVE_STATIC_FILES true
: 1466449785:0;shipit env config_list walkadoo-production > final-working-wd-prod-shipit-vars.062016.txt
: 1466449795:0;grep LOGGER final-working-wd-prod-shipit-vars.062016.txt
: 1466449802:0;grep RAILS final-working-wd-prod-shipit-vars.062016.txt
: 1466452485:0;cat final-working-wd-prod-shipit-vars.062016.txt | sort | awk '{print $1}'
: 1466452574:0;cat final-working-wd-prod-shipit-vars.062016.txt
: 1466522208:0;	ssh -A -L 7005:walkadoo-production2.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 heidischmidt@ec2-54-90-17-51.compute-1.amazonaws.com -N
: 1466524941:0;history | grep aws | grep elasticache
: 1466524962:0;aws elasticache describe-cache-clusters --show-cache-node-info > Documents/aws-elasticache-desc-cache-clusters-show-node-info.062116
: 1466524982:0;aws elasticache describe-cache-clusters --show-cache-node-info > ~/Documents/aws-elasticache-desc-cache-clusters-show-node-info.062116
: 1466525142:0;history | grep knife | grep iris
: 1466525189:0;knife data bag show container-config iris-staging
: 1466525223:0;knife environment edit iris-staging
: 1466525256:0;shipit env config_list iris-staging  | grep redis
: 1466525282:0;shipit env config_set REDIS_URL redis://iris-staging.teaj7o.ng.0001.use1.cache.amazonaws.com
: 1466525295:0;shipit env config_set iris-staging REDIS_URL redis://iris-staging.teaj7o.ng.0001.use1.cache.amazonaws.com
: 1466525357:0;shipit deployment create iris-staging c53addde55b5dca7ac5d3c4326f49f1916a6b14e
: 1466525568:0;shipit deployment logs 3450
: 1466529484:0;history | grep chef | grep blue
: 1466529692:0;knife ssh -x ubuntu "chef_environment:walkadoo-production  AND role:walkadoo-prod-job-master" "sudo /opt/chef/embedded/bin/bluepill status"
: 1466529718:0;knife search "chef_environment:walkadoo-production"
: 1466529823:0;knife ssh -x ubuntu "chef_environment:walkadoo-production  AND role:hotsauce_job_master" "sudo /opt/chef/embedded/bin/bluepill status"
: 1466529846:0;knife ssh -x ubuntu "chef_environment:walkadoo-production  AND role:hotsauce_job_master AND role:hotsauce_jobs" "sudo /opt/chef/embedded/bin/bluepill status"
: 1466529862:0;knife ssh -x ubuntu "chef_environment:walkadoo-production  AND role:hotsauce_jobs" "sudo /opt/chef/embedded/bin/bluepill status"
: 1466530192:0;ssh ubuntu@ec2-54-146-91-11.compute-1.amazonaws.com
: 1466530320:0;history | grep chef | grep unic
: 1466531430:0;cat script-for-dc-maint-shutdown.sh
: 1466531585:0;cp script-for-dc-maint-shutdown.sh ../walkadoo/script-for-WD-maint-shutdown.sh
: 1466531600:0;cp script-for-dc-startup.sh ../walkadoo/script-for-WD-startup.sh
: 1466531885:0;history | grep walkadoo | grep mysql 
: 1466531901:0;history | grep walkadoo | grep ssh | grep P
: 1466531905:0;history | grep walkadoo | grep ssh | grep L
: 1466532057:0;knife ssh -x ubuntu "chef_environment:walkadoo-production AND role:hotsauce_web" "sudo /opt/chef/embedded/bin/bluepill status"
: 1466533055:0;ssh ubuntu@ec2-54-90-17-51.compute-1.amazonaws.com 
: 1466534347:0;history | grep mysql | grep 9005
: 1466535888:0;history | grep ssh | grep L | grep walkadoo-production2
: 1466538963:0;cp script-for-WD-startup.sh script-for-WD-startup.sh.dc.orig
: 1466538972:0;cp script-for-WD-maint-shutdown.sh script-for-WD-startup.sh
: 1466601149:0;AND role:hotsauce_jobs" "sudo /opt/chef/embedded/bin/bluepill status"
: 1466601153:0;knife ssh -x ubuntu "chef_environment:walkadoo-staging AND role:hotsauce_jobs" "sudo /opt/chef/embedded/bin/bluepill status"
: 1466601524:0;ls -lt *.sh
: 1466601551:0;ls -l *.sh*
: 1466601596:0;cp script-for-WD-startup.sh.dc.orig script-for-WD-chef-startup.sh
: 1466601703:0;cat script-for-WD-startup.sh
: 1466601706:0;vi script-for-WD-startup.sh
: 1466603488:0;knife ssh -x ubuntu 'chef_environment:walkadoo-staging AND role:hotsauce_web' 'sudo /opt/chef/embedded/bin/bluepill help"
: 1466603490:0;knife ssh -x ubuntu 'chef_environment:walkadoo-staging AND role:hotsauce_web' 'sudo /opt/chef/embedded/bin/bluepill help'
: 1466603497:0;knife ssh -x ubuntu 'chef_environment:walkadoo-staging AND role:hotsauce_web' 'sudo /opt/chef/embedded/bin/bluepill --help'
: 1466603591:0;knife ssh -x ubuntu "chef_environment:${ENV} AND role:hotsauce_job_master" "uname -n ; sudo /opt/chef/embedded/bin/bluepill stop hotsauce_clockwork"
: 1466603613:0;knife ssh -x ubuntu "chef_environment:${ENV} AND role:hotsauce_jobs" "uname -n ; sudo /opt/chef/embedded/bin/bluepill stop hotsauce_clockwork"
: 1466603642:0;knife ssh -x ubuntu 'chef_environment:walkadoo-staging AND role:hotsauce_jobs' 'sudo /opt/chef/embedded/bin/bluepill stop hotsauce_clockwork && sudo /opt/chef/embedded/bin/bluepill quit'
: 1466603664:0;knife ssh -x ubuntu 'chef_environment:walkadoo-staging AND role:hotsauce_jobs' 'sudo /opt/chef/embedded/bin/bluepill stop hotsauce_resque'
: 1466603695:0;knife ssh -x ubuntu 'chef_environment:walkadoo-staging AND role:hotsauce_jobs' 'sudo /opt/chef/embedded/bin/bluepill hotsauce_resque stop'
: 1466603711:0;knife ssh -x ubuntu 'chef_environment:walkadoo-staging AND role:hotsauce_jobs' 'sudo /opt/chef/embedded/bin/bluepill hotsauce_clockwork stop'
: 1466603722:0;knife ssh -x ubuntu 'chef_environment:walkadoo-staging AND role:hotsauce_jobs' 'sudo /opt/chef/embedded/bin/bluepill hotsauce_hurby stop'
: 1466603960:0;more /Users/heidischmidt/Documents/walkadoo/script-for-WD-maint-shutdown.sh-walkadoo-staging-20160622-1466603909.log
: 1466603999:0;ls -l *.dc*
: 1466604010:0;more script-for-WD-startup.sh.dc.orig
: 1466604056:0;mv script-for-WD-startup.sh script-for-WD-startup.sh-OOW
: 1466604289:0;rm script-for-WD-chef-startup.sh--20160622-1466604258.log
: 1466604302:0;rm script-for-WD-maint-shutdown.sh-walkadoo-staging-20160622-1466603428.log
: 1466604371:0;grep web script-for-WD-maint-shutdown.sh-walkadoo-staging-20160622-1466603909.log
: 1466604383:0;grep hotsauce script-for-WD-maint-shutdown.sh-walkadoo-staging-20160622-1466603909.log
: 1466604407:0;ls -ltra *.sh
: 1466604444:0;cat hotsauce_web
: 1466604476:0;knife ssh -x ubuntu "chef_environment:${ENV} AND role:hotsauce_web" "sudo rm /data/hotsuace/current/public/system/maintenance.html"
: 1466604814:0;bash -xv script-for-WD-chef-startup.sh
: 1466604865:0;knife search chef_environment:walkadoo-staging > list-staging
: 1466604871:0;more list-staging
: 1466604888:0;bash -xv script-for-WD-chef-startup.sh walkadoo-staging
: 1466604900:0;rm script-for-WD-chef-startup.sh--20160622-1466604814.log
: 1466607056:0;more script-for-WD-chef-startup.sh-walkadoo-staging-20160622-1466604277.log
: 1466608476:0;history | grep concierge
: 1466608526:0;history | grep 5432
: 1466608692:0;history | grep psql | grep 9999
: 1466608776:0;ssh -A -L 9999:localhost:9999 ubuntu@52.86.252.208 ssh -L 9999:concierge-staging.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:5432 -N ubuntu@10.0.14.127
: 1466608806:0;ssh -A -L 9990:localhost:9990 ubuntu@52.86.252.208 ssh -L 9990:concierge-staging.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:5432 -N ubuntu@10.0.14.127
: 1466608899:0;ssh -A -L 9990:localhost:9990 ubuntu@52.86.252.208 ssh -L 9990:concierge-staging.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:5432 -N heidischmidt@10.0.14.127
: 1466608966:0;ls -lrta *.sh
: 1466608991:0;vi /opt/chef/embedded/bin/bluepill
: 1466609033:0;more script-for-WD-maint-shutdown.sh-walkadoo-staging-20160622-1466603909.log
: 1466609169:0;more ../dc/*maint*.sh
: 1466609434:0;grep -v hotsauce script-for-WD-maint-shutdown.sh
: 1466609448:0;grep -v walkadoo script-for-WD-maint-shutdown.sh
: 1466609482:0;ls -ltra *.sh 
: 1466609521:0;grep maintenance.html script-for-WD-maint-shutdown.sh
: 1466609533:0;grep maintenance.html script-for-WD-chef-startup.sh
: 1466609572:0;mkdir logs/
: 1466609593:0;bash -xv script-for-WD-maint-shutdown.sh walkadoo-staging 
: 1466609942:0;bash -xv script-for-WD-maint-shutdown.sh walkadoo-staging
: 1466610023:0;bash -xv script-for-WD-chef-startup.sh walkadoo-staging 
: 1466610663:0;more script-for-WD-chef-startup.sh-walkadoo-staging-20160622-1466610023.log
: 1466610754:0;knife ssh -x ubuntu "chef_environment:${ENV} AND role:hotsauce_web" "sudo rm /data/hotsauce/current/public/system/maintenance.html"
: 1466610830:0;knife ssh -x ubuntu "chef_environment:walkadoo-staging AND role:hotsauce_web" "sudo rm /data/hotsauce/current/public/system/maintenance.html"
: 1466610868:0;grep maint script-for-WD-maint-shutdown.sh
: 1466610955:0;knife ssh -x ubuntu "chef_environment:walkadoo-staging AND role:hotsauce_web" "sudo ls -l /data/hotsauce/current/public/"
: 1466610964:0;knife ssh -x ubuntu "chef_environment:walkadoo-staging AND role:hotsauce_web" "sudo ls -l /data/hotsauce/current/public/maint*"
: 1466610989:0;knife ssh -x ubuntu "chef_environment:walkadoo-staging AND role:hotsauce_web" "sudo rm /data/hotsauce/current/public/maintenance.html"
: 1466611336:0;ls -l *.sh 
: 1466611359:0;grep "/data/hotsauce/current/public/maintenance.html" script-for-WD-maint-shutdown.sh
: 1466611493:0;grep "/data/hotsauce/current/public/maintenance.html" script-for-WD-chef-startup.sh
: 1466611568:0;vi script-for-WD-maint-shutdown.sh 
: 1466611619:0;grep "/data/hotsauce/current/public/" script-for-WD-chef-startup.sh
: 1466611639:0;grep script-for-WD-chef-startup.sh script-for-WD-maint-shutdown.sh
: 1466611664:0;grep /data/hotsauce/current/public/system/maintenance.html script-for-WD-maint-shutdown.sh
: 1466611669:0;grep /data/hotsauce/current/public/system/maintenance.html script-for-WD-chef-startup.sh
: 1466616758:0;cat script-for-WD-chef-startup.sh
: 1466616769:0;more script-for-WD-maint-shutdown.sh
: 1466616823:0;more script-for-WD-maint-shutdown.sh-walkadoo-staging-20160622-1466609593.log
: 1466616857:0;ls-l *.sh
: 1466616878:0;cat script-for-WD-maint-shutdown.sh
: 1466616890:0;vi script-for-WD-maint-shutdown.sh
: 1466617009:0;ls -ltrah *.sh
: 1466617014:0;vi script-for-WD-chef-startup.sh
: 1466617246:0;ssh -A -L 9005:walkadoo-production.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 heidischmidt@ec2-54-90-17-51.compute-1.amazonaws.com -N 2>&1 & 
: 1466617267:0;	mysql -u meyouhealth -p -P 9005 -h 127.0.0.1 -e "show master status; status;" >  ~/Documents/walkadoo/checking_master_status_wd_prod_rds_ec2.log
: 1466617289:0;more ~/Documents/walkadoo/checking_master_status_wd_prod_rds_ec2.log
: 1466617340:0;	mysql -u meyouhealth -p -P 9005 -h 127.0.0.1 -e "select id, user, host, db, state, now() from information_schema.processlist; status;" >  ~/Documents/walkadoo/checking_process_list_wd_prod_rds_ec2.log
: 1466617356:0;more ~/Documents/walkadoo/checking_process_list_wd_prod_rds_ec2.log
: 1466617390:0;grep date *.sh 
: 1466617417:0;	mysql -u meyouhealth -p -P 9005 -h 127.0.0.1 -e "select id, user, host, db, state, now() from information_schema.processlist; status;" >  ~/Documents/walkadoo/checking_process_list_wd_prod_rds_ec2-`date +"%Y%m%d-%s"`.log
: 1466617432:0;more ~/Documents/walkadoo/checking_process_list_wd_prod_rds_ec2-20160622-1466617417.log
: 1466617562:0;	ssh -A -L 7005:walkadoo-production2.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 heidischmidt@ec2-54-90-17-51.compute-1.amazonaws.com -N 2>&1 & 
: 1466617576:0;	mysql -u meyouhealth -p -P 7005 -h 127.0.0.1 -e "show master status; show slave status \G status;" >  ~/Documents/walkadoo/checking_master_status_wd_prod_VPC_RDS-`date +"%Y%m%d-%s"`.log
: 1466617593:0;more ~/Documents/walkadoo/checking_master_status_wd_prod_VPC_RDS-20160622-1466617576.log
: 1466617672:0;vi check_RDS_EC2_connections.sh 
: 1466617756:0;vi check_RDS_VPC_NEW_connections.sh 
: 1466617795:0;rm checking_master_status_wd_prod_rds_ec2.log checking_process_list_wd_prod_rds_ec2.log checking_process_list_wd_prod_rds_ec2-20160622-1466617417.log checking_master_status_wd_prod_VPC_RDS-20160622-1466617576.log
: 1466617817:0;vi ~/.my.cnf
: 1466617879:0;vi check*RDS*connections.sh
: 1466617914:0;more ~/Documents/walkadoo/checking_master_status_wd_prod_RDS_EC2-20160622-1466617907.log
: 1466617936:0;mysql -u meyouhealth -P 9005 -h 127.0.0.1 -e "select id, user, host, db, state, now() from information_schema.processlist; status;" >  ~/Documents/walkadoo/checking_process_list_wd_prod_rds_ec2-`date +"%Y%m%d-%s"`.log
: 1466617943:0;more ~/Documents/walkadoo/checking_process_list_wd_prod_rds_ec2-
: 1466617945:0;more ~/Documents/walkadoo/checking_process_list_wd_prod_rds_ec2-20160622-1466617936.log
: 1466617962:0;rm checking_process_list_wd_prod_rds_ec2-20160622-1466617936.log
: 1466617968:0;rm checking_master_status_wd_prod_RDS_EC2-20160622-1466617907.log
: 1466618014:0;cat check_RDS_EC2_connections.sh
: 1466618026:0;mysql -u meyouhealth -P 9005 -h 127.0.0.1 -e "select id, user, host, db, state, now() from information_schema.processlist; status;" > ~/Documents/walkadoo/checking_process_list_wd_prod_RDS_EC2-`date +"%Y%m%d-%s"`.log
: 1466618036:0;more check_RDS_VPC_NEW_connections.sh
: 1466618044:0;mysql -u meyouhealth -P 7005 -h 127.0.0.1 -e "select id, user, host, db, state, now() from information_schema.processlist; status;" >  ~/Documents/walkadoo/checking_process_list_wd_prod_VPC_RDS_NEW-`date +"%Y%m%d-%s"`.log
: 1466618051:0;more checking_process_list_wd_prod_VPC_RDS_NEW-20160622-1466618044.log
: 1466618066:0;rm checking_process_list_wd_prod_VPC_RDS_NEW-20160622-1466618044.log
: 1466618071:0;rm checking_process_list_wd_prod_RDS_EC2-20160622-1466618026.log
: 1466618075:0;vi check_RDS_VPC_NEW_connections.sh
: 1466618110:0;vi check_RDS_EC2_connections.sh
: 1466618144:0;hisotry | grep knife | grep edit
: 1466618152:0;history | grep knife | grep edit
: 1466618162:0;knife environment edit walkadoo-staging
: 1466618188:0;knife environment show walkadoo-staging --format json
: 1466618210:0;vi knife_edit_env_wd.sh
: 1466618345:0;bash -xv knife_edit_env_wd.sh walkadoo-staging 
: 1466618356:0;more /Users/heidischmidt/Documents/walkadoo/knife_env_show_walkadoo-staging_20160622-1466618345.log
: 1466618382:0;mv knife_edit_env_wd.sh knife_show_wd_env.sh 
: 1466618409:0;vi knife_show_wd_env.sh
: 1466618466:0;bash -xv knife_show_wd_env.sh
: 1466618479:0;bash -xv knife_show_wd_env.sh walkadoo-staging
: 1466618488:0;rm knife_env_show__20160622-1466618466.log
: 1466618526:0;grep -i database knife_env_show_walkadoo-staging_20160622-1466618479.log
: 1466618536:0;grep walkadoo- knife_env_show_walkadoo-staging_20160622-1466618479.log
: 1466618554:0;grep host knife_env_show_walkadoo-staging_20160622-1466618479.log
: 1466618574:0;grep cduvhvvpw6oo knife_env_show_walkadoo-staging_20160622-1466618479.log
: 1466618610:0;mv list list-production
: 1466618612:0;more list-production
: 1466618624:0;mv list-production list-shipit-vars
: 1466618645:0;mv script-for-WD-maint-shutdown.sh Step1-script-for-WD-maint-shutdown.sh
: 1466618663:0;more check_RDS_EC2_connections.sh
: 1466618676:0;mv check_RDS_EC2_connections.sh Step2-check_RDS_EC2_connections.sh
: 1466618722:0;mv knife_show_wd_env.sh Step3-knife_show_wd_env.sh
: 1466618730:0;more script-for-WD-chef-startup.sh
: 1466618752:0;mv script-for-WD-chef-startup.sh Step4-script-for-WD-chef-startup.sh
: 1466618771:0;mv check_RDS_VPC_NEW_connections.sh Step5-check_RDS_VPC_NEW_connections.sh
: 1466618818:0;more checking_process_list_wd_prod_RDS_EC2-20160622-1466618803.log
: 1466618831:0;more checking_master_status_wd_prod_RDS_EC2-20160622-1466618803.log
: 1466618921:0;cd ../ShipIt
: 1466618928:0;more shipit_list
: 1466618965:0;mv shipit_list shipit_list_old
: 1466618970:0;shipit env list > shipit_list
: 1466619061:0;for i in `cat shipit_list` \
do \
shipit env config_get ${i} REDIS_URL > ${i}_redis_url_check_`date +"%Y%m%d-%s"`.log\
done
: 1466619314:0;grep REDIS *redis*url*check*log
: 1466619354:0;grep -i redis *redis_url_check_20160622ZZ*.log
: 1466619370:0;more concierge-production_redis_url_check_20160622-1466619061.log
: 1466619402:0;grep -i 6379 *redis_url_check_20160*.log
: 1466619416:0;grep -i 6379 *redis_url_check_20160*.log | grep ng
: 1466619423:0;grep -i 6379 *redis_url_check_20160*.log | grep -v ng
: 1466619483:0;grep -i 6379 *redis_url_check_20160*.log | grep -v ng | sort
: 1466622195:0;cat pg_grants_hello200_prod.sql
: 1466622240:0;cp pg_grants_hello200_prod.sql pg_grants_concierge_prod.sql
: 1466623147:0;cat pg_grants_concierge_prod.sql
: 1466623370:0;vi pg_grants_concierge_prod.sql
: 1466687239:0;kill %1 
: 1466687524:0;rbenv 2.1.5 
: 1466687546:0;shipit env config_list walkadoo_staging | grep HES
: 1466687560:0;shipit env config_list walkadoo-staging | grep -i hes
: 1466687570:0;shipit env config_list walkadoo-production | grep -i hes
: 1466690204:0;bash -xv Step1-script-for-WD-maint-shutdown.sh
: 1466690240:0;rm Step1-script-for-WD-maint-shutdown.sh--20160623-1466690204.log
: 1466690246:0;bash -xv Step1-script-for-WD-maint-shutdown.sh walkadoo-staging
: 1466690312:0;more Step1-script-for-WD-maint-shutdown.sh-walkadoo-staging-20160623-1466690246.log
: 1466690353:0;ssh -A -L 9005:walkadoo-production.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 heidischmidt@ec2-54-90-17-51.compute-1.amazonaws.com -N 2>&1 &
: 1466690369:0;ssh -A -L 9004:walkadoo-production.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 heidischmidt@ec2-54-90-17-51.compute-1.amazonaws.com -N 2>&1 &
: 1466690390:0;mysql -u meyouhealth -P 9004 -h 127.0.0.1 -e "select @@hostname; show master status; status;" >  ~/Documents/walkadoo/checking_master_status_wd_prod_RDS_EC2-`date +"%Y%m%d-%s"`.log
: 1466690409:0;mysql -u meyouhealth -P 9004 -h 127.0.0.1 -e "select @@hostname; select id, user, host, db, state, now() from information_schema.processlist; status;" > ~/Documents/walkadoo/checking_process_list_wd_prod_RDS_EC2-`date +"%Y%m%d-%s"`.log
: 1466690424:0;rm checking_process_list_wd_prod_RDS_EC2-20160623-1466690340.log checking_master_status_wd_prod_RDS_EC2-20160623-1466690340.log
: 1466690433:0;more checking_master_status_wd_prod_RDS_EC2-20160623-1466690390.log 
: 1466690436:0;more checking_process_list_wd_prod_RDS_EC2-20160623-1466690409.log
: 1466690449:0;bash -xv Step3-knife_show_wd_env.sh
: 1466690467:0;rm knife_env_show__20160623-1466690449.log
: 1466690474:0;bash -xv Step3-knife_show_wd_env.sh walkadoo-stagin
: 1466690477:0;bash -xv Step3-knife_show_wd_env.sh walkadoo-staging
: 1466690485:0;rm knife_env_show_walkadoo-stagin_20160623-1466690474.log
: 1466690504:0;bash -xv Step4-script-for-WD-chef-startup.sh
: 1466690536:0;bash -xv Step4-script-for-WD-chef-startup.sh walkadoo-staging
: 1466690739:0;more Step4-script-for-WD-chef-startup.sh-walkadoo-staging-20160623-1466690536.log
: 1466690769:0;rm Step4-script-for-WD-chef-startup.sh--20160623-1466690504.log
: 1466692940:0;git clone git@github.com:meyouhealth/chef-dailychallenge.git
: 1466693006:0;cd chef-dailychallenge/templates
: 1466693010:0;cd default
: 1466693019:0;more nginx-site.erb
: 1466693215:0;vi nginx-site.erb
: 1466693371:0;git add templates/default/nginx-site.erb
: 1466693460:0;vi templates/default/nginx-site.erb
: 1466695674:0;berks install
: 1466695694:0;berks upload 
: 1466695802:0;berks verify 
: 1466700093:0;ls -l *run*
: 1466700097:0;mv *run* logs/
: 1466700158:0;ls =l
: 1466700188:0;bash -xv Step2-check_RDS_EC2_connections.sh
: 1466700201:0;more ~/Documents/walkadoo/checking_process_list_wd_prod_RDS_EC2-20160623-1466700189.log
: 1466700236:0;vi Step5-check_RDS_VPC_NEW_connections.sh
: 1466700254:0;bash -xv Step5-check_RDS_VPC_NEW_connections.sh
: 1466700276:0;ssh -A -L 7015:walkadoo-production2.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 heidischmidt@ec2-54-90-17-51.compute-1.amazonaws.com -N 2>&1 &
: 1466700285:0;mysql -u meyouhealth -P 7015 -h 127.0.0.1 -e "select @@hostname; show master status; show slave status \G status;"
: 1466700309:0;mysql -u meyouhealth -P 7015 -h 127.0.0.1 -e 'select @@hostname; select id, user, host, db, state, now() from information_schema.processlist; status;'
: 1466700332:0;rm checking_process_list_wd_prod_VPC_RDS_NEW-20160623-1466700217.log checking_master_status_wd_prod_VPC_RDS_NEW-20160623-1466700217.log checking_process_list_wd_prod_VPC_RDS_NEW-20160623-1466700254.log checking_master_status_wd_prod_VPC_RDS_NEW-20160623-1466700254.log
: 1466700454:0;more checking_master_status_wd_prod_RDS_EC2-20160623-1466700409.log
: 1466700458:0;checking_process_list_wd_prod_RDS_EC2-20160623-1466700409.log
: 1466700461:0;more checking_process_list_wd_prod_RDS_EC2-20160623-1466700409.log
: 1466700468:0;more checking_process_list_wd_prod_VPC_RDS_NEW-20160623-1466700430.log
: 1466700475:0;more checking_master_status_wd_prod_VPC_RDS_NEW-20160623-1466700429.log
: 1466700509:0;bash Step3-knife_show_wd_env.sh walkadoo-staging
: 1466700519:0;bash Step3-knife_show_wd_env.sh walkadoo-production
: 1466700542:0;vi knife_env_show_walkadoo-production_20160623-1466700519.log
: 1466700567:0;vi knife_env_show_walkadoo-staging_20160623-1466700509.log
: 1466700846:0;bash Step4-script-for-WD-chef-startup.sh 
: 1466700857:0;bash Step4-script-for-WD-chef-startup.sh walkadoo-staging
: 1466702044:0;vi Step4-script-for-WD-chef-startup.sh--20160623-1466700846.log
: 1466702057:0;rm Step4-script-for-WD-chef-startup.sh--20160623-1466700846.log
: 1466702194:0;rm Step1-script-for-WD-maint-shutdown.sh--20160623-1466702179.log
: 1466702211:0;bash Step1-script-for-WD-maint-shutdown.sh
: 1466702227:0;rm Step1-script-for-WD-maint-shutdown.sh--20160623-1466702211.log
: 1466702237:0;bash Step1-script-for-WD-maint-shutdown.sh walkadoo-staging
: 1466702302:0;history | grep while
: 1466702334:0;while true ; do echo "$(date)" ; sleep 360; echo "$(date)"; done 
: 1466702734:0;bash Step4-script-for-WD-chef-startup.sh walkadoo-staging 
: 1466703195:0;ls -l Step5*
: 1466703242:0;knife ssh -x ubuntu "chef_environment:walkadoo-staging AND role:hotsauce_job_master" "sudo chef-client"
: 1466703306:0;vi Step4-script-for-WD-chef-startup.sh-walkadoo-staging-20160623-1466702734.log
: 1466703417:0;grep maint Step1-script-for-WD-maint-shutdown.sh
: 1466703448:0;vi Step1-script-for-WD-maint-shutdown.sh-walkadoo-staging-20160623-1466702237.log
: 1466703662:0;knife ssh -x ubuntu "chef_environment:walkadoo-staging AND role:hotsauce_jobs" "sudo chef-client"
: 1466703759:0;export ENV=walkadoo-staging
: 1466782496:0;ls -l cred*
: 1466782505:0;grep JPVQ cred*
: 1466782518:0;cat credentials (2).csv
: 1466782536:0;mv "credentials (2).csv" credentials2.csv
: 1466782542:0;cat credentials2.csv
: 1466783783:0;cat ~/.ssh/config
: 1466796504:0;history | grep shipit | grep ami
: 1466796518:0;which packer
: 1466796564:0;history > packer-build-notes.txt
: 1466796575:0;vi packer-build-notes.txt
: 1466796640:0;;gidr
: 1466796677:0;berks install && berks vendor cookbooks
: 1466796966:0;cd packer
: 1466796983:0;history | grep packer | grep build
: 1466796994:0;more shipit-ami.template
: 1466797012:0;history | grep packer | grep build | grep tempalte
: 1466797014:0;history | grep packer | grep build | grep temp
: 1466797042:0;packer build -var 'source_ami=ami-8cda19e1'  -var 'lsb_release=14.0.4' shipit-ami.template
: 1466800081:0;grep -i ami *
: 1466804519:0;cd ../walkdadoo
: 1466804531:0;more Step3-knife_show_wd_env.sh
: 1466804564:0;more knife_env_show_walkadoo-production_20160623-1466700519.log
: 1466804642:0;more Step2-check_RDS_EC2_connections.sh
: 1466804652:0;bash Step2-check_RDS_EC2_connections.sh
: 1466804673:0;rm checking_process_list_wd_prod_RDS_EC2-20160624-1466804652.log checking_master_status_wd_prod_RDS_EC2-20160624-1466804652.log
: 1466804689:0;bash Step5-check_RDS_VPC_NEW_connections.sh
: 1466804705:0;rm checking_process_list_wd_prod_VPC_RDS_NEW-20160624-1466804689.log checking_master_status_wd_prod_VPC_RDS_NEW-20160624-1466804689.log
: 1466804724:0;more checking_master_status_wd_prod_VPC_RDS_NEW-20160624-1466804691.log
: 1466804751:0;more checking_process_list_wd_prod_RDS_EC2-20160624-1466804656.log | grep app | wc -l 
: 1466804754:0;more checking_process_list_wd_prod_RDS_EC2-20160624-1466804656.log | grep app 
: 1466805367:0;grep maint Step4-script-for-WD-chef-startup.sh
: 1466805743:0;more checking_process_list_wd_prod_VPC_RDS_NEW-20160624-1466804691.log
: 1466805895:0;history | grep mysql | grep 7005
: 1466805906:0;more checking_process_list_wd_prod_RDS_EC2-20160624-1466804656.log
: 1466818211:0;mysql -u meyouhealth -P 9005 -h 127.0.0.1
: 1466818913:0;cat Step1-script-for-WD-maint-shutdown.sh
: 1466819180:0;ls -l Step*.sh
: 1466819193:0;cat Step3-knife_show_wd_env.sh
: 1466819252:0;ls -ltra Step4-script-for-WD-chef-startup.sh
: 1466819297:0;history | grep knife | grep search | grep walkadoo-production
: 1466819320:0;knife search chef_environment:walkadoo-production > knife-env-walkadoo-production-fri-948pm.txt
: 1466819333:0;cat knife-env-walkadoo-production-fri-948pm.txt
: 1466819380:0;cat Step5-check_RDS_VPC_NEW_connections.sh
: 1466819412:0;ssh ubuntu@54.196.178.138
: 1466819443:0;ssh ubuntu@54.146.91.11
: 1466820556:0;vi Step4-script-for-WD-chef-startup.sh
: 1466823099:0;bash Step3-knife_show_wd_env.sh walkadoo-production 
: 1466823379:0;echo "See purple window" 
: 1466823828:0;bash Step1-script-for-WD-maint-shutdown.sh walkadoo-production 
: 1466823996:0;grep ip-10-155-49-39 /Users/heidischmidt/Documents/walkadoo/knife_env_show_walkadoo-production_20160624-1466823099.log
: 1466824008:0;vi /Users/heidischmidt/Documents/walkadoo/knife_env_show_walkadoo-production_20160624-1466823099.log
: 1466824046:0;grep ip-10-155-49-39 knife-env-walkadoo-production-fri-948pm.txt
: 1466824446:0;knife ssh -x ubuntu "chef_environment:${ENV} AND role:hotsauce_web" "uname -n ; sudo cp /data/hotsauce/current/public/maintenance.html /data/hotsauce/current/public/system/maintenance.html"
: 1466824763:0;knife ssh -x ubuntu "chef_environment:${ENV} AND role:hotsauce_job_master" "uname -n ; sudo /opt/chef/embedded/bin/bluepill hotsauce_clockwork stop && sudo /opt/chef/embedded/bin/bluepill hotsauce_clockwork quit"  | tee -a ${LOGFILE}
: 1466824793:0;knife ssh -x ubuntu "chef_environment:${ENV} AND role:hotsauce_job_master" "uname -n ; ps -ef | egrep 'resq|blue|Hurby'"
: 1466824859:0;exho $ENV
: 1466824881:0;export ENV=walkadoo-production
: 1466824936:0;ls -l Step1*
: 1466824948:0;more Step1-script-for-WD-maint-shutdown.sh-walkadoo-staging-20160623-1466702237.log
: 1466824984:0;vi Step1-script-for-WD-maint-shutdown.sh
: 1466825039:0;bash Step1-script-for-WD-maint-shutdown.sh walkadoo-production
: 1466825081:0;mysql -u meyouhealth -P 9005 -h 127.0.0.1 -e "select @@hostname; show master status; status;" >  ~/Documents/walkadoo/checking_master_status_wd_prod_RDS_EC2-`date +"%Y%m%d-%s"`.log
: 1466825088:0;more ~/Documents/walkadoo/checking_master_status_wd_prod_RDS_EC2-20160624-14668
: 1466825091:0;more ~/Documents/walkadoo/checking_master_status_wd_prod_RDS_EC2-20160624-14668.log
: 1466825100:0;more checking_master_status_wd_prod_RDS_EC2-20160624-1466825081.log
: 1466825108:0;cat Step2-check_RDS_EC2_connections.sh
: 1466825117:0;mysql -u meyouhealth -P 9005 -h 127.0.0.1 -e "select @@hostname; select id, user, host, db, state, now() from information_schema.processlist; status;" > ~/Documents/walkadoo/checking_process_list_wd_prod_RDS_EC2-`date +"%Y%m%d-%s"`.log
: 1466825123:0;more checking_process_list_wd_prod_RDS_EC2-20160624-1466825117.log
: 1466825273:0;knife environment show walkadoo-production | grep walkadoo-production
: 1466825819:0;knife environment show walkadoo-production | grep walkadoo_prd_app
: 1466825826:0;knife environment show walkadoo-production 
: 1466825841:0;knife environment edit walkadoo-production 
: 1466826102:0;mysql -u walkadoo_prod_app -p  -P 7015 -h 127.0.0.1
: 1466826115:0;mysql -u walkadoo_prd_app -p  -P 7015 -h 127.0.0.1
: 1466826218:0;mysql -u meyouhealth -P 7015 -h 127.0.0.1
: 1466826855:0;ls -ltra Step*
: 1466826857:0;ls -ltra Step*.sh
: 1466826976:0;bash  Step4-script-for-WD-chef-startup.sh walkadoo-production 
: 1466827131:0;history | grep ssh | grep L | grep walkadoo-production
: 1466827175:0;history | grep mysql | tail -4
: 1466827266:0;mysql -u meyouhealth -P 7015 -h 127.0.0.1 -e "select @@hostname; show master status; show slave status \G status;" >  ~/Documents/walkadoo/checking_master_status_wd_prod_VPC_RDS_NEW-`date +"%Y%m%d-%s"`.log 
: 1466827346:0;echo
: 1466827563:0;knife ssh -x ubuntu "chef_environment:walkadoo-production AND role:hotsauce_web" "sudo rm /data/hotsauce/current/public/system/maintenance.html"
: 1466828261:0;echo $ENV
: 1466828271:0;unset ENV
: 1466828618:0;ls -ltra *.lgo
: 1466828621:0;ls -ltra *.log
: 1467032023:0;history | grep ssh | grep dc-prod
: 1467035841:0;cd environments/production/walkadoo
: 1467041643:0;cd ../../production
: 1467041682:0;grep ami variables.tf
: 1467041697:0;grep ami ../../staging/quitnet/variables.tf
: 1467051239:0;cd .ssh/
: 1467052044:0;cp cd ~/Documents/git
: 1467052547:0;more ../walkadoo/variables.tf| grep db
: 1467052552:0;more ../walkadoo/variables.tf| grep database
: 1467113079:0;ssh heidischmidt@On branch master
: 1467113085:0;On branch master
: 1467113106:0;ssh heidschmidt@ec2-54-225-238-195.compute-1.amazonaws.com
: 1467121944:0;knife search chef_environment:quitnet-staging > knife-env-quitnet-staging-b4-patching.txt
: 1467127208:0;cd environments/staging/quitnet
: 1467127280:0;grep -i ami variables.tf
: 1467127298:0;grep -i ami ../../production/quitnet/variables.tf
: 1467127409:0;more knife-env-quitnet-staging-b4-patching.txt
: 1467127433:0;grep "Node Name" knife-env-quitnet-staging-b4-patching.txt
: 1467128202:0;ssh -A -L 7001:walkadoo-production2-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 heidischmidt@ec2-54-90-17-51.compute-1.amazonaws.com -N
: 1467128277:0;ssh -A -L 7001:walkadoo-production2.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 heidischmidt@ec2-54-90-17-51.compute-1.amazonaws.com -N
: 1467128464:0;ls -l mysql
: 1467128504:0;find . -name "*" -print -exec grep reporting_prod_app {} \;
: 1467128777:0;mysql -u meyouhealth -P 7002 -h 127.0.0.1 -p 
: 1467128870:0;ssh -A -L 9999:127.0.0.1:9999 ubuntu@54.152.155.24 ssh -L 9999:wbt-production2.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N ubuntu@10.0.14.248
: 1467129649:0;mysql -u report_prd_app -p -P 7001 -h 127.0.0.1 
: 1467137734:0;mysql -u meyouhealth -P 7001 -h 127.0.0.1 
: 1467138695:0;history | grep 7001
: 1467138820:0;mysql -u report_prd_app -p -P 7002 -h 127.0.0.1
: 1467139133:0;mysql -u meyouhealth -P 7001 -h 127.0.0.1
: 1467139241:0;mysql -u meyouhealth -P 7002 -h 127.0.0.1
: 1467139401:0;git commit rds.tf
: 1467139709:0;git checkout ~/Documents/git/myh-terraform/environments/production/walkadoo/terraform.tfstate
: 1467139744:0;mv terraform.tfstate terraform.tfstate.dc.new.prod2
: 1467139859:0;git checkout environments/production/concierge/ec2.tf
: 1467139867:0;git checkout environments/production/concierge/terraform.tfstate
: 1467139874:0;git checkout environments/production/dailychallenge/terraform.tfstate
: 1467139883:0;git checkout environments/production/insight/terraform.tfstate
: 1467139889:0;git checkout environments/production/walkadoo/ec2.tf
: 1467139897:0;git checkout environments/production/walkadoo/terraform.tfstate
: 1467139906:0;git checkout environments/production/wellbeingtracker/ec2.tf
: 1467139911:0;git checkout environments/production/wellbeingtracker/terraform.tfstate
: 1467139917:0;git checkout environments/production/wilder/ec2.tf
: 1467139925:0;git checkout environments/production/wilder/terraform.tfstate
: 1467140003:0;mv myh-terraform myh-terraform.oops
: 1467140057:0;history | grep git | grep myh-ter
: 1467140065:0;git clone git@github.com:meyouhealth/myh-terraform.git
: 1467140126:0;cp ~/Documents/git/myh-terraform.oops/environments/production/dailychallenge/variables.tf .
: 1467140134:0;cp ~/Documents/git/myh-terraform.oops/environments/production/dailychallenge/terraform.tfvars .
: 1467140168:0;cp ~/Documents/git/myh-terraform.oops/environments/production/dailychallenge/rds.tf .
: 1467140199:0;cp ~/Documents/git/myh-terraform.oops/environments/production/dailychallenge/terraform.tfstate.dc.new.prod2 .
: 1467140214:0;diff -u terraform.tfstate terraform.tfstate.dc.new.prod2
: 1467140234:0;cp terraform.tfstate terraform.tfstate.git
: 1467140273:0;cp terraform.tfstate.git terraform.tfstate
: 1467140309:0;cp terraform.tfstate.dc.new.prod2 terraform.tfstate
: 1467140324:0;rm terraform.tfstate.dc.new.prod2
: 1467140334:0;rm terraform.tfstate.*
: 1467140352:0;rm terraform.tfstate
: 1467142383:0;ls -l Step*
: 1467142394:0;more Step4-script-for-WD-chef-startup.sh
: 1467142467:0;ssh ubuntu@ec2-54-226-66-54.compute-1.amazonaws.com
: 1467142508:0;knife ssh -x ubuntu "chef_environment:walkadoo-staging AND role:hotsauce_web" "ls -l /data/hotsauce/current/public/system/"
: 1467142522:0;cat Step4-script-for-WD-chef-startup.sh
: 1467142543:0;knife ssh -x ubuntu "chef_environment:walkadoo-staging AND role:hotsauce_jobs" "uname -n ; ps -ef | egrep 'resq|blue|Hurby'"
: 1467142569:0;knife ssh -x ubuntu "chef_environment:walkadoo-staging AND role:hotsauce_job_master" "uname -n ; ps -ef | egrep 'resq|blue|Hurby'"
: 1467142591:0;knife ssh -x ubuntu "chef_environment:walkadoo-staging AND role:hotsauce_web" "uname -n ; ps -ef | egrep 'unicorn|resq|blue|Hurby'"
: 1467143115:0;ls -l *mysql*dump*
: 1467143150:0;find . -name "*mysqldump*" -print 
: 1467143161:0;ls -l mysqldumps
: 1467143182:0;more ./walkadoo/mysqldump_schema.sh
: 1467144728:0;ssh -A -L 9999:127.0.0.1:9999 ubuntu@54.152.155.24 ssh -L 9999:dc-production2.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N ubuntu@10.159.160.235
: 1467144777:0;ssh -A -L 9999:dc-production2.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 heidischmidt@ec2-54-242-121-10.compute-1.amazonaws.com
: 1467144791:0;ssh -A -L 9999:dc-production2.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 heidischmidt@ec2-54-242-121-10.compute-1.amazonaws.com -N 
: 1467144826:0;mysql -u meyouhealth -P 9999 -h 127.0.0.1 -p 
: 1467145327:0;mysql -u meyouhealth -P 9999 -h 127.0.0.1 -poLEs5h9tlgyWEA
: 1467208258:0;ls -l *knife*
: 1467208265:0;more knife-env-walkadoo-staging.042016.txt
: 1467209425:0;history | grep aws | grep cli
: 1467209443:0;aws dms describe-endpoints --generate-cli-skeleton
: 1467209462:0;aws dms describe-replication-tasks  --generate-cli-skeleton
: 1467209537:0;cd dms
: 1467209539:0;ls -ls
: 1467209546:0;cd aws_
: 1467209566:0;;lla
: 1467209600:0;aws dms describe-replication-instances > aws-dms-describe-replication-instances-dw.txt
: 1467209634:0;aws dms describe-refresh-schemas-status
: 1467209693:0;aws dms describe-refresh-schemas-status --endpoint-arn arn:aws:dms:us-east-1:732800507661:rep:FTSS5Q5JCHXE7T22VAIDJDQDSI
: 1467210685:0;aws dms describe-refresh-schemas-status --endpoint-arn arn:aws:dms:us-east-1:732800507661:task:ZKW4C6VR2YTHUUDMG3NWHP4THY
: 1467210714:0;aws dms describe-refresh-schemas-status --endpoint-arn arn:aws:dms:us-east-1:732800507661:endpoint:2HBMLHXPSNMCZUTNT6C72LAWHQ
: 1467210768:0;aws dms describe-refresh-schemas-status --endpoint-arn arn:aws:dms:us-east-1:732800507661:endpoint:2HBMLHXPSNMCZUTNT6C72LAWHQ > aws-describe-refresh-schemas-status---endpoint-arn-walkadoo.txt
: 1467210807:0;aws dms help describe-orderable-replication-instances
: 1467210821:0;aws dms describe-orderable-replication-instances help
: 1467211115:0;ls ../daily*
: 1467212366:0;grep table *.*
: 1467220424:0;grep -i autossh *
: 1467221647:0;more .my.cnf 
: 1467221853:0;ls -l knife*
: 1467221858:0;cat knife-env-walkadoo-staging.042016.txt
: 1467221916:0;shipit env config_list walkadoo-staging > shipit-wd-staging-env-062916.txt
: 1467222197:0;grep -i smarty shipit-wd-staging-env-062916.txt
: 1467222218:0;shipit env config_list walkadoo-production > shipit-wd-prod-env-062916.txt
: 1467222226:0;grep -i smart shipit-wd-prod-env-062916.txt
: 1467222245:0;cat shipit-wd-staging-env-062916.txt > ordered_shipit-wd-staging-env-062916.txt
: 1467222275:0;cat shipit-wd-staging-env-062916.txt | sort > ordered_shipit-wd-staging-env-062916.txt
: 1467222290:0;cat shipit-wd-prod-env-062916.txt | sort > ordered_shipit-wd-prod-env-062916.txt
: 1467222298:0;paste ordered_shipit-wd-staging-env-062916.txt ordered_shipit-wd-prod-env-062916.txt
: 1467223040:0;grep DATA ordered_shipit-wd-staging-env-062916.txt
: 1467223069:0;shipit env walkadoo-staging config_unset DATABASE_PASSWROD 
: 1467223102:0;shipit env  config_unset walkadoo-staging  DATABASE_PASSWROD 
: 1467223118:0;grep DATA ordered_shipit-wd-prod-env-062916.txt
: 1467223152:0;shipit env config_set walkadoo-staging DATABASE_PASSWORD LBay1GTZgD081Y
: 1467223179:0;shipit env config_set walkadoo-production DATABASE_PASSWORD tje7mnEOkp77k5
: 1467223197:0;history | grep ordered
: 1467223224:0;shipit env config_list walkadoo-staging | sort > ordered_shipit-wd-staging-env-062916.txt
: 1467223265:0;shipit env  config_unset walkadoo-staging DEPLOY_REVISION
: 1467223276:0;shipit env  config_unset walkadoo-production DEPLOY_REVISION
: 1467223364:0;shipit env config_set walkadoo-production DATABASE_HOST walkadoo-production2.cduvhvvpw6oo.us-east-1.rds.amazonaws.com
: 1467223395:0;shipit env config_set walkadoo-production DATABASE_PASSIVE_HOST walkadoo-production2-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com
: 1467223416:0;shipit env config_set walkadoo-staging DATABASE_HOST walkadoo-staging2.cduvhvvpw6oo.us-east-1.rds.amazonaws.com
: 1467223457:0;shipit env config_set walkadoo-staging DATABASE_PASSIVE_HOST walkadoo-staging-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com
: 1467223522:0;shipit env config_set walkadoo-staging DATABASE_USERNAME walkadoo_stg_app
: 1467223536:0;shipit env config_set walkadoo-production DATABASE_USERNAME walkadoo_prd_app
: 1467223645:0;grep HIPPO ordered_shipit-wd-staging-env-062916.txt
: 1467223672:0;shipit env  config_unset walkadoo-staging HIPPO_CLIENT_ID
: 1467223682:0;shipit env  config_unset walkadoo-staging HIPPO_ENDPOINT
: 1467223688:0;shipit env  config_unset walkadoo-staging HIPPO_HOST
: 1467223696:0;shipit env  config_unset walkadoo-staging HIPPO_SECRET
: 1467223782:0;grep DESK ordered_shipit-wd-staging-env-062916.txt
: 1467223793:0;shipit env  config_unset walkadoo-staging DESK_DOT_COM_API_KEY
: 1467224058:0;shipit env  config_unset walkadoo-production BLUEPILL_BIN
: 1467224123:0;shipit env  config_unset walkadoo-staging APN_API_CLIENT_ID
: 1467224135:0;shipit env config_list walkadoo-staging | sort > ordered_shipit-wd-staging-env-062916.txt 
: 1467224139:0;shipit env config_list walkadoo-production | sort > ordered_shipit-wd-prod-env-062916.txt
: 1467224934:0;ls -l knife*sta*
: 1467224940:0;vi knife-env-walkadoo-staging.042016.txt
: 1467225126:0;grep PERISH ordered_shipit-wd-prod-env-062916.txt
: 1467225152:0;shipit env  config_unset walkadoo-production PERISHABLE_TOKENS_HASH_SECRET
: 1467225163:0;shipit env  config_unset walkadoo-production PERISHABLE_TOKENS_ENCRYPTION_KEY
: 1467225254:0;shipit env  config_unset walkadoo-staging NEW_RESQUE_URL
: 1467225434:0;shipit env  config_unset walkadoo-staging RABBITMQ_PASSWORD
: 1467225456:0;shipit env  config_unset walkadoo-production RABBITMQ_PASSWORD
: 1467225463:0;shipit env  config_unset walkadoo-production RABBITMQ_PORT
: 1467225472:0;shipit env  config_unset walkadoo-staging RABBITMQ_PORT
: 1467225508:0;shipit env  config_unset walkadoo-production RABBITMQ_USER
: 1467225798:0;shipit env  config_unset walkadoo-production RACK_ENV
: 1467225806:0;shipit env  config_unset walkadoo-staging RACK_ENV
: 1467226293:0;shipit env  config_unset walkadoo-staging WBID_ACCOUNT_CHANGE_LISTENER
: 1467226303:0;shipit env  config_unset walkadoo-production WBID_ACCOUNT_CHANGE_LISTENER
: 1467226451:0;grep ZEN ordered_shipit-wd-staging-env-062916.txt
: 1467226462:0;shipit env  config_unset walkadoo-production ZENDESK_BASE_URL
: 1467226470:0;shipit env  config_unset walkadoo-production ZENDESK_SSO_KEY
: 1467226481:0;shipit env  config_unset walkadoo-staging ZENDESK_BASE_URL
: 1467226490:0;shipit env  config_unset walkadoo-staging ZENDESK_SSO_KEY
: 1467226613:0;shipit env  config_set help
: 1467226811:0;cat ordered_shipit-wd-staging-env-062916.txt | awk '{print $1}'
: 1467226831:0;shipit env config_list walkadoo-staging > list_s
: 1467226843:0;cat ordered_shipit-wd-staging-env-062916.txt | awk '{print $1}' > 1 
: 1467226857:0;cat list_s | awk '{print $1}' > 2 
: 1467226958:0;vimdiff ordered_shipit-wd-staging-env-062916.txt ordered_shipit-wd-prod-env-062916.txt
: 1467227053:0;grep placeholder ordered_shipit-wd-staging-env-062916.txt
: 1467227085:0;shipit env  config_set walkadoo-staging API_CLIENT_INSIGHT_CLIENT_ID placeholder
: 1467227099:0;shipit env  config_set walkadoo-staging API_CLIENT_INSIGHT_SECRET placeholder
: 1467227111:0;shipit env  config_set walkadoo-staging CONCIERGE_DOMAIN placeholder
: 1467227122:0;shipit env  config_set walkadoo-staging CONCIERGE_TOKEN placeholder
: 1467227133:0;shipit env  config_set walkadoo-staging ORDER_FUFILLMENT_EMAIL_ADDRESS placeholder
: 1467227146:0;shipit env  config_set walkadoo-staging SECRET_KEY_BASE placeholder
: 1467227156:0;shipit env  config_set walkadoo-staging SERVICE_CREDENTIAL_ENCRYPTION_KEY placeholder
: 1467227168:0;shipit env  config_set walkadoo-staging SMARTYSTREETS_AUTH_ID placeholder
: 1467227176:0;shipit env  config_set walkadoo-staging SMARTYSTREETS_AUTH_TOKEN placeholder
: 1467227187:0;shipit env  config_set walkadoo-staging TWILIO_SHORT_CODE_IMPLEMENTATION_DATE placeholder
: 1467227199:0;shipit env  config_set walkadoo-staging UTILITY_SERVER_HOST placeholder
: 1467227212:0;shipit env  config_set walkadoo-staging WBID_OAUTH_AUTHORIZE_URL placeholder
: 1467227224:0;shipit env  config_set walkadoo-staging WBID_OAUTH_CLIENT_ID placeholder
: 1467227233:0;shipit env  config_set walkadoo-staging WBID_OAUTH_CLIENT_SECRET placeholder
: 1467227245:0;shipit env  config_set walkadoo-staging WBID_OAUTH_TOKEN_URL placeholder
: 1467227255:0;shipit env  config_set walkadoo-staging WELL_BEING_TRACKER_AUTHORIZE_URL placeholder
: 1467227268:0;shipit env  config_set walkadoo-staging WELL_BEING_TRACKER_CLIENT_ID placeholder
: 1467227277:0;shipit env  config_set walkadoo-staging WELL_BEING_TRACKER_CLIENT_SECRET placeholder
: 1467227308:0;history | grep ordered_shipit-wd-
: 1467227326:0;shipit env config_list walkadoo-production | sort 2
: 1467227350:0;more 2
: 1467227370:0;cat 1 | awk '{print $1}' > stage
: 1467227383:0;cat 2 | awk '{print $1}' > prod
: 1467227392:0;diff -u stage| prod
: 1467227395:0;diff -u stage prod
: 1467227426:0;diff stage prod
: 1467227522:0;grep MONITOR ordered_shipit-wd-prod-env-062916.txt
: 1467227532:0;shipit env  config_set walkadoo-staging DATABASE_MONITORING_PASSWORD placeholder
: 1467227574:0;grep FITB ordered_shipit-wd-prod-env-062916.txt
: 1467227584:0;grep FITB prod
: 1467227614:0;shipit env  config_unset walkadoo-production FITBIT_CONSUMER_KEY
: 1467227621:0;shipit env  config_unset walkadoo-production FITBIT_CONSUMER_SECRET
: 1467227633:0;grep FITL prod
: 1467227641:0;shipit env  config_unset walkadoo-production FITLINXX_GUID
: 1467227651:0;shipit env  config_unset walkadoo-production FITLINXX_PASSWORD
: 1467227756:0;grep WILDER prod
: 1467227766:0;grep HURBY prod
: 1467227784:0;shipit env  config_set walkadoo-staging HURBY_RMQ_NAMESPACE placeholder
: 1467227803:0;shipit env  config_set walkadoo-staging WILDER_HOST placeholder
: 1467227811:0;shipit env  config_set walkadoo-staging WILDER_ENV placeholder
: 1467227824:0;shipit env  config_set walkadoo-staging WILDER_PASSWORD placeholder
: 1467227836:0;shipit env  config_set walkadoo-staging WILDER_SCHEME placeholder
: 1467227848:0;shipit env  config_set walkadoo-staging WILDER_USER placeholder
: 1467227859:0;shipit env config_list walkadoo-production | sort > 2 
: 1467227906:0;grep MYH prod
: 1467227921:0;shipit env  config_set walkadoo-staging MYH_MONITORING_STACK placeholder
: 1467227950:0;shipit env  config_set walkadoo-staging SENSU_USE_SSL placeholder
: 1467227979:0;shipit env  config_set walkadoo-staging SMARTYSTREETS_URL placeholder
: 1467228007:0;shipit env  config_set walkadoo-staging REDIS_HOST placeholder
: 1467228017:0;shipit env  config_set walkadoo-staging RESQUE_HOST placeholder
: 1467228045:0;shipit env config_list walkadoo-production | sort > 2
: 1467228053:0;cat 1 | awk '{print $1}' | sort > stage
: 1467228057:0;cat 2 | awk '{print $1}' | sort > prod
: 1467228064:0;vimdiff stage prod
: 1467228722:0;more 1 | grep placeholder
: 1467228757:0;ssh ubuntu@ec2-54-90-101-115.compute-1.amazonaws.com
: 1467228790:0;more 1 | grep placeholder | awk '{print $1}'
: 1467228870:0;vi 1.5
: 1467228938:0;sh -xv 1.5
: 1467228984:0;mv 1.5 shipit-wd-stage-reconcilng-vars.txt
: 1467229003:0;shipit env config_list walkadoo-staging | sort > 1
: 1467229063:0;history | grep -i twilio | grep unset
: 1467229078:0;shipit env config_unset walkadoo-staging TWILIO_SHORT_CODE_IMPLEMENTATION_DATE
: 1467229103:0;shipit env config_set walkadoo-staging TWILIO_SHORT_CODE_IMPLEMENTATION_DATE placeholder 
: 1467229206:0;shipit env config_list wilder-staging | grep RE
: 1467229225:0;shipit env config_list dc-staging | grep RE
: 1467229234:0;shipit env config_list wbt-staging | grep RE
: 1467230178:0;history | grep -i REDIS
: 1467230224:0;more ShipIt/shipit_list
: 1467230228:0;cd ShipIt
: 1467230254:0;for i in `cat shipit_list` \ndo \nshipit env config_get ${i} RESQUE_URL > ${i}_resque_url_check_`date +"%Y%m%d-%s"`.log\ndone
: 1467230282:0;for i in `cat shipit_list`\
do\
shipit env config_get ${i} RESQUE_URL > ${i}_resque_url_check_`date +"%Y%m%d-%s"`.log\
done
: 1467230329:0;more walkadoo-staging_resque_url_check_20160629-1467230308.log
: 1467230371:0;for i in `cat shipit_list`\
do\
shipit env config_get ${i} RE > ${i}_re_pattern_check_`date +"%Y%m%d-%s"`.log\
done
: 1467230681:0;cd ../aws_cli_output
: 1467230683:0;ls-ltra
: 1467230686:0;ls -ltraha
: 1467230692:0;cat aws-dms-describe-replication-instances-dw.txt
: 1467230716:0;more aws-describe-refresh-schemas-status---endpoint-arn-walkadoo.txt
: 1467255827:0;ls -ltra > list_logs_redis_checks
: 1467255856:0;mv walkadoo-staging_resque_url_check_20160629-1467230308.log keep-walkadoo-staging_resque_url_check_20160629-1467230308.log
: 1467255879:0;rm mv keep-walkadoo-staging_resque_url_chk.log 
: 1467255894:0;mv keep-walkadoo-staging_resque_url_check_20160629-1467230308.log keep-walkadoo-staging_resque_url_chk.log 
: 1467255919:0;rm *resque_url_check_20160629-146*.log
: 1467260802:0;aws-info i-8701ed7f
: 1467293410:0;history | grep knife 
: 1467293422:0;knife environment list
: 1467318666:0;aws cloudwatch describe-alarms
: 1467318726:0;aws cloudwatch describe-alarms > Documents/aws_cli_output/aws-cloudwatch-describe-alarms.063016.txt
: 1467318766:0;aws cloudwatch get-metric-statistics
: 1467318773:0;aws cloudwatch get-metric-statistics help
: 1467333169:0;knife client delete -y i-3165cfab
: 1467333193:0;knife node delete -y i-3165cfab
: 1467333211:0;knife client delete -y i-a07ad03a
: 1467333221:0;knife node delete -y i-a07ad03a
: 1467334541:0;ls -ltra ~/Documents/git/myh-terraform.oops/environments/staging/*/terraform.tfvars 
: 1467334560:0;cp /Users/heidischmidt/Documents/git/myh-terraform.oops/environments/staging/wilder/terraform.tfvars ~/Documents/git/myh-terraform/environments/staging/wilder/terraform.tfvars
: 1467334579:0;cp /Users/heidischmidt/Documents/git/myh-terraform.oops/environments/staging/daily-challenge/terraform.tfvars ~/Documents/git/myh-terraform/environments/staging/daily-challenge/terraform.tfvars
: 1467334600:0;cp /Users/heidischmidt/Documents/git/myh-terraform.oops/environments/staging/iris/terraform.tfvars ~/Documents/git/myh-terraform/environments/staging/iris/terraform.tfvars
: 1467334618:0;cp /Users/heidischmidt/Documents/git/myh-terraform.oops/environments/staging/quitnet/terraform.tfvars ~/Documents/git/myh-terraform/environments/staging/quitnet/terraform.tfvars
: 1467334630:0;ls -ltra ~/Documents/git/myh-terraform.oops/environments/staging/*/variables.tf
: 1467334657:0;cp /Users/heidischmidt/Documents/git/myh-terraform.oops/environments/staging/iris/variables.tf  ~/Documents/git/myh-terraform/environments/staging/iris
: 1467334687:0;ls -ltra */variables.tf
: 1467334746:0;cd ../production/dailychallenge
: 1467334773:0;cd staging/quitnet
: 1467418865:0;shipit env config_list iris-production
: 1467418945:0;shipit env config_set iris-production REDIS_URL  redis://iris-production.teaj7o.ng.0001.use1.cache.amazonaws.com:6379
: 1467419019:0;shipit deployment create iris-production c53addde55b5dca7ac5d3c4326f49f1916a6b14e
: 1467423632:0;shipit env config_set concierge-production REDIS_URL redis://concierge-production.teaj7o.ng.0001.use1.cache.amazonaws.com:6379/0
: 1467423659:0;shipit deployment create concierge-production 942319e29c6b41670dd708f712c3ac11013e611e
: 1467423895:0;shipit env show insight-production 
: 1467423965:0;shipit env config_set insight-production REDIS_URL redis://insight-production.teaj7o.ng.0001.use1.cache.amazonaws.com:6379
: 1467423993:0;shipit deployment create insight-production 043993164393646d7dec6814fb3ef8c60e7714fc
: 1467424388:0;shipit env config_set quitnet-production REDIS_URL redis://quitnet-production.teaj7o.ng.0001.use1.cache.amazonaws.com:6379
: 1467424439:0;shipit deployment create quitnet-production 51c8414cc4f974b0a6c1b07ff02a7af8d4012c73
: 1467424939:0;shipit env show rza-production
: 1467742501:0;shipit env show myh-builder 
: 1467742623:0;shipit env config_set myh-builder REDIS_URL redis://shipit-shared-prod.teaj7o.ng.0001.use1.cache.amazonaws.com:6379/0 
: 1467742677:0;shipit deployment create myh-builder fef086907fd0b6d792e9698a4b2ffe18f8c78d24 
: 1467743136:0;shipit env config_get concierge-production REDIS_URL
: 1467743220:0;shipit env config_set hello200-production REDIS_URL redis://hello200-prod.teaj7o.ng.0001.use1.cache.amazonaws.com:6379/0 
: 1467743281:0;shipit env create hello200-production 539d7b2d25ae35eacfbab5e7784504440481608a
: 1467743327:0;shipit env show hello200-production
: 1467743410:0;shipit env config_set hello200-production REDIS_URL redis://hello200-prod.teaj7o.ng.0001.use1.cache.amazonaws.com/1
: 1467743443:0;shipit env config_get hello200-production REDIS_URL
: 1467743547:0;shipit env config_get hello200-staging REDIS_URL
: 1467743672:0;shipit env config_get hurby-server-production REDIS_URL
: 1467743683:0;shipit env config_get hurby-server-staging REDIS_URL
: 1467743694:0;shipit env config_get insight-production REDIS_URL
: 1467743704:0;shipit env config_get iris-production REDIS_URL
: 1467743712:0;shipit env config_get iris-staging REDIS_URL
: 1467743794:0;shipit env config_set legacy-deploy-dashboard REDIS_URL redis://legacy-deploy-dash.teaj7o.ng.0001.use1.cache.amazonaws.com
: 1467743843:0;shipit env config_get myh-builder REDIS_URL
: 1467743857:0;shipit env config_get quitnet-production REDIS_URL
: 1467743931:0;shipit env config_set quitnet-staging REDIS_URL redis://quitnet-staging.teaj7o.ng.0001.use1.cache.amazonaws.com:6379
: 1467744045:0;shipit env config_set reporting-production REDIS_URL redis://reporting-prod.teaj7o.ng.0001.use1.cache.amazonaws.com:6379 
: 1467744139:0;shipit env config_get rza-production REDIS_URL
: 1467744159:0;shipit env config_get rza-staging REDIS_URL
: 1467744208:0;shipit env config_get sampleapp REDIS_URL
: 1467744219:0;shipit env config_get walkadoo-production REDIS_URL
: 1467744340:0;shipit env config_set walkadoo-staging REDIS_URL redis://wd-staging-redis.teaj7o.ng.0001.use1.cache.amazonaws.com:6379 
: 1467744896:0;shipit env config_set wellbeingid-production REDIS_URL redis://wellbeingid-prod.teaj7o.ng.0001.use1.cache.amazonaws.com/0 
: 1467744945:0;shipit env config_get wellbeingid-staging REDIS_URL
: 1467744976:0;shipit env config_get wellbeingtracker-production REDIS_URL
: 1467745004:0;shipit env config_get wellbeingtracker-staging REDIS_URL
: 1467745061:0;shipit env config_set wellbeingtracker-staging REDIS_URL redis://wbt-staging.teaj7o.ng.0001.use1.cache.amazonaws.com/1
: 1467745119:0;shipit env config_get wilder-staging REDIS_URL
: 1467812613:0;shipit env show hello200-production 
: 1467812797:0;shipit deployment create hello200-production 539d7b2d25ae35eacfbab5e7784504440481608a
: 1467813396:0;shipit deployment create hello200-staging 539d7b2d25ae35eacfbab5e7784504440481608a
: 1467815659:0;ls -l ~/Applications/
: 1467815682:0;ls -l ~/Applications/Utilities
: 1467815689:0;ls -l Applications
: 1467815866:0;ls -l mysql-connector-java-5.1.39/*
: 1467815868:0;ls -l mysql-connector-java-5.1.39/
: 1467815870:0;ls -l mysql-connector-java-5.1.39
: 1467815882:0;rm -rf mysql-connector-java-5.1.39
: 1467815959:0;rm AWS*Schema*Conversion*Tool*.dmg
: 1467815980:0;ls -ltr *.zip
: 1467815994:0;cp mysql-connector-java-5.1.39.zip ../
: 1467816001:0;ls -l /opt/
: 1467816026:0;mv mysql-connector-java-5.1.39.zip /opt/
: 1467816034:0;sudo mv mysql-connector-java-5.1.39.zip /opt/
: 1467816056:0;unzip mysql-connector-java-5.1.39.zip
: 1467816066:0;cd mysql-connector-java-5.1.39
: 1467816201:0;history | grep walkadoo | grep ssh 
: 1467816248:0;ssh -A -L 7015:walkadoo-production2-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 heidischmidt@ec2-54-90-17-51.compute-1.amazonaws.com -N
: 1467816397:0;ssh -A -L 8888:analysis-warehouse-production.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:5432 heidischmidt@ec2-54-226-92-154.compute-1.amazonaws.com -N
: 1467816901:0;grep -i schema *
: 1467818851:0;rm "@"
: 1467818857:0;vi AWS-SCT-WD-M-to-P.sql
: 1467818865:0;ls -ltra *
: 1467818877:0;cd AWS-SCT-Test2
: 1467818886:0;grep walkadoo-production *.xml
: 1467818892:0;grep walkadoo-production target.xml
: 1467818895:0;more target.xml
: 1467818921:0;more *sct
: 1467818999:0;vi test.sql
: 1467819047:0;rm AWS-SCT-WD-M-to-P.sql
: 1467819056:0;rm -rf AWS-SCT-Test-WD-070616/
: 1467819157:0;cp test.sql ../aws-sct-test-convert-using-modded-sql-file.sql 
: 1467819206:0;cp test.sql ../../aws-sct-test-convert-using-modded-sql-file.sql 
: 1467819209:0;ls -l /Users/heidischmidt/aws-sct-test-convert-using-modded-sql-file.sql
: 1467819288:0;vi aws-sct-test-convert-using-modded-sql-file.import.loaded.log
: 1467820526:0;head -1750 wd-prod-test-convert.sql
: 1467826681:0;grep user_prompt wd-prod-test-convert.sql
: 1467827001:0;cd h200
: 1467827010:0;grep -i sequence *
: 1467827138:0;psql -E -h 127.0.0.1 -p 8888 -U meyouhealth  --dbname=dw_sct_test
: 1467829991:0;grep "CREATE INDEX" wd-prod-test-convert.sql
: 1467830005:0;grep "CREATE INDEX" wd-prod-test-convert.sql | awk '{print $3}'
: 1467830010:0;grep "CREATE INDEX" wd-prod-test-convert.sql | awk '{print $3}' | wc -l 
: 1467830027:0;grep "CREATE INDEX" wd-prod-test-convert.sql | awk '{print $3}' | length 
: 1467830034:0;grep "CREATE INDEX" wd-prod-test-convert.sql | awk '{print $3}' | wc -c
: 1467830072:0;for i in `cat list`\
do \
wc -c $i >> list_length\
done 
: 1467830111:0;more list_length
: 1467830143:0;echo idx_active_admin_comments_on_author_type_and_id | wc -c
: 1467831060:0;grep "CREATE INDEX" wd-prod-test-convert.sql | awk '{print $3}' > list
: 1467831068:0;rm list_length
: 1467831071:0;for i in `cat list`\
do \
echo $i | wc -c  >> list_length\
done 
: 1467831080:0;paste list list_length
: 1467832259:0;cd ../AWS\ Schema\ Conversion\ Tool
: 1467832271:0;vi wd-prod-test-convert.sql
: 1467832320:0;cp wd-prod-test-convert.sql ../../wd-prod-test-convert.sql
: 1467832340:0;psql -E -h 127.0.0.1 -p 8888 -U dw_sct_test_app  --dbname=dw_sct_test --help
: 1467832444:0;mv "AWS\ Schema\ Conversion\ Tool/" AWSSchemaConversionTool
: 1467832536:0;psql -E -h 127.0.0.1 -p 8888 -U dw_sct_test_app  --dbname=dw_sct_test -a -L /Users/heidischmidt/wd-prod-test-convert.log -F /Users/heidischmidt/wd-prod-test-convert.sql
: 1467832818:0;more wd-prod-test-convert.log
: 1467832845:0;mv wd-prod* AWS\ Schema\ Conversion\ Tool/Projects/
: 1467832857:0;more aws-sct-test-convert-using-modded-sql-file.sql
: 1467832862:0;rm aws-sct-test-convert-using-modded-sql-file.sql
: 1467838754:0;shipit env config_get walkadoo-staging REDIS_URL
: 1467838839:0;shipit env config_set walkadoo-staging REDIS_URL redis://wd-staging-redis.teaj7o.ng.0001.use1.cache.amazonaws.com
: 1467838857:0;shipit deployment create walkadoo-staging 28eb05e9212ddefefbfc2601a873e034074628bb
: 1467899075:0;shipit env config_get reporting-production REDIS_URL
: 1467901760:0;shipit env config_get quitnet-staging REDIS_URL
: 1467902109:0;shipit deployment create quitnet-staging 9296c2208865fb2e7841687943fe50e2309480ec
: 1467902863:0;shipit env config_get wellbeingid-production REDIS_URL
: 1467902878:0;shipit env show wellbeingid-production 
: 1467904894:0;history | grep ssh | grep iris-production
: 1467905002:0;history | grep ssh | grep L | grep 54.152.155.24
: 1467905155:0;history | grep 9990
: 1467905268:0;ls -l *pg*grants*
: 1467905272:0;ls -l *grant*
: 1467905284:0;grep reporting *
: 1467905530:0;psql -E -h 127.0.0.1 -p 9990 -U reporting_production_app  --dbname=iris_production
: 1467905612:0;psql -E -h 127.0.0.1 -p 9990 -U iris_production_app  --dbname=iris_production
: 1467905704:0;psql -E -h 127.0.0.1 -p 9990 -U meyouhealth  --dbname=iris_production
: 1467908260:0;more get_mysql_users_grants.sql
: 1467914338:0;ls -l reporting
: 1467915297:0;grep monitoring_interval
: 1467915301:0;grep monitoring_interval *
: 1467915455:0;grep newrelic_monitored *
: 1468001237:0;history | grep "^aws rds " | grep rds
: 1468001241:0;history | grep "^aws rds " | more
: 1468001307:0;aws rds describe-db-instances --db-instance-identifier quitnet-production
: 1468001372:0;aws rds describe-db-instances --db-instance-identifier analysis-datawarehouse-production
: 1468001394:0;aws rds describe-db-instances --db-instance-identifier analysis-warehouse-production
: 1468001694:0;vi aws-dms-quitnet-prod-db-version-issue-070816.txt
: 1468002176:0;aws rds describe-db-instances --db-instance-identifier wellbeingid-production
: 1468002227:0;aws rds describe-db-instances --db-instance-identifier wellbeingid-production > wellbeingid-prod-rds-describe-instance-070816.txt
: 1468011632:0;aws-nfo i-7c3992e6
: 1468011647:0;aws-info i-7c3992e6
: 1468245402:0;shipit env config_get legacy-deploy-dashboard REDIS_URL 
: 1468249718:0;history | grep analysis
: 1468249829:0;more /Users/heidischmidt/wd-prod-test-convert.sql
: 1468249858:0;cd AWS\ Schema\ Conversion\ Tool
: 1468249863:0;cd Projects
: 1468249917:0;more wd-prod-test-convert.sql
: 1468249973:0;history | grep psql | grep 8888
: 1468251254:0;psql -E -h 127.0.0.1 -p 8888 -U meyouhealth  --dbname=dw_sct_test 
: 1468252212:0;psql -E -h 127.0.0.1 -p 8888 -U meyouhealth  --dbname=postgres
: 1468252308:0;aws-info i-7eb69282
: 1468257974:0;more pg_settings-analysis-warehouse-prod-dms-testing.txt
: 1468258913:0;more pg_settings-analysis-warehouse-prod-dms-testing.txt 
: 1468258936:0;psql -E -h 127.0.0.1 -p 8888 -U dw_sct_test_app  --dbname=dw_sct_test
: 1468259391:0;vi table-drop-list.sql
: 1468259461:0;cat table-drop-list.sql
: 1468260719:0;shipit env config_get wilder-production REDIS_URL
: 1468260735:0;shipit env config_list wilder-production
: 1468260911:0;shipit deployment logs 3571
: 1468306300:0;aws-info i-d238bda2
: 1468341903:0;ssh ubuntu@ec2-54-88-73-139.compute-1.amazonaws.com
: 1468342235:0;grep date *.sh
: 1468351836:0;ssh -A -L 9999:127.0.0.1:9999 ubuntu@52.86.252.208 ssh -L 9999:wilder-staging.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N ubuntu@10.0.5.160
: 1468352032:0;ssh -A -L 9990:127.0.0.1:9990 ubuntu@54.152.155.24 ssh -L 9990:wilder-production.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N ubuntu@10.0.14.26
: 1468352118:0;ssh -A -L 9990:127.0.0.1:9990 ubuntu@54.152.155.24 ssh -L 9990:wilder-production2.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N ubuntu@10.0.14.26
: 1468352155:0;ssh -A -L 3309:127.0.0.1:3309 ubuntu@54.152.155.24 ssh -L 3309:wilder-production2.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N ubuntu@10.0.14.26
: 1468352200:0;mysql -u meyouhealth -P 3309 -h 127.0.0.1
: 1468352228:0;history | grep ssh | grep L | grep 3306
: 1468352397:0;mysql -u meyouhealth -P 9900 -h 127.0.0.1
: 1468352411:0;mysql -u meyouhealth -P 9900 -h 127.0.0.1 -p 
: 1468418529:0;aws-info i-acf00136
: 1468421371:0;ssh ubuntu@ec2-54-174-13-32.compute-1.amazonaws.com
: 1468432335:0;more /etc/hsots
: 1468436912:0;cd environments/production/dailychallenge
: 1468439354:0;psql -E -h 127.0.0.1 -p 8888 -U dw_sct_test_app  --dbname=dw_sct_test 
: 1468507266:0;xs
: 1468507408:0;more ../dailychallenge/ec2.tf
: 1468507640:0;terraform plan -out=test.plan --target=aws_db_instance.dc_production2
: 1468507732:0;la
: 1468508156:0;history | grpe ssh | grep L | grep quitnet
: 1468508165:0;history | grep ssh | grep L | grep quitnet
: 1468508219:0;ssh -A -L 9001:localhost:9001 ubuntu@54.152.155.24 ssh -L 9001:qn-test-upgrade-93-94.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:5432 -N ubuntu@10.0.5.179
: 1468508254:0;history | grep psql | grep 9001
: 1468508396:0;ssh -A -L 9001:localhost:9001 ubuntu@54.152.155.24 ssh -L 9001:qn-test-upgrade-93-94.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:5432 -N ubuntu@10.0.14.226
: 1468508406:0;psql -h 127.0.0.1 -p 9001 -U meyouhealth --dbname=quitnet_production 
: 1468508572:0;ssh -A -L 9002:localhost:9002 ubuntu@54.152.155.24 ssh -L 9002:quitnet-research.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:5432 -N ubuntu@10.0.14.226
: 1468508601:0;psql -h 127.0.0.1 -p 9002 -U meyouhealth --dbname=quitnet_production 
: 1468508775:0;ssh -A -L 9002:localhost:9002 ubuntu@54.152.155.24 ssh -L 9002:quitnet-research.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:5432 -N ubuntu@ec2-54-81-188-182.compute-1.amazonaws.com
: 1468509980:0;git checkout terraform.tfstate
: 1468510361:0;cd legacy-deploy-dashboard
: 1468510469:0;git checkout variables.tf
: 1468510637:0;cd myh-builder
: 1468510764:0;terraform plan -out=test.plan --target=module.quitnet-production-all-in-one.aws_autoscaling_group.layer_auto_scale
: 1468510819:0;cd reporting-dashboard
: 1468511253:0;cp ~/Documents/git/chef-repo/terraform/production/terraform.tfvars .
: 1468511349:0;cp ~/Documents/git/chef-repo/terraform/staging/terraform.tfvars .
: 1468511452:0;ls -ltra */*terraform.tfvars
: 1468511474:0;cp wilder/terraform.tfvars analytics-data-pipeline/terraform.tfvars.new
: 1468511476:0;cd analytics-data-pipeline
: 1468511488:0;mv terraform.tfvars terraform.tfvars.old 
: 1468511494:0;cp terraform.tfvars.new terraform.tfvars
: 1468511629:0;cd daily-challenge
: 1468511791:0;gi add rds.tf
: 1468511837:0;cd hurby-server
: 1468511989:0;cd rabbitmq
: 1468512076:0;cd vpc
: 1468512240:0;git add rds.tf 
: 1468512284:0;cd wellbeingtracker
: 1468512300:0;cp ../analytics-data-pipeline/terraform.tfvars .
: 1468512418:0;ls -la
: 1468518514:0;ssh -A -L 9001:localhost:9001 ubuntu@54.152.155.24 ssh -L 9001:qn-test-93to94.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:5432 -N ubuntu@10.0.14.226
: 1468519979:0;psql -h 127.0.0.1 -p 9010  -U meyouhealth --dbname=quitnet_production
: 1468520020:0;psql -h 127.0.0.1 -p 9010  -U meyouhealth --dbname=quitnet_production -e " SELECT * from pg_stat_activity" 
: 1468520096:0;vi pg_stat_activity.sql 
: 1468520210:0;vi pg_stat_activity.
: 1468520286:0;vi pg_stat_activity.sql
: 1468520306:0;psql -h 127.0.0.1 -p 9010  -U meyouhealth --dbname=quitnet_production -f pg_stat_activity.sql --log-file=pg_stat_activity.log
: 1468520438:0;rm pg_stat_activity.log
: 1468521999:0;ping qn-test-93to94.cduvhvvpw6oo.us-east-1.rds.amazonaws.com
: 1468522016:0;ping -c 10 qn-test-93to94.cduvhvvpw6oo.us-east-1.rds.amazonaws.com
: 1468522064:0;while true; echo date; ping -c 10 qn-test-93to94.cduvhvvpw6oo.us-east-1.rds.amazonaws.com; done
: 1468522082:0;while true; echo date; do ping -c 10 qn-test-93to94.cduvhvvpw6oo.us-east-1.rds.amazonaws.com
: 1468522102:0;while true; do echo date; ping -c 10 qn-test-93to94.cduvhvvpw6oo.us-east-1.rds.amazonaws.com; done 
: 1468522149:0;while true; do date; ping -c 3 qn-test-93to94.cduvhvvpw6oo.us-east-1.rds.amazonaws.com; done 
: 1468522701:0;nslookup qn-test-93to94.cduvhvvpw6oo.us-east-1.rds.amazonaws.com
: 1468522720:0;ssh -A -L 9010:localhost:9010 ubuntu@54.152.155.24 ssh -L 9010:qn-test-93to94.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:5432 -N ubuntu@10.0.14.226
: 1468522727:0;psql -h 127.0.0.1 -p 9010  -U meyouhealth --dbname=quitnet_production -f pg_stat_activity.sql --log-file=pg_stat_activity.log 
: 1468603801:0;history | grep dc-productio
: 1468603818:0;ssh -A -L 9999:dc-production2.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 heidischmidt@ec2-54-242-121-10.compute-1.amazonaws.com -N
: 1468603830:0;history | grep mysql | grep 9999
: 1468603840:0;mysql -u meyouhealth -P 9999 -h 127.0.0.1
: 1468603861:0;ssh shipit-prouduction
: 1468850748:0;brew install bash-completion
: 1468850768:0;brew upgrde bash-completion
: 1468850773:0;brew upgrade bash-completion
: 1468850794:0;brew tap homebrew/completions
: 1468852524:0;aws-events
: 1468852643:0;aws elasticache describe-events
: 1468948260:0;history | grpe iris
: 1468948266:0;history | grep iris
: 1468948282:0;ssh -A -L 9990:127.0.0.1:9990 ubuntu@54.152.155.24 ssh -L 9990:iris-production.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:5432 -N ubuntu@10.0.5.129
: 1468948488:0;ssh -A -L 9999:127.0.0.1:9999 ubuntu@54.152.155.24 ssh -L 9999:iris-research.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:5432 -N ubuntu@10.0.5.129
: 1468948530:0;ssh -A -L 9111:127.0.0.1:9111 ubuntu@54.152.155.24 ssh -L 9111:iris-research.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:5432 -N ubuntu@10.0.5.129
: 1468952312:0;aws elasticache describe-events --source-type quitnet-staging --max-items 40 
: 1468952340:0;aws elasticache describe-events --source-type quitnet-stg-001 --max-items 40 
: 1468952360:0;aws elasticache describe-events --source-type --help
: 1468952366:0;aws elasticache describe-events --source-type help
: 1468952371:0;aws elasticache describe-events help
: 1468952425:0;aws elasticache describe-events --source-type cache-cluster
: 1469023172:0;history | grep dc-production
: 1469023182:0;history | grep dc-production | grpe ssh 
: 1469023187:0;history | grep dc-production | grep ssh 
: 1469023210:0;ssh -A -L 9998:dc-production.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 heidischmidt@ec2-54-226-92-154.compute-1.amazonaws.com -N
: 1469023259:0;mysql -u meyouhealth -p -P 9998 -h 127.0.0.1 -e "show full processlist"
: 1469023360:0;ssh -A -L 9900:localhost:9900 ubuntu@54.152.155.24 ssh -L 9900:wilder-production2.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N ubuntu@10.0.14.26
: 1469023407:0;ssh -A -L 9000:localhost:9000 ubuntu@54.152.155.24 ssh -L 9000:wilder-production2.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N ubuntu@10.0.14.26
: 1469023423:0;mysql -u meyouhealth -p -P 9000 -h 127.0.0.1 -e "show full processlist"
: 1469023456:0;mysql -u meyouhealth -p -P 9000 -h 127.0.0.1 
: 1469026519:0;aws rds help 
: 1469035634:0;ssh ubuntu@ec2-54-81-188-182.compute-1.amazonaws.com
: 1469043436:0;aws dms modify-replication-instance help
: 1469043615:0;aws dms help | grep stop
: 1469043620:0;aws dms help | grep replication
: 1469111346:0;shipit env config_get hello200-production REDIS_URL 
: 1469113168:0;more ../concierge/elasticache.tf
: 1469113175:0;cp ../concierge/elasticache.tf . 
: 1469113451:0;grep aws_security_group ../concierge/*.tf
: 1469113477:0;cd myh-terraform/environments/production/concierge
: 1469113519:0;cp ../concierge/ec2.tf .
: 1469114293:0;grep aws_subnet ../concierge/*.tf
: 1469114311:0;grep aws_subnet *.tf
: 1469114349:0;more rds
: 1469114358:0;cp ../concierge/rds.tf . 
: 1469114493:0;grep deployment-api_production_elasticache
: 1469114497:0;grep deployment-api_production_elasticache *
: 1469114559:0;terraform plan -out=test.plan --target aws_security_group.deployment_api_production
: 1469115846:0;find . -name "*.tf" -exec grep -i memcache {} \;
: 1469115881:0;grep memcache *
: 1469115929:0;mv elasticache.tf elasticache.tf.cp.concierge
: 1469115936:0;cp ../walkadoo/elasticache.tf . 
: 1469115961:0;more elasticache.tf.cp.concierge
: 1469116270:0;grep aws_security_group *
: 1469123120:0;head -315 terraform.tfstate
: 1469123430:0;vi argh
: 1469123869:0;terraform plan -out=test.plan --target=aws_security_group.deployment_api_production
: 1469123901:0;terraform plan -out=test.plan --target=aws_elasticache_subnet_group.deployment_api_production_redis
: 1469124063:0;cd concierge
: 1469125559:0;git add elasticache.tf
: 1469125567:0;rm argh
: 1469126251:0;more ../concierge/ec2.tf
: 1469126559:0;shipit env config_set deployment-api REDIS_URL redis://deployment-api-prod.teaj7o.ng.0001.use1.cache.amazonaws.com:6379/1
: 1469126756:0;knife data bag edit container-config deployment-api 
: 1469126812:0;knife data bag show container-config deployment-api  --format json
: 1469127811:0;shipit deployment create iris-staging 4018304baacca18f0c5db9ecb63eb999847947f2
: 1469128694:0;aws rds describe-instance insight-production
: 1469132048:0;aws rds describe-db-instances insight-production
: 1469132077:0;aws rds describe-db-instances --db-instance-identifier insight-production
: 1469216638:0;more postgres_h200_grants_snippet_pg_dump_edits.sql
: 1469422060:0;ssh shipit-production 
: 1469553657:0;shipit env config_get deployment-api REDIS_URL
: 1469553684:0;shipit env show  deployment-api 
: 1469629787:0;history | grep iris-prod
: 1469629812:0;ssh -A -L 9900:127.0.0.1:9900 ubuntu@54.152.155.24 ssh -L 9900:iris-production.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:5432 -N ubuntu@10.0.5.129
: 1469629857:0;history | grep psql | grep 9990
: 1469643417:0;ssh -A -L 9002:127.0.0.1:9002 ubuntu@54.152.155.24 ssh -L 9001:concierge-production.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:5432 -N ubuntu@10.0.14.127
: 1469643434:0;ssh -A -L 9002:127.0.0.1:9002 ubuntu@54.152.155.24 ssh -L 9002:concierge-production.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:5432 -N ubuntu@10.0.14.127
: 1469643513:0;psql -E -h 127.0.0.1 -p 9002 -U concierge_app --dbname=concierge_production 
: 1469644491:0;cat /tmp/list
: 1469650632:0;shipit env show
: 1469650663:0;shipit env show iris-production
: 1469650675:0;shipit deployment logs 3714
: 1469650725:0;psql -E -h 127.0.0.1 -p 9001 -U iris_production_app  --dbname=iris_production
: 1469717752:0;shipit env show legacy-deploy-dash
: 1469717923:0;shipit env show legacy-deploy-dashboard
: 1469717940:0;shipit env config_get legacy-deploy-dashboard REDIS_URL
: 1469718457:0;cd myh-terraform/environments/production/legacy-deploy-dashboard
: 1469718564:0;cd quitnet
: 1469730835:0;shipit console wilder-production
: 1469730920:0;history | grep "--console"
: 1469730925:0;history | grep "console"
: 1469733441:0;ssh -A -L 9001:127.0.0.1:9001 ubuntu@54.152.155.24 ssh -L 9001:iris-production.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:5432 -N ubuntu@10.0.5.129
: 1469733463:0;ssh -A -L 9003:127.0.0.1:9003 ubuntu@54.152.155.24 ssh -L 9003:iris-production.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:5432 -N ubuntu@10.0.5.129
: 1469733541:0;psql -E -h 127.0.0.1 -p 9003 -U iris_production_app  --dbname=iris_production
: 1470147930:0;mkdir shared-staging-mysql
: 1470147951:0;grep -i rds *.tf
: 1470147962:0;cp rds.tf ../shared-staging-mysql
: 1470150176:0;grep shared_mysql_staging_database_password *.tf
: 1470150261:0;grep region variables.tf
: 1470150291:0;grep region *.tf
: 1470150294:0;grep region *.tf*
: 1470150351:0;git add .
: 1470150389:0;grep aws.region *.tf
: 1470150478:0;grep vpc *.tf
: 1470150701:0;find . -name "rds.tf" -exec grep -i region {}\;
: 1470150706:0;find . -name "rds.tf" -exec grep -i region {} \;
: 1470150713:0;find . -name "rds.tf" -exec grep -i zone {} \;
: 1470150725:0;find . -name "variables.tf" -exec grep -i zone {} \;
: 1470150732:0;find . -name "variables.tf" -exec grep -i region {} \;
: 1470150786:0;grep Daily *.tf
: 1470150894:0;find . -name "*.tf" -exec grep -i sg-6f621714 {} \;
: 1470150902:0;find . -name "*.tf" -exec grep -i DMS {} \;
: 1470150922:0;find . -name "*.tf" -exec grep -i sg- {} \;
: 1470150937:0;find . -name "*.tf" -print -exec grep -i sg- {} \;
: 1470150962:0;find ../production -name "*.tf" -print -exec grep -i sg- {} \;
: 1470150976:0;find ../production -name "*.tf" -print -exec grep -i sg-6f621714 {} \;
: 1470150984:0;cd ../production
: 1470151047:0;cd ../staging
: 1470151429:0;grep walkadoo_staging2 *.tf
: 1470151584:0;ls -l .ter*
: 1470151600:0;ls -l .t*
: 1470151692:0;terraform -V
: 1470151813:0;cd ../shared
: 1470151828:0;cd ../shared-staging-mysql
: 1470151867:0;grep shared-staging-mysql rds.tf
: 1470151921:0;mv shared-staging-mysql shared-mysql-staging
: 1470151928:0;git add shared-mysql-staging
: 1470151954:0;git rm shared-staging-mysql/rds.tf
: 1470151984:0;git rm shared-staging-mysql/variables.tf
: 1470151992:0;ls -l shared-mysql-staging
: 1470152342:0;diff -u variables.tf ../walkadoo/variables.tf
: 1470152361:0;grep provider *.tf
: 1470152374:0;cp provider.tf ../shared-mysql-staging/provider.tf
: 1470152416:0;terraform plan -out=test.plan --target=aws_db_subnet_group.shared_mysql_staging
: 1470152674:0;vi rds.td
: 1470153115:0;cp walkadoo.tf ../shared-mysql-staging
: 1470153124:0;vi walkadoo.tf
: 1470153223:0;terraform plan -out=test.plan --target=aws_security_group.shared_mysql_staging
: 1470156745:0;ssh ubuntu@ec2-54-163-184-166.compute-1.amazonaws.com
: 1470161734:0;ssh ubuntu@ec2-54-147-229-235.compute-1.amazonaws.com
: 1470162709:0;aws-info i-17c6ec8d
: 1470164693:0;vi /tmp/list
: 1470164795:0;cat /tmp/list| sort
: 1470231253:0;shipit env show walkadoo_staging
: 1470231281:0;shipit env config_list walkadoo-staging | grep URL
: 1470231291:0;shipit env config_list walkadoo-staging 
: 1470231306:0;shipit help  
: 1470231316:0;shipit env help
: 1470231375:0;history | grpe gem
: 1470231382:0;history | grep gem
: 1470231392:0;gem install shipit
: 1470231421:0;gem list 
: 1470231875:0;shipit env config_list wilder-staging 
: 1470232338:0;shipit env config_list wellbeingtracker-staging 
: 1470232529:0;shipit env config_list dc-staging 
: 1470232538:0;shipit env config_list dailychallenge-staging 
: 1470234606:0;history | grpe hello200
: 1470234612:0;history | grep hello200
: 1470234628:0;history | grep hello200 | grep ssh
: 1470234732:0;cat .ssh/config | more
: 1470234793:0;ssh -A -L 9900:localhost:9900 ubuntu@54.152.155.24 ssh -L 9900:hello200-production.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:5432 -N ubuntu@10.0.5.183
: 1470234817:0;ssh -A -L 8900:localhost:8900 ubuntu@54.152.155.24 ssh -L 8900:hello200-production.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:5432 -N ubuntu@10.0.5.183
: 1470234845:0;history | grep hello200_production
: 1470234895:0;psql -h 127.0.0.1 -p 8900 -U meyouhealth --dbname=hello200_production 
: 1470234972:0;more hello200_production_936_show_all_variables_aug_03_2016.txt
: 1470235016:0;git diff rds.tf
: 1470235332:0;grep -i MonitoringRoleARN ../*/*.tf
: 1470235339:0;grep -i MonitoringRoleARN ../../*/*.tf
: 1470235372:0;find . -name "*.tf" -print -exec grep -i monitoring {} \;
: 1470235391:0;cd ./production/wellbeingtracker/rds.tf
: 1470235394:0;cd ./production/wellbeingtracker/
: 1470235403:0;grep -i monitor *.tf
: 1470235421:0;grep -i arn *.tf
: 1470235595:0;cd staging/shared-mysql-staging
: 1470235756:0;grep security_group rds.tf
: 1470236016:0;more ./wilder/shipit.tf
: 1470236024:0;more ../wilder/shipit.tf
: 1470236048:0;more ../wilder/ec2.tf
: 1470236120:0;git mv walkadoo.tf ec2.tf
: 1470236140:0;cp walkadoo.tf ec2.tf 
: 1470236145:0;git rm walkadoo.tf
: 1470236160:0;git checkout walkadoo.tf
: 1470236166:0;rm walkadoo.tf
: 1470236212:0;;tfdp
: 1470236268:0;find . -name "*.tf" -print -exec grep -i security_group {} \;
: 1470236281:0;more ../wellbeingid/rds.tf
: 1470236288:0;more ./wellbeingid/rds.tf
: 1470236511:0;find . -name "*.tf" -print -exec grep -i security_group {} \; 
: 1470236573:0;more ./walkadoo/rds.tf
: 1470236597:0;vi rds.t
: 1470236607:0;rm rds.td
: 1470237590:0;mv hello200_production_936_show_all_variables_aug_03_2016.txt Documents/postgres
: 1470237609:0;more aws-dms-quitnet-prod-db-version-issue-070816.txt
: 1470237631:0;history | grep hello200_staging | grep ssh
: 1470237640:0;history | grep staging | grep ssh
: 1470237673:0;history | grep staging | grep ssh | grep 543
: 1470237686:0;ssh -A -L 9999:concierge-staging.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:5432 heidischmidt@10.0.14.127 -N
: 1470237704:0;history | grep 9999
: 1470237722:0;history | grep psql | grep 99
: 1470237783:0;ssh -A -L 9999:concierge-production.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:5432 heidischmidt@10.0.14.127 -N
: 1470237792:0;psql -E -h 127.0.0.1 -p 9999 -U concierge_app --dbname=concierge_production
: 1470237822:0;history | grep analysis | grep ssh
: 1470237831:0;ssh -A -L 8888:analysis-warehouse-production.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:5432 heidischmidt@ec2-54-242-121-10.compute-1.amazonaws.com -N
: 1470237847:0;history | grep psql | grep consolidate
: 1470237880:0;psql -E -h 127.0.0.1 -p 8888 -U meyouhealth  --dbname=consolidated
: 1470237924:0;mv analysis_datawarehouse_prod_show_all_aug_03_2016.txt Documents/postgres/
: 1470237936:0;diff -u hello200_production_936_show_all_variables_aug_03_2016.txt analysis_datawarehouse_prod_show_all_aug_03_2016.txt
: 1470237959:0;tail hello200_production_936_show_all_variables_aug_03_2016.txt
: 1470237963:0;tail analysis_datawarehouse_prod_show_all_aug_03_2016.txt
: 1470238752:0;paste analysis_datawarehouse_prod_show_all_aug_03_2016.txt hello200_production_936_show_all_variables_aug_03_2016.txt
: 1470239647:0;vi rds_ext_936
: 1470239666:0;vi rds_ext_947
: 1470239693:0;paste rds_ext_936 rds_ext_947
: 1470239728:0;cat rds_ext_936
: 1470239733:0;cat rds_ext_947
: 1470254374:0;ssh ubuntu@ec2-54-243-58-41.compute-1.amazonaws.com
: 1470254565:0;shipit env config_list walkadoo-staging
: 1470254588:0;shipit env config_list walkadoo-staging | grep -i data
: 1470319748:0;shipit deployment create reporting-production e297140
: 1470320054:0;shipit deployment logs 3756
: 1470320122:0;shipit deployment logs 3756 > Documents/reporting/shipit-deploy-after-upgrade-mysql-5629-080416-log-3756.log 
: 1470321164:0;shipit env config_list  wilder-staging
: 1470321274:0;shipit env config_set  wilder-staging WILDER_DB_HOST shared-staging-mysql.cduvhvvpw6oo.us-east-1.rds.amazonaws.com
: 1470321291:0;shipit env config_list  wilder-staging | grep DB_HOST
: 1470321307:0;shipit env show  wilder-staging 
: 1470321325:0;shipit ssh wilder-staging
: 1470321547:0;shipit console wilder-staging
: 1470322212:0;cd mysql
: 1470322230:0;vi migrate-wilder-staging-to-shared-mysql-rake-task-output.log
: 1470322848:0;shipit env show wilder-staging
: 1470322867:0;shipit deployment create wilder-staging bf65aae6f60fa2ff8f39878663bf2b0c0b0c1552
: 1470323343:0;shipit deployment logs 3759 > Documents/mysql/shipit-deploy-wilder-after-migrate-shared-mysql-staging-080416-log-3759.log 
: 1470323362:0;shipit deployment logs 3759 > ~/Documents/mysql/shipit-deploy-wilder-after-migrate-shared-mysql-staging-080416-log-3759.log 
: 1470323372:0;more shipit-deploy-wilder-after-migrate-shared-mysql-staging-080416-log-3759.log
: 1470323431:0;shipit env show  wellbeingtracker-staging 
: 1470323452:0;shipit env config_list  wellbeingtracker-staging | grep DB
: 1470323492:0;shipit env config_set  wellbeingtracker-staging DATABASE_HOST shared-staging-mysql.cduvhvvpw6oo.us-east-1.rds.amazonaws.com
: 1470323504:0;shipit env config_set  wellbeingtracker-staging PASSIVE_DATABASE_HOST shared-staging-mysql.cduvhvvpw6oo.us-east-1.rds.amazonaws.com
: 1470323515:0;shipit console wellbeingtracker-staging 
: 1470323761:0;shipit env config_set  wellbeingtracker-staging DATABASE_USERNAME wbt_stg_app
: 1470323858:0;vi migrate-wbt-staging-to-shared-mysql-rake-task-output.log
: 1470331417:0;shipit env show we3llbeingtracker-staging
: 1470331449:0;shipit deployment create wellbeingtracker-staging 655fc0ac0bed855ee6ae05f9162b05517f4b01e1
: 1470331473:0;shipit env config_list wellbeingtracker-staging | grep DATA
: 1470331701:0;shipit deployment logs 3761
: 1470332765:0;shipit env show dc-staging
: 1470333022:0;knife environment edit dc-staging 
: 1470333047:0;history | grep knife | grep dc | grep staging
: 1470333073:0;knife data bag show container-config dc-staging
: 1470333116:0;knife data bag show dc-staging | grep -i DATABASE
: 1470333130:0;knife data bag show  dc-staging
: 1470333142:0;history | grep knife | grep dc | grep staging | grep config
: 1470333151:0;knife data bag list container-config dc-staging
: 1470333167:0;knife data bag list  dc-staging
: 1470333350:0;shipit env config_list wellbeingid-staging | grep -i redi
: 1470340220:0;shipit env config_list rza-staging 
: 1470340250:0;shipit env config_unset rza-staging DATABASE_URL
: 1470340265:0;shipit env config_set rza-staging  DATABASE_URL mysql2://rza_staging_app:S2P3Zby5qYukKEn9RgPXBTmK@shared-staging-mysql.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306/rza_staging
: 1470340268:0;shipit env config_list rza-staging | grep -i data 
: 1470340292:0;shipit env show rza-staging
: 1470340320:0;shipit deployment create rza-staging cb29648ce14e3d0a8f3fdee38c8599794eb694ae
: 1470406048:0;ssh ubuntu@ec2-54-146-212-108.compute-1.amazonaws.com
: 1470406607:0;history | grep knife | grep dc | grep staging 
: 1470640694:0;scp ubuntu@ec2-54-225-238-195.compute-1.amazonaws.com:*.log .
: 1470640717:0;scp ubuntu@ec2-54-225-238-195.compute-1.amazonaws.com:dmesg_0312AM_08062016-wbid-prod-all-in-one.log .
: 1470640735:0;scp ubuntu@ec2-54-225-238-195.compute-1.amazonaws.com:docker-ps-info0312AM_08062016-wbid-prod-all-in-one.log .
: 1470640755:0;scp ubuntu@ec2-54-225-238-195.compute-1.amazonaws.com:sudo-docker-logs-container-05071a302276.log .
: 1470640774:0;scp ubuntu@ec2-54-225-238-195.compute-1.amazonaws.com:info-docker-logs-diff-than-redirect-output.log .
: 1470640823:0;cd ..wbid
: 1470640853:0;vi sudo-docker-ps-output-timing-info-container-restarted.log
: 1470667952:0;history | grep chef | grep dc-prod
: 1470667997:0;knife ssh chef_environment:dc-production "grep portforw /etc/passwd" -x ubuntu
: 1470668052:0;knife ssh chef_environment:dc-production "grep openvas /etc/passwd" -x ubuntu
: 1470668198:0;knife ssh -x ubuntu "chef_environment:dc-production AND role:dc_web" "grep port /etc/passwd"
: 1470668215:0;knife ssh -x ubuntu "chef_environment:dc-production AND role:dc_web" "grep portforwar /etc/passwd"
: 1470668333:0;knife ssh -x ubuntu "chef_environment:dc-production" "sudo useradd -s /bin/bash -o -u 22121 -d /home/openvas -m  openvas"
: 1470668343:0;knife ssh -x ubuntu "chef_environment:dc-production" "grep portforwar /etc/passwd"
: 1470668354:0;knife ssh -x ubuntu "chef_environment:dc-production" "grep openvas /etc/passwd"
: 1470668770:0;more dmesg_0312AM_08062016-wbid-prod-all-in-one.log
: 1470668810:0;tail  dmesg_0312AM_08062016-wbid-prod-all-in-one.log
: 1470668816:0;tail -100  dmesg_0312AM_08062016-wbid-prod-all-in-one.log
: 1470672256:0;cd myh-terraform/environments/production/reporting-dashboard
: 1470672267:0;grep -i reporting-app *
: 1470672293:0;cd research
: 1470681646:0;iam_policy_mod-reporting-app.080816.txt
: 1470681649:0;vi iam_policy_mod-reporting-app.080816.txt
: 1470682497:0;cat iam_policy_mod-reporting-app.080816.txt
: 1470682750:0;cp iam_policy_mod-reporting-app.080816.txt proposed-iam-policy-mods-reporting-app.080816.txt
: 1470682982:0;vi proposed-iam-policy-mods-reporting-app.080816.txt
: 1470683250:0;cat proposed-iam-policy-mods-reporting-app.080816.txt
: 1470764541:0;shipit ssh wbt-production
: 1470764555:0;shipit ssh wellbeingtracker-production
: 1470834206:0;ssh ubuntu@ec2-54-242-121-10.compute-1.amazonaws.com
: 1470834249:0;ssh ubuntu@ec2-54-90-52-244.compute-1.amazonaws.com
: 1470840090:0;ssh ubuntu@ec2-54-242-47-84.compute-1.amazonaws.com
: 1470840547:0;ssh ubuntu@ec2-54-90-17-51.compute-1.amazonaws.com
: 1470842729:0;history | grep dc-statging
: 1470842736:0;history | grep knife
: 1470856606:0;pip install asyncpg
: 1470856740:0;history | grpe terraform
: 1470856745:0;history | grep terraform 
: 1470856758:0;history | grep brew
: 1470905248:0;history | grep knife | grep dc-production
: 1470905307:0;knife env show dc-production --format json > ~/Documents/dc/dc-prod-b4-vpc-mods.081116.txt
: 1470905355:0;knife environment show dc-production --format json > ~/Documents/dc/dc-prod-b4-vpc-mods.081116.txt
: 1470905551:0;knife environment edit dc-production 
: 1470905633:0;knife environment show dc-production --format json > ~/Documents/dc/dc-prod-after-vpc-mods.081116.txt
: 1470905644:0;diff -u ~/Documents/dc/dc-prod-b4-vpc-mods.081116.txt ~/Documents/dc/dc-prod-after-vpc-mods.081116.txt
: 1470906304:0;echo " CALL mysql.rds_stop_replication;" 
: 1470906314:0;echo " CALL mysql.rds_stop_replication;"
: 1470908488:0;history | grep dc-prod | grep ssh
: 1470908506:0;ssh -A -L 9998:dc-production-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 heidischmidt@ec2-54-226-92-154.compute-1.amazonaws.com -N
: 1470908527:0;history | grep mysql | grep 9998
: 1470908624:0;ssh -A -L 9998:dc-production-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 heidischmidt@ec2-54-242-121-10.compute-1.amazonaws.com -N
: 1470908639:0;mysql -u meyouhealth -p -P 9998 -h 127.0.0.1
: 1470908861:0;ssh -A -L 9997:dc-production.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 heidischmidt@ec2-54-242-121-10.compute-1.amazonaws.com -N
: 1470908883:0;mysql -u meyouhealth -p -P 9997 -h 127.0.0.1
: 1470923880:0;shipit env config_set reporting-production DATABASES_DC_URL       mysql2://reporting:vK4NA4X%236ygK%29rX8k%28jazwCa@dc-production3-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306/meyouhealth_production
: 1470924314:0;shipit deployment create reporting-production 6811bb9
: 1470924795:0;shipit deployment logs 3161
: 1470924943:0;rbenv local 2.15
: 1470924958:0;rbenv local 2.1.15
: 1470924990:0;shipit env show reporting-production 
: 1470925741:0;shipit deployment create reporting-production e7f1afd64a147d91e1ed33508c6c450d99b315b8
: 1470926519:0;shipit deployment create reporting-production 020055b
: 1471026571:0;shipit ssh wilder-production
: 1471026649:0;shipit env show wilder-production
: 1471870339:0;gem update shipit
: 1471870412:0;shipit --version
: 1471870425:0;env | grep -i SHIP
: 1471874264:0;cd Documents/ShipIt
: 1471874274:0;env > original_env_pre_1.1.0
: 1471895470:0;ssh -A -L 9999:127.0.0.1::9999 ubuntu@52.86.252.208 ssh -L 9999:shared-staging-postgres.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:5432 -N ubuntu@10.0.17.163
: 1471895550:0;history | grep psql | grep 999
: 1471895638:0;psql -E -h 127.0.0.1 -p 9999 -U meyouhealth  --dbname=postgres
: 1471895742:0;ssh -A -L 9999:127.0.0.1:9999 ubuntu@52.86.252.208 ssh -L 9999:shared-staging-postgres.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:5432 -N ubuntu@10.0.17.163
: 1471974082:0;shipit deployment logs 3844
: 1471974203:0;shipit deployment logs 3845
: 1471974246:0;shipit deployment logs 3845 | grep -i node
: 1471974768:0;ssh -A -L 9991:127.0.0.1:9991 ubuntu@52.86.252.208 ssh -L 9991:shared-staging-postgres.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:5432 -N ubuntu@10.0.17.163
: 1472044960:0;ssh -A -L 9990:127.0.0.1:9990 ubuntu@52.86.252.208 ssh -L 9990:shared-staging-postgres.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:5432 -N ubuntu@10.0.17.163
: 1472044990:0;ssh -A -L 9989:127.0.0.1:9989 ubuntu@52.86.252.208 ssh -L 9989:shared-staging-postgres.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:5432 -N ubuntu@10.0.17.163
: 1472045037:0;more pg_grants_hello200_prod.sql
: 1472045052:0;cp pg_grants_hello200_prod.sql pg_grants_hello200_stg.sql
: 1472045326:0;vi pg_grants_hello200_stg.sql
: 1472045665:0;ssh shipit-staging
: 1472047417:0;shipit env config_set hello200-staging  DATABASE_HOST=shared-staging-postgres.cduvhvvpw6oo.us-east-1.rds.amazonaws.com
: 1472047467:0;shipit env config_set hello200-staging  DATABASE_HOST shared-staging-postgres.cduvhvvpw6oo.us-east-1.rds.amazonaws.com
: 1472047583:0;shipit env config_set hello200-staging  DATABASE_URL postgres://hello200_staging_app:H6gOzpo1xfwSxdjr@shared-staging-postgres.cduvhvvpw6oo.us-east-1.rds.amazonaws.com/hello200_staging
: 1472047617:0;shipit deployment create help
: 1472047624:0;shipit deployment help
: 1472047642:0;shipit deployment create hello200-staging 9f391b23efbf86eb28d592af4439f76332a87ec4 
: 1472047838:0;shipit deployment logs 3847 
: 1472048345:0;shipit env config_list hello200-staging | grep -i data
: 1472048459:0;vi create-h200-stg-app-shared-stg.log
: 1472048940:0;shipit env config_list iris-staging
: 1472048948:0;shipit env config_list iris-staging | grep -i data
: 1472049096:0;cp pg_grants_hello200_stg.sql pg_grants_iris_stg.sql 
: 1472049101:0;vi pg_grants_iris_stg.sql
: 1472049247:0;cat pg_grants_iris_stg.sql
: 1472049765:0;shipit env config_set iris-staging DATABASE_URL postgres://iris_staging_app:Jg3UFDAinKHRerW6@shared-staging-postgres.cduvhvvpw6oo.us-east-1.rds.amazonaws.com/iris_staging
: 1472049769:0;shipit env config_list iris-staging | grep DATABASE_URL 
: 1472049781:0;shipit env show iris-staging 
: 1472049954:0;shipit deployment create iris-staging 246a5be7643e6f933db07686c2a0aa3c40b2d635 
: 1472049990:0;shipit env show iris-staging
: 1472050031:0;shipit deployment logs 3848
: 1472050331:0;shipit env config_list iris-staging | grep DATABASE_URL
: 1472050563:0;vi create-iris-stg-app-shared-stg.log
: 1472050725:0;shipit env config_list quitnet-staging
: 1472050987:0;cp pg_grants_iris_stg.sql pg_grants_qn_stg.sql
: 1472051092:0;psql -E -h 127.0.0.1 -p 9989 -U meyouhealth  --dbname=postgres
: 1472051666:0;cat pg_grants_qn_stg.sql
: 1472051673:0;vi pg_grants_qn_stg.sql
: 1472051771:0;shipit env config_set quitnet-staging DATABASE_HOST shared-staging-postgres.cduvhvvpw6oo.us-east-1.rds.amazonaws.com
: 1472051804:0;shipit env config_set quitnet-staging DATABASE_URL postgres://quitnet_staging_app:LNC6Ke5bpQG3f3@shared-staging-postgres.cduvhvvpw6oo.us-east-1.rds.amazonaws.com/quitnet_staging
: 1472051992:0;shipit deployment create quitnet-staging 4cf79f109b1d78c0836bd258d2e79158a969b305
: 1472052328:0;shipit env show quitnet-staging
: 1472052343:0;shipit deployment logs 3849
: 1472052521:0;ssh ubuntu@ec2-54-225-238-195.compute-1.amazonaws.com
: 1472052711:0;vi create-qn-stg-app-shared-stg.log
: 1472053047:0;shipit env config_list quitnet-staging | grep -i DATA
: 1472060165:0;ssh -A -L 9000:127.0.0.1:9000 ubuntu@54.152.155.24 ssh -L 9000:walkadoo-production2.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N ubuntu@10.0.14.132
: 1472060184:0;ssh -A -L 9200:127.0.0.1:9200 ubuntu@54.152.155.24 ssh -L 9200:walkadoo-production2.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N ubuntu@10.0.14.132
: 1472060193:0;cd ../mysql
: 1472060206:0;history | grep mysql | grep -P
: 1472060212:0;history | grep mysql | grep 9
: 1472060249:0;mysql -u meyouhealth -p -P 9200 -h 127.0.0.1
: 1472065180:0;rbenv loical 2.2.0
: 1472065199:0;history | grep knife | grep daily
: 1472065220:0;history | grep knife | grep show
: 1472065233:0;knife environment show dc-staging --format json
: 1472071281:0;git checkout environments/staging/daily-challenge/terraform.tfstate
: 1472071287:0;git checkout environments/staging/walkadoo/elasticache.tf
: 1472071298:0;more environments/staging/walkadoo/walkadoo.tf
: 1472071303:0;git diff environments/staging/walkadoo/walkadoo.tf
: 1472071336:0;git diff environments/staging/walkadoo/walkadoo.tf > diff-wd-tf-security-grp-ports
: 1472071350:0;git checkout environments/staging/walkadoo/walkadoo.tf
: 1472071377:0;cd environments/staging/wellbeingid
: 1472133036:0;knife environment show dc-production --format json
: 1472133294:0;more pg_grants_hello200_stg.sql
: 1472133321:0;lsa -ltra
: 1472133359:0;more pg_grants_qn_stg.sql
: 1472150456:0;history | grep export
: 1472150579:0;history | grep env
: 1472150602:0;find . -name "original_env_pre_1.1.0" -print
: 1472150620:0;more ./Documents/ShipIt/original_env_pre_1.1.0
: 1472150649:0;grep DEP ./Documents/ShipIt/original_env_pre_1.1.0
: 1472150663:0;grep MYH ./Documents/ShipIt/original_env_pre_1.1.0
: 1472150669:0;DEPLOYMENT_API_URL=https://deployment-api.meyouhealth.com
: 1472150669:0;DEPLOYMENT_API_PASSWORD=qqY7TTBWvZf7Kk3RbbD5bhpL
: 1472150676:0;MYH_BUILDER_PASSWORD=JXvJpVb443MRdENRDux5qDCn
: 1472150737:0;more .oh-my-zsh
: 1472150744:0;more .oh-my-zsh/oh-my-zsh.sh
: 1472150826:0;more Documents/ShipIt/original_env_pre_1.1.0
: 1472150861:0;export SHIPIT_USERNAME=heidischmidt
: 1472150965:0;ssh shipit-production
: 1472152490:0;cd ~/.berkshelf
: 1472152496:0;cd cookbooks
: 1472152509:0;cd nginx-2.7.
: 1472152515:0;cd nginx-2.7.2
: 1472152529:0;cd ubuntu
: 1472152549:0;find . -name "nginx.conf*" -print
: 1472152557:0;more ./templates/default/nginx.conf.erb
: 1472152564:0;grep server ./templates/default/nginx.conf.erb
: 1472152719:0;cd templates/default
: 1472152726:0;grep server_token
: 1472152730:0;grep server_token *
: 1472152749:0;grep server_token ~/.berkshelf/cookbooks/nginx-2.7.2/templates/default/nginx.conf.erb
: 1472154832:0;ssh -A -L 9200:127.0.0.1:9200 ubuntu@54.152.155.24 ssh -L 9200:wd-prd-replica-online-ddl-test.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N ubuntu@10.0.14.132
: 1472154853:0;ssh -A -L 9300:127.0.0.1:9300 ubuntu@54.152.155.24 ssh -L 9300:wd-prd-replica-online-ddl-test.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N ubuntu@10.0.14.132
: 1472154876:0;mysql -u meyouhealth -p -P 99300 -h 127.0.0.1 
: 1472154922:0;mysql -u meyouhealth -p -P 9300 -h 127.0.0.1 
: 1472218726:0;ssh -A -L 9400:127.0.0.1:9400 ubuntu@54.152.155.24 ssh -L 9400:wd-prd-replica-online-ddl-test.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N ubuntu@10.0.14.132
: 1472219122:0;ssh -A -L 9400:127.0.0.1:9400 ubuntu@54.152.155.24 ssh -L 9400:wd-prd-replica-online-ddl-test.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N ubuntu@10.0.5.36
: 1472219142:0;mysql -u meyouhealth -p -P 9400 -h 127.0.0.1 
: 1472219380:0;cp mysql_compare_connection_collation.sql mysql_online_ddl_testing.sql 
: 1472219386:0;vi mysql_online_ddl_testing.sql
: 1472220404:0;vi mysql_online_ddl_wd_user_cards.sql 
: 1472220892:0;mv mysql_online_ddl_wd_user_cards.sql mysql_test_online_ddl_wd_user_cards.sql
: 1472220915:0;more mysql_test_online_ddl_wd_user_cards.sql
: 1472220964:0;cd Documents/mysql 
: 1472220974:0;more mysql_online_ddl_testing.sql
: 1472221022:0;history | grep while | grep mysql 
: 1472221058:0;mv mysql_online_ddl_testing.sql mysql_online_ddl_testing_perf_stats_gather.sql
: 1472221111:0;while true ; do echo "$(date)" ;mysql -u meyouhealth -P 9400 -h 127.0.0.1 < mysql_online_ddl_testing_perf_stats_gather.sql > mysql_online_ddl_testing_perf_stats_gather-`date +"%Y%m%d-%s"`.log ; sleep 360;  done
: 1472221189:0;mysql -u meyouhealth  -P 9400 -h 127.0.0.1 -vvv < mysql_test_online_ddl_wd_user_cards.sql > mysql_test_online_ddl_wd_user_cards.log 
: 1472221688:0;cd Documents/mysql
: 1472221703:0;more mysql_online_ddl_testing_perf_stats_gather-20160826-1472221111.log
: 1472221951:0;more mysql_test_online_ddl_wd_user_cards.log
: 1472222037:0;vi mysql_test_online_ddl_wd_user_cards.sql
: 1472222084:0;more mysql_online_ddl_testing_perf_stats_gather-20160826-1472221832.log
: 1472222125:0;vi mysql_test_online_ddl_wd_user_cards.log
: 1472222301:0;more mysql_online_ddl_testing_perf_stats_gather-20160826-1472222193.log
: 1472222471:0;more mysql_online_ddl_testing_perf_stats_gather-20160826-1472222334.log
: 1472222506:0;cp mysql_test_online_ddl_wd_user_cards.sql mysql_replica_online_ddl_wd_user_cards.sql
: 1472222511:0;vi mysql_replica_online_ddl_wd_user_cards.sql
: 1472222566:0;cat mysql_replica_online_ddl_wd_user_cards.sql
: 1472223452:0;mysql -u meyouhealth  -P 9400 -h 127.0.0.1 -vvv < mysql_replica_online_ddl_wd_user_cards.sql > mysql_replica_online_ddl_wd_user_cards.log
: 1472223554:0;more mysql_replica_online_ddl_wd_user_cards.log
: 1472235961:0;shipit help
: 1472235976:0;shipit environment help 
: 1472235989:0;shipit environment show quitnet-production
: 1472236015:0;shipit environment update help
: 1472236019:0;shipit environment update 
: 1472236037:0;history | grep hipchat
: 1472240170:0;cp mysql_replica_online_ddl_wd_user_cards.sql mysql_covering_idx_replica_online_ddl_wd_user_cards.sql
: 1472240354:0;while true ; do echo "$(date)" ;mysql -u meyouhealth -P 9400 -h 127.0.0.1 < mysql_online_ddl_testing_perf_stats_gather.sql > mysql_online_ddl_testing_perf_stats_gather-`date +"%Y%m%d-%s"`.log ; sleep 30;  done
: 1472240479:0;mysql -u meyouhealth  -P 9400 -h 127.0.0.1 -vvv < mysql_covering_idx_replica_online_ddl_wd_user_cards.sql > mysql_covering_idx_replica_online_ddl_wd_user_cards.log 
: 1472243483:0;shipit env config_list quitnet-production | grep REDIS
: 1472243977:0;more mysql_covering_idx_replica_online_ddl_wd_user_cards.log
: 1472247282:0;history | grep aws | grep "rds desc"
: 1472247330:0;aws rds describe-db-instances --db-instance-identifier wd-prd-replica-online-ddl-test
: 1472247417:0;aws rds describe-db-instances --db-instance-identifier wd-prd-replica-online-ddl-test > wd-prd-replica-online-ddl-test-aws-rds-info.txt
: 1472481083:0;mkdir logs_test
: 1472481088:0;mv *.log logs_test/
: 1472481097:0;history | grep ssh | grep L
: 1472481149:0;ssh -A -L 9400:127.0.0.1:9400 ubuntu@54.152.155.24 ssh -L 9400:walkadoo-production2.cduvhvvpw6oo.us-east-1.rds.amazonaws.com3306 -N ubuntu@10.0.5.36
: 1472481158:0;ssh -A -L 9400:127.0.0.1:9400 ubuntu@54.152.155.24 ssh -L 9400:walkadoo-production2.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N ubuntu@10.0.5.36
: 1472481190:0;ssh -A -L 9500:127.0.0.1:9500 ubuntu@54.152.155.24 ssh -L 9500:walkadoo-production2.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N ubuntu@10.0.5.36
: 1472481271:0;history | grep while 
: 1472481285:0;more mysql_online_ddl_testing_perf_stats_gather.sql
: 1472481294:0;vi mysql_online_ddl_testing_perf_stats_gather.sql
: 1472481357:0;while true ; do echo "$(date)" ;mysql -u meyouhealth -P 9500 -h 127.0.0.1 < mysql_online_ddl_testing_perf_stats_gather.sql > mysql_online_ddl_testing_perf_stats_gather-`date +"%Y%m%d-%s"`.log ; sleep 30;  done
: 1472481369:0;more mysql_online_ddl_testing_perf_stats_gather-20160829-1472481357.log
: 1472481457:0;more mysql_covering_idx_replica_online_ddl_wd_user_cards.sql
: 1472481464:0;mysql -u meyouhealth  -P 9500 -h 127.0.0.1 -vvv < mysql_covering_idx_replica_online_ddl_wd_user_cards.sql > mysql_covering_idx_replica_online_ddl_wd_user_cards_production.log
: 1472481577:0;more mysql_online_ddl_testing_perf_stats_gather-20160829-1472481542.log
: 1472481684:0;more mysql_covering_idx_replica_online_ddl_wd_user_cards_production.log
: 1472482373:0;vi mysql_covering_idx_replica_online_ddl_wd_user_cards.sql
: 1472482445:0;mysql -u meyouhealth  -P 9500 -h 127.0.0.1 -vvv < mysql_covering_idx_replica_online_ddl_wd_user_cards.sql > mysql_covering_idx_replica_online_ddl_wd_user_cards_production_take2.log
: 1472482633:0;more mysql_covering_idx_replica_online_ddl_wd_user_cards_production_take2.log
: 1472482692:0;mkdir logs_prod
: 1472482696:0;mv *.log logs_prod
: 1472504177:0;more rds_ext_947
: 1472504193:0;grep wal_buffers analysis_datawarehouse_prod_show_all_aug_03_2016.txt
: 1472504204:0;grep wal_buffers hello200_production_936_show_all_variables_aug_03_2016.txt
: 1472505526:0;grep wal_ hello200_production_936_show_all_variables_aug_03_2016.txt
: 1472505642:0;grep dir hello200_production_936_show_all_variables_aug_03_2016.txt
: 1472505653:0;grep rds hello200_production_936_show_all_variables_aug_03_2016.txt
: 1472505934:0;grep archive_mode hello200_production_936_show_all_variables_aug_03_2016.txt
: 1472506491:0;grep restore hello200_production_936_show_all_variables_aug_03_2016.txt
: 1472506758:0;aws rds create-db-instance-read-replica --help
: 1472506767:0;aws rds create-db-instance-read-replica help
: 1472507359:0;grep pause hello200_production_936_show_all_variables_aug_03_2016.txt
: 1472509124:0;grep max_wal hello200_production_936_show_all_variables_aug_03_2016.txt
: 1472509250:0;grep wal_keep hello200_production_936_show_all_variables_aug_03_2016.txt
: 1472509690:0;grep application hello200_production_936_show_all_variables_aug_03_2016.txt
: 1472509907:0;history | grep psql | grep 127
: 1472510065:0;psql -E -h 127.0.0.1 -p 9990 -U meyouhealth  --dbname=postgres
: 1472510272:0;grep cont hello200_production_936_show_all_variables_aug_03_2016.txt
: 1472510286:0;grep sync hello200_production_936_show_all_variables_aug_03_2016.txt
: 1472564701:0;ssh -A -L 9900:wellbeingid-production.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:5432 heidischmidt@54.225.238.195 -N
: 1472564726:0;psql -E -h 127.0.0.1 -p 9900 -U meyouhealth  --dbname=postgres
: 1472571137:0;pog_dump -V
: 1472571141:0;pg_dump -V
: 1472571194:0;more postgres_h200_reset_lastvalue_sequences.sql
: 1472571214:0;more postgres_pg_dump_mods_snippet.sql
: 1472571391:0;pg_dump --clean --create --file=pg_dump_wbid-original-data-dump-inserts.sql --verbose --column-inserts
: 1472571408:0;vi pg_dump_ex.sql 
: 1472571421:0;cat pg_dump_ex.sql
: 1472571434:0;pg_dump --clean --create --file=pg_dump_wbid-original-data-dump-inserts.sql --verbose --column-inserts --host=127.0.0.1 --port=9900 --username=wellbeingid_production_app --password --dbname=wellbeingid_production
: 1472578511:0;tail pg_dump_wbid-original-data-dump-inserts.sql
: 1472578525:0;which pgbasebackup
: 1472578530:0;which pg_basebackup
: 1472578537:0;/usr/local/bin/pg_basebackup --help
: 1472578576:0;mkdir pgdata_wbid
: 1472578908:0;df -Ph .
: 1472579353:0;ls -l ../staging
: 1472579363:0;cd ../staging/wellbeingid
: 1472579403:0;mkdir ../../production/wellbeingid/
: 1472579411:0;ls -l ../../production/wellbeingid/
: 1472579417:0;cd ../../production/wellbeingid/
: 1472579435:0;cp ~/Documents/git/myh-terraform/environments/staging/wellbeingid/*.tf .
: 1472579442:0;cp ~/Documents/git/myh-terraform/environments/staging/wellbeingid/*.tfvars .
: 1472579451:0;vi *.tf
: 1472579580:0;grep staging *
: 1472579595:0;cd ../insight
: 1472579628:0;cp rds.tf ../wellbeingid/insight_rds.tf
: 1472579641:0;cat insight_rds.tf >> rds.tf
: 1472579855:0;grep security_group *.tf
: 1472579901:0;cat ../insight/ec2.tf
: 1472579939:0;cp ../insight/ec2.tf insight_ec2.tf
: 1472579950:0;cat insight_ec2.tf >> ec2.tf
: 1472580163:0;rm insight_*.tf
: 1472580167:0;grep insight *.tf
: 1472582139:0;rbenv local 2.1.2
: 1472582143:0;/usr/local/bin/ubuntu-cloud-images
: 1472585878:0;brew help 
: 1472585931:0;locate pgbuffercache.sql 
: 1472585954:0;locate pg_buffercache.sql
: 1472585994:0;brew install postgresql-contrib
: 1472586066:0;ls -ltra /usr/local/Cellar/postgresql/9.5.3/share/postgresql/
: 1472586078:0;ls -ltra /usr/local/Cellar/postgresql/9.5.3/share/
: 1472586083:0;ls -ltra /usr/local/Cellar/postgresql/9.5.3/share/doc
: 1472586087:0;ls -ltra /usr/local/Cellar/postgresql/9.5.3/
: 1472586185:0;ls -ltra /usr/local/Cellar/postgresql/9.5.3/lib
: 1472586191:0;ls -ltra /usr/local/Cellar/postgresql/9.5.3/include
: 1472586203:0;ls -ltra /usr/local/Cellar/postgresql/9.5.3/lib/postgresql
: 1472586260:0;brew link docker
: 1472586274:0;sudo brew link docker
: 1472586297:0;brew link --overwrite docker
: 1472586306:0;brew link --overwrite docker-machine
: 1472586619:0;softwareupdate --list
: 1472587017:0;softwareupdate --install xcode 
: 1479694301:0;aws-info i-b55a26bb
: 1479752357:0;compinit
: 1479752392:0;ls -ltr *.sql 
: 1479752425:0;more db-research-objects-to-create
: 1479752470:0;ls -ltra *.sql
: 1479752526:0;vi check_charset_collation.sql 
: 1479752733:0;more dc-optimize-1.log
: 1479752759:0;more db_status_dc-research-110415.log
: 1479752783:0;ls -l *logfile*
: 1479752789:0;more dc-staging-proc_analyse_columns.logfile
: 1479752830:0;vi dc-staging-proc_analyse_columns.logfile
: 1479752916:0;grep "^select "  dc-staging-proc_analyse_columns.logfile
: 1479752929:0;grep "^select "  dc-staging-proc_analyse_columns.logfile > procedure_analyze_template_dc.sql 
: 1479753059:0;history | grep walkadoo | tail -10 
: 1479753243:0;ssh -A -L 9500:127.0.0.1:9500 heidischmidt@54.152.155.24 ssh -L 9500:walkadoo-production2.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1479753284:0;history | grep mysql | grep 9500
: 1479753297:0;mysql -u meyouhealth  -P 9500 -h 127.0.0.1
: 1479753395:0;vi wd_utf8mb4_work_112116.txt
: 1479753532:0;cp ../dc/*.sql .
: 1479754912:0;history | grep ruby | grep rbenv
: 1479754936:0;rbenv install 2.3.2
: 1479760018:0;cd data
: 1479760024:0;cd migrate
: 1479760033:0;cat schema.rb
: 1479760044:0;cat schema.rb | grep varchar
: 1479760053:0;cat schema.rb | grep var
: 1479760062:0;cat schema.rb | grep t.string
: 1479760076:0;cat schema.rb | grep t.string | sort +1n
: 1479760079:0;cat schema.rb | grep t.string | sort 
: 1479760102:0;cat schema.rb | grep t.string | grep -v 255 
: 1479830029:0;ssh -A -L 9500:127.0.0.1:9500 ubuntu@54.152.155.24 ssh -L 9500:walkadoo-production2.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1479836192:0;ssh -A -L 9600:127.0.0.1:9600 ubuntu@54.152.155.24 ssh -L 9600:walkadoo-production2.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1479836332:0;mv spool-wd-varchar-length-inventory.txt ../walkadoo/
: 1479836342:0;mv wd-slow-log-112216.log ../walkadoo/
: 1479836350:0;more wd-slow-log-112216.log
: 1479836379:0;cat wd-slow-log-112216.log| head -100
: 1479836782:0;cat wd-slow-log-112216.log| grep "2016-11-22 10:00"
: 1479836923:0;mysql -u meyouhealth  -P 9600 -h 127.0.0.1
: 1480345486:0;history | grep shipit 
: 1480346503:0;ssh -A -L 9500:127.0.0.1:9500 ubuntu@54.152.155.24 ssh -L 9500:wd-utf8mb4-test-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1480346527:0;ssh -A -L 9400:127.0.0.1:9400 ubuntu@54.152.155.24 ssh -L 9400:wd-utf8mb4-test-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1480347616:0;more proc_analyse_columns.sql 
: 1480347624:0;more procedure_analyze_template_dc.sql
: 1480347634:0;grep -i CONCATE *.sql 
: 1480347642:0;grep -i CONCATE ../dc/*.sql 
: 1480347681:0;rm db-research-objects-to-create.sql 
: 1480347683:0;grep -i CONCAT ../dc/*.sql 
: 1480347697:0;grep -i CONCAT *.sql 
: 1480347706:0;more proc_analyse_columns.sql
: 1480347789:0;cat /dev/null > proc_analyse_columns.sql
: 1480347795:0;vi proc_analyse_columns.sql
: 1480348052:0;mv proc_analyse_columns.sql wd_proc_analyse_columns.sql
: 1480348112:0;rm wd_proc_analyse_columns_112816.log
: 1480348300:0;grep comments wd_proc_analyse_columns.sql
: 1480348638:0;grep "ALTER" *.sql 
: 1480349105:0;cat wd_alter_table_convert_char_set_collate_utf8mb4.sql | sed s/\|//g 
: 1480349128:0;cat wd_alter_table_convert_char_set_collate_utf8mb4.sql | sed s/\|//g  > wd_alter_table_convert_char_set_collate_utf8mb4_unicode_ci.sql 
: 1480349134:0;rm wd_alter_table_convert_char_set_collate_utf8mb4.sql
: 1480349289:0;more alter_utf8mb4_tables.sql
: 1480349302:0;mv alter_utf8mb4_tables.sql dc_alter_utf8mb4_tables.sql
: 1480349314:0;rm dc_alter_utf8mb4_tables.sql
: 1480349322:0;rm dc-optimize.sql
: 1480349328:0;rm procedure_analyze_template_dc.sql
: 1480349335:0;rm wd_utf8mb4_work_112116.txt
: 1480349365:0;mysql -u meyouhealth  -P 9400 -h 127.0.0.1
: 1480349660:0;cat wd_alter_columns_modify_varchar191.sql | sed s/\|//g  > wd_alter_columns_modify_varchar191_utf8mb4.sql 
: 1480349667:0;rm wd_alter_columns_modify_varchar191.sql
: 1480355801:0;wc -l wd_proc_analyse_columns_112816.log
: 1480355807:0;vi wd_proc_analyse_columns.sql
: 1480355863:0;mysql -u meyouhealth  -P 9400 -h 127.0.0.1 < wd_proc_analyse_columns.sql > wd_proc_analyse_columns_112816.log
: 1480356023:0;head wd_alter_table_char_set_collate_utf8mb4.sql
: 1480356037:0;head wd_alter_table_convert_char_set_collate_utf8mb4_unicode_ci.sql
: 1480356122:0;more wd_proc_analyse_columns_112816.log
: 1480356256:0;echo "20120517001237" | wc -c
: 1480358740:0;vi wd_alter_columns_modify_varchar191_utf8mb4.sql
: 1480359726:0;head -20 wd_alter_columns_modify_varchar191_utf8mb4.sql
: 1480359947:0;vi wd_alter_columns_modify_varchar191_utf8mb4.sql 
: 1480360244:0;mysql -u meyouhealth  -P 9400 -h 127.0.0.1 
: 1480360454:0;mysql -u meyouhealth  -P 9400 -h 127.0.0.1 < wd_alter_columns_modify_varchar191_utf8mb4.sql > wd_alter_columns_modify_varchar191_utf8mb4_112816.log 
: 1480365051:0;head -40 wd_alter_columns_modify_varchar191_utf8mb4.sql
: 1480365589:0;head -45 wd_alter_columns_modify_varchar191_utf8mb4.sql
: 1480365796:0;cat wd_alter_columns_modify_varchar191_utf8mb4.sql
: 1480366056:0;cd mysql 
: 1480366077:0;more mysql_compare_connection_collation.sql
: 1480366715:0;tail wd_alter_columns_modify_varchar191_utf8mb4_112816.log
: 1480366734:0;grep "^2016-" wd_alter_columns_modify_varchar191_utf8mb4_112816.log
: 1480367582:0;more ../dc/*utf8*
: 1480367879:0;vi wd_alter_table_char_set_collate_utf8mb4.sql
: 1480368119:0;mysql -u meyouhealth  -P 9400 -h 127.0.0.1 < wd_alter_table_char_set_collate_utf8mb4.sql > wd_alter_table_char_set_collate_utf8mb4_112816.log 
: 1480368126:0;more wd_alter_table_char_set_collate_utf8mb4_112816.log
: 1480369096:0;vi wd_alter_table_convert_char_set_collate_utf8mb4_unicode_ci.sql
: 1480369136:0;mysql -u meyouhealth  -P 9400 -h 127.0.0.1 -vvv < wd_alter_table_convert_char_set_collate_utf8mb4_unicode_ci.sql > wd_alter_table_convert_char_set_collate_utf8mb4_unicode_ci_112816.log
: 1480369286:0;more wd_alter_columns_modify_varchar191_utf8mb4*.log
: 1480369306:0;more wd_alter_table_convert_char_set_collate_utf8mb4_unicode_ci.sql
: 1480369362:0;history | grep wd_alter_columns_modify_varchar191_utf8mb4.sql
: 1480369395:0;mysql -u meyouhealth  -P 9400 -h 127.0.0.1 -vvv < wd_alter_columns_modify_varchar191_utf8mb4.sql > wd_alter_columns_modify_varchar191_utf8mb4_112816.log
: 1480430438:0;ssh -A -L 9300:127.0.0.1:9300 ubuntu@54.152.155.24 ssh -L 9300:wd-utf8mb4-test-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1480430454:0;mysql -u meyouhealth  -P 9300 -h 127.0.0.1 
: 1480430578:0;more wd_alter_columns_modify_varchar191_utf8mb4_112816.log
: 1480430612:0;more wd_alter_table_convert_char_set_collate_utf8mb4_unicode_ci_112816.log
: 1480431074:0;mysql -u meyouhealth  -P 9300 -h 127.0.0.1 -vvv < preTableInfo.sql > preTableInfo.log 
: 1480431142:0;mysql -u meyouhealth  -P 9300 -h 127.0.0.1 -vvv < preTableInfo.sql > preTableInfo_version1.sql 
: 1480431155:0;cp preTableInfo_version1.sql preTableInfo_version1_modded.sql 
: 1480431161:0;vi preTableInfo_version1_modded.sql
: 1480431325:0;cat preTableInfo_version1_modded.sql | sed s/\| / /g
: 1480431339:0;cat preTableInfo_version1_modded.sql | sed s/^| / /g
: 1480431345:0;cat preTableInfo_version1_modded.sql | sed s/\^| / /g
: 1480431350:0;cat preTableInfo_version1_modded.sql | sed s/\|/ /g
: 1480431379:0;cat preTableInfo_version1_modded.sql | sed s/\|//g
: 1480431392:0;cat preTableInfo_version1_modded.sql | sed s/\|//g > preTableInfo_version1.sql
: 1480431398:0;rm preTableInfo_version1_modded.sql
: 1480431407:0;more preTableInfo_version1.sql
: 1480431537:0;mv preTableInfo_version1.sql wd_mods_utf8mb4_all.sql 
: 1480444092:0;mysql -u meyouhealth  -P 9300 -h 127.0.0.1 -vvv < wd_mods_utf8mb4_all.sql > end_to_end_test-most-191-wd-chg-convert.log
: 1480444121:0;more wd_proc_analyse_columns.sql
: 1480446043:0;cat wd_alter_table_char_set_collate_utf8mb4.sql
: 1480446060:0;cat wd_mods_utf8mb4_all.sql
: 1480447254:0;cat end_to_end_test-most-191-wd-chg-convert.log
: 1480447282:0;grep "^--" wd_mods_utf8mb4_all.sql
: 1480447333:0;vi wd_leftovers.sql 
: 1480447533:0;mysql -u meyouhealth  -P 9300 -h 127.0.0.1 -vvv < wd_leftovers.sql > end_to_end_wd_leftovers.log
: 1480447539:0;more end_to_end_wd_leftovers.log
: 1480447671:0;more end_to_end_test-most-191-wd-chg-convert.log
: 1480448809:0;ssh -A -L 9200:127.0.0.1:9200 ubuntu@54.152.155.24 ssh -L 9200:wd-utf8mb4-test-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1480448827:0;mysql -u meyouhealth  -P 9200 -h 127.0.0.1 
: 1480451950:0;mysql -u meyouhealth  -P 9100 -h 127.0.0.1 
: 1480452414:0;history | grep wd_leftovers.sql
: 1480452427:0;mysql -u meyouhealth  -P 9300 -h 127.0.0.1 -vvv < wd_leftovers.sql > fresh_end_to_end_wd_leftovers.log
: 1480452446:0;mysql -u meyouhealth  -P 9100 -h 127.0.0.1 -vvv < wd_leftovers.sql > fresh_end_to_end_wd_leftovers.log
: 1480520252:0;ssh -A -L 9000:127.0.0.1:9000 ubuntu@54.152.155.24 ssh -L 9000:wd-utf8mb4-test-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1480520275:0;ssh -A -L 9900:127.0.0.1:9900 ubuntu@54.152.155.24 ssh -L 9900:wd-utf8mb4-test-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1480520307:0;ssh -A -L 9100:127.0.0.1:9100 ubuntu@54.152.155.24 ssh -L 9100:wd-utf8mb4-test-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1480520337:0;ssh -A -L 8100:127.0.0.1:8100 ubuntu@54.152.155.24 ssh -L 8100:wd-utf8mb4-test-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1480520461:0;vi wd_mods_utf8mb4_all.sql
: 1480520546:0;mysql -u meyouhealth  -P 8100 -h 127.0.0.1 -vvv < wd_mods_utf8mb4_all.sql > fresh_end_to_end_wd_mods_utf8mb4_all.log
: 1480520773:0;head -100 wd_mods_utf8mb4_all.sql
: 1480520783:0;head -110 wd_mods_utf8mb4_all.sql
: 1480520797:0;head -110 wd_mods_utf8mb4_all.sql > leftovers_2.sql 
: 1480520802:0;vi leftovers_2.sql
: 1480520860:0;mv wd_leftovers.sql wd_leftovers_old.sql 
: 1480520866:0;mv leftovers_2.sql wd_leftovers.sql
: 1480522800:0;vagrant --help
: 1480522926:0;more /Users/heidischmidt/.rbenv/shims/vagrant
: 1480522939:0;mv /Users/heidischmidt/.rbenv/shims/vagrant /Users/heidischmidt/.rbenv/shims/vagrant.old.method
: 1480522943:0;which vagrant
: 1480522951:0;/usr/local/bin/vagrant --help
: 1480523065:0;terraform --help
: 1480523183:0;which kops
: 1480523858:0;rm ./deis
: 1480523865:0;cd /usr/local/bin/
: 1480523869:0;ls -ltr deis
: 1480523877:0;cd /Users/heidischmidt/deis
: 1480523880:0;cd /Users/heidischmidt/
: 1480523883:0;ls -ltra deis
: 1480523968:0;history | grep aws | grep python
: 1480523973:0;history | grep py
: 1480523983:0;history | grep aws
: 1480524420:0;file kops-darwin-amd64
: 1480524431:0;chmod +x kops-darwin-amd64
: 1480524442:0;./kops-darwin-amd64 --help
: 1480524464:0;brew search kops
: 1480524475:0;brew install kops
: 1480524683:0;which kubectl --version
: 1480524688:0;which kubectl --help
: 1480524692:0;which kubectl -h
: 1480524696:0;which kubectl
: 1480524701:0;kubectl -h
: 1480531271:0;mysql -u meyouhealth  -P 8100 -h 127.0.0.1 < wd_leftovers.sql > fresh_end_to_end_wd_leftovers.log
: 1480531307:0;mysql -u meyouhealth  -P 8100 -h 127.0.0.1 -vvv < wd_leftovers.sql > fresh_end_to_end_wd_leftovers.log
: 1480531319:0;more fresh_end_to_end_wd_leftovers.log
: 1480531418:0;mysql -u meyouhealth  -P 8100 -h 127.0.0.1 -vvv -e "select TABLE_SCHEMA, TABLE_NAME, COLUMN_NAME,  ORDINAL_POSITION, COLUMN_DEFAULT, IS_NULLABLE,  CHARACTER_SET_NAME, COLLATION_NAME , COLUMN_TYPE, COLUMN_KEY FROM INFORMATION_SCHEMA.COLUMNS where table_schema like "%production%" and CHARACTER_MAXIMUM_LENGTH > 190;" > verification_wd_utf8mb4_list.log
: 1480531458:0;mysql -u meyouhealth  -P 8100 -h 127.0.0.1 -vvv -e ""select TABLE_SCHEMA, TABLE_NAME, COLUMN_NAME,  ORDINAL_POSITION, COLUMN_DEFAULT, IS_NULLABLE,  CHARACTER_SET_NAME, COLLATION_NAME , COLUMN_TYPE, COLUMN_KEY FROM INFORMATION_SCHEMA.COLUMNS where table_schema like "%production%" and CHARACTER_MAXIMUM_LENGTH > 190;"" > verification_wd_utf8mb4_list.log
: 1480531489:0;mysql -u meyouhealth  -P 8100 -h 127.0.0.1 -vvv -e "select TABLE_SCHEMA, TABLE_NAME, COLUMN_NAME,  ORDINAL_POSITION, COLUMN_DEFAULT, IS_NULLABLE,  CHARACTER_SET_NAME, COLLATION_NAME , COLUMN_TYPE, COLUMN_KEY FROM INFORMATION_SCHEMA.COLUMNS where table_schema like '%production%' and CHARACTER_MAXIMUM_LENGTH > 190;" > verification_wd_utf8mb4_list.log
: 1480531494:0;more verification_wd_utf8mb4_list.log
: 1480531536:0;mysql -u meyouhealth  -P 8100 -h 127.0.0.1 -vvv < preTableInfo.sql > verification_post_assessment.sql 
: 1480531608:0;echo "change to run before convert" > README_wd_utf8
: 1480531933:0;mysql -u meyouhealth  -P 8100 -h 127.0.0.1 
: 1480605786:0;more verification_post_assessment.sql
: 1480605968:0;vi 190
: 1480605976:0;rm 190
: 1480606030:0;cp wd_mods_utf8mb4_all.sql wd_mods_utf8mb4_ordered_all.sql 
: 1480606477:0;more preTableInfo.log
: 1480606506:0;more preTableInfo.sql 
: 1480606537:0;cp preTableInfo.sql preTableInfo_original.sql 
: 1480606539:0;vi preTableInfo.sql
: 1480606976:0;ls -l pre*
: 1480607025:0;mysql -u meyouhealth  -P 8000 -h 127.0.0.1 < preTableInfo.sql > wd-prod2-read1-preTableInfo.log  
: 1480610821:0;ssh -A -L 8000:127.0.0.1:8000 ubuntu@54.152.155.24 ssh -L 8000:walkadoo-production2-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1480610868:0;ssh -A -L 7999:127.0.0.1:7999 ubuntu@54.152.155.24 ssh -L 7999:wd-utf8mb4-test-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1480610883:0;mysql -u meyouhealth  -P 7900 -h 127.0.0.1 
: 1480612885:0;more wd-prod2-read1-preTableInfo.log
: 1480612890:0;vi wd-prod2-read1-preTableInfo.log
: 1480613087:0;more wd_leftovers.sql
: 1480613128:0;ls -l *ori*
: 1480613136:0;more preTableInfo_original.sql
: 1480613152:0;more wd_mods_utf8mb4_all.sql
: 1480613169:0;mv wd_mods_utf8mb4_all.sql wd_mods_utf8mb4_all_old.sql 
: 1480613174:0;vi wd_mods_utf8mb4_ordered_all.sql
: 1480615428:0;mysql -u meyouhealth  -P 7998 -h 127.0.0.1 < wd_mods_utf8mb4_ordered_all.sql > clean_end_to_end_wd_mods_utf8mb4_ordered_all.log 
: 1480615691:0;mysql -u meyouhealth  -P 7999 -h 127.0.0.1 
: 1480626032:0;more fresh_end_to_end_wd_mods_utf8mb4_all.log
: 1480626174:0;grep CHANGE wd_mods_utf8mb4_ordered_all.sql
: 1480626191:0;grep CHANGE wd_mods_utf8mb4_ordered_all.sql > changes.sql 
: 1480626524:0;vi changes.sql
: 1480626576:0;cat changes1.sql
: 1480626581:0;cat changes2.sql
: 1480626624:0;cat changes3.sql
: 1480643659:0;more clean_end_to_end_wd_mods_utf8mb4_ordered_all.log
: 1480643732:0;grep CONVERT wd_mods_utf8mb4_ordered_all.sql
: 1480643750:0;grep CONVERT wd_mods_utf8mb4_ordered_all.sql > convert.sql
: 1480643756:0;grep CONVERT wd_mods_utf8mb4_ordered_all.sql > convert1.sql
: 1480643761:0;grep CONVERT wd_mods_utf8mb4_ordered_all.sql > convert2.sql
: 1480643766:0;grep CONVERT wd_mods_utf8mb4_ordered_all.sql > convert3.sql
: 1480643771:0;grep CONVERT wd_mods_utf8mb4_ordered_all.sql > convert4.sql
: 1480643776:0;grep CONVERT wd_mods_utf8mb4_ordered_all.sql > convert5.sql
: 1480643780:0;grep CONVERT wd_mods_utf8mb4_ordered_all.sql > convert6.sql
: 1480643790:0;wc -l convert.sql
: 1480643809:0;wc -l changes.sql
: 1480643817:0;wc -l changes1.sql
: 1480643833:0;vi convert.sql
: 1480643934:0;tail convert.sql
: 1480643942:0;head convert1.sql
: 1480644056:0;head convert2.sql
: 1480644070:0;tail convert1.sql
: 1480644089:0;vi convert2.sql
: 1480644115:0;rm convert3.sql
: 1480644120:0;rm convert4.sql
: 1480644125:0;rm convert5.sql
: 1480644130:0;rm convert6.sql
: 1480644374:0;head wd_mods_utf8mb4_ordered_all.sql
: 1480644386:0;vi c*.sql 
: 1480644581:0;ls -ltrea
: 1480644592:0;ls -l logs
: 1480645006:0;mysql -u meyouhealth  -P 7998 -h 127.0.0.1 -vvv  < changes6.sql > wd_parallel_changes6.log
: 1480645054:0;mysql -u meyouhealth  -P 7998 -h 127.0.0.1  -vvv < convert.sql > wd_parallel_convert.log
: 1480645073:0;mysql -u meyouhealth  -P 7998 -h 127.0.0.1  -vvv < convert1.sql > wd_parallel_convert1.log
: 1480645084:0;mysql -u meyouhealth  -P 7998 -h 127.0.0.1  -vvv < convert2.sql > wd_parallel_convert2.log
: 1480645091:0;more wd_parallel_convert2.log
: 1480649398:0;mysql -u meyouhealth  -P 7998 -h 127.0.0.1  
: 1480650726:0;history 
: 1480650749:0;mysql -u meyouhealth  -P 7998 -vvv -h 127.0.0.1  < changes.sql > wd_parallel_changes.log
: 1480650752:0;mysql -u meyouhealth  -P 7998 -h 127.0.0.1  -vvv < changes1.sql > wd_parallel_changes1.log
: 1480650754:0;mysql -u meyouhealth  -P 7998 -h 127.0.0.1  -vvv < changes2.sql > wd_parallel_changes2.log
: 1480650758:0;mysql -u meyouhealth  -P 7998 -h 127.0.0.1  -vvv < changes3.sql > wd_parallel_changes3.log
: 1480650760:0;mysql -u meyouhealth  -P 7998 -h 127.0.0.1  -vvv < changes4.sql > wd_parallel_changes4.log
: 1480650763:0;mysql -u meyouhealth  -P 7998 -h 127.0.0.1  -vvv < changes5.sql > wd_parallel_changes5.log
: 1480650765:0;mysql -u meyouhealth  -P 7998 -h 127.0.0.1 -vvv  < changes6.sql > wd_parallel_changes6.log 
: 1480650954:0;mysql -u meyouhealth  -P 7998 -h 127.0.0.1
: 1480696268:0;cat wd_parallel_changes.log
: 1480696290:0;grep 2016 wd_parallel_changes*.log
: 1480697888:0;grep min --------------
: 1480697888:0;ALTER TABLE `walkadoo_production`.`sms_logs` CHANGE `phone_number` `phone_number` varchar(191) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci
: 1480697888:0;--------------
: 1480697888:0;Query OK, 7403024 rows affected (29 min 19.18 sec)
: 1480697888:0;Records: 7403024  Duplicates: 0  Warnings: 0
: 1480697906:0;grep min  wd_parallel_changes5.log
: 1480698400:0;vi wd_parallel_changes3.log
: 1480698875:0;grep Warnings wd_parallel_changes*.log
: 1480698900:0;grep Warnings wd_parallel_changes*.log | grep -v "Duplicates: 0  Warnings: 0"
: 1480698958:0;grep Duplicates wd_parallel_changes*.log | grep -v "Duplicates: 0  Warnings: 0"
: 1480698979:0;vi wd_parallel_changes.log
: 1480699042:0;head -220 wd_parallel_changes.log
: 1480699056:0;head -230 wd_parallel_changes.log
: 1480699119:0;vi wd_parallel_changes4.log
: 1480699139:0;head -60 wd_parallel_changes4.log
: 1480699239:0;head -365 wd_parallel_changes5.log
: 1480966608:0;ssh -A -L 7998:127.0.0.1:7998 ubuntu@54.152.155.24 ssh -L 7998:wd-utf8mb4-test-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1480966627:0;ssh -A -L 7997:127.0.0.1:7997 ubuntu@54.152.155.24 ssh -L 7997:wd-utf8mb4-test-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1480966802:0;more global_variables.log
: 1480966901:0;mv global_variables.log wd-global-vars-original-b4-testing-utf8mb4.log
: 1480966910:0;more list_s
: 1480967277:0;mysql -u meyouhealth  -P 7997 -vvv -h 127.0.0.1 
: 1480967386:0;mysql -u meyouhealth  -P 7996 -h 127.0.0.1
: 1480967798:0;ssh -A -L 7995:127.0.0.1:7995 ubuntu@54.152.155.24 ssh -L 7995:wd-utf8mb4-test-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1480968006:0;ssh -A -L 7996:127.0.0.1:7996 ubuntu@54.152.155.24 ssh -L 7996:walkadoo-production2-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1480968438:0;ssh -A -L 7994:127.0.0.1:7994 ubuntu@54.152.155.24 ssh -L 7994:walkadoo-production2-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1480968450:0;mysql -u meyouhealth  -P 7994 -h 127.0.0.1
: 1480968952:0;vi changes4.sql 
: 1480970013:0;mysql -u meyouhealth  -P 7995 -h 127.0.0.1
: 1481038350:0;mkdir utf8mb4_logs
: 1481038396:0;more wd_parallel_changes.log
: 1481038435:0;grep min wd_parallel_changes.log
: 1481038443:0;grep min wd_parallel_changes1.log
: 1481038449:0;grep min wd_parallel_changes2.log
: 1481038453:0;grep min wd_parallel_changes3.log
: 1481038458:0;grep min wd_parallel_changes4.log
: 1481038469:0;more wd_parallel_changes4.log
: 1481038591:0;cp changes4.sql changes4_likes.sql 
: 1481038596:0;vi changes4_likes.sql
: 1481038629:0;vi changes1.sql
: 1481038637:0;vi changes2.sql
: 1481038647:0;vi changes3.sql
: 1481038660:0;vi changes4.sql
: 1481038689:0;cp changes5.sql changes5_a.sql 
: 1481038759:0;vi changes5_a.sql 
: 1481038901:0;cp changes5_a.sql changes5_sms_logs.sql 
: 1481038927:0;cp changes5.sql changes5_sms_logs.sql 
: 1481038967:0;cp changes5_sms_logs.sql changes5_sms_logs_2.sql
: 1481038972:0;vi changes5_sms_logs_2.sql
: 1481038990:0;vi changes5_sms_logs.sql
: 1481039066:0;grep min wd_parallel_changes*.log | grep -v "Uptime"
: 1481039082:0;grep min wd_parallel_changes*.log | grep -v "Uptime" | grep -v "ALTER TABLE" 
: 1481039290:0;grep user_events changes*.sql
: 1481039339:0;grep Warnings *.log | grep -v "Duplicates: 0  Warnings: 0"
: 1481039384:0;grep team_challenge_cheers c*.sql 
: 1481039405:0;grep text c*.sql 
: 1481039422:0;vi changes5.sql
: 1481039470:0;ls -l changes5*
: 1481039482:0;cp changes5.sql changes5_b.sql
: 1481039544:0;vi changes5_b.sql
: 1481039641:0;cp changes5_a.sql changes5_c.sql 
: 1481039648:0;vi changes5_c.sql 
: 1481039771:0;vi changes5_d.sql 
: 1481039793:0;cat changes5_c.sql changes5_d.sql
: 1481039807:0;more wd_parallel_changes5.log
: 1481039860:0;cat changes5_d.sql > changes5_c.sql 
: 1481039887:0;more wd_parallel_changes5.log | grep "ALTER TABLE" 
: 1481039891:0;more wd_parallel_changes5.log | grep "ALTER TABLE"  | grep users
: 1481039897:0;more wd_parallel_changes5.log | grep "ALTER TABLE"  | grep users >> changes5_c.sql
: 1481040041:0;grep min wd_parallel_changes*.log
: 1481040144:0;vi changes5_a.sql
: 1481040208:0;vi changes5_c.sql
: 1481040304:0;grep user_cards changes*.sql 
: 1481040332:0;vi changes_3_user_cards.sql
: 1481040351:0;grep min wd_parallel_changes*.log | grep -v "Uptime" | grep -v "ALTER"
: 1481040366:0;vi wd_parallel_changes5.log
: 1481040407:0;grep user_events wd_parallel_changes5.log changes*.sql 
: 1481040440:0;cp changes_3_user_cards.sql changes_1_user_events.sql 
: 1481040446:0;vi changes_user_events.sql 
: 1481040455:0;vi changes_1_user_events.sql 
: 1481040512:0;more changes1.sql
: 1481040544:0;cp changes1.sql changes1_email_logs.sql
: 1481040549:0;cp changes1.sql changes1_email_logs_2.sql
: 1481040555:0;rm changes1.sql
: 1481040562:0;vi changes1_email_logs.sql
: 1481040571:0;vi changes1_email_logs2.sql
: 1481040580:0;vi changes1_email_logs_2.sql
: 1481040593:0;cat changes1_email_logs*.sql 
: 1481040632:0;vi changes1_email_logs*.sql 
: 1481040671:0;mv wd_parallel*.log utf8mb4_logs
: 1481040719:0;more changes5.sql
: 1481040729:0;grep sms_logs *.sql 
: 1481040736:0;grep sms_logs changes*.sql 
: 1481040764:0;vi changes5*.sql 
: 1481040960:0;rm changes5_d.sql
: 1481040982:0;cat *email_logs*.sql 
: 1481041003:0;grep "ALTER TABLE" changes*.sql | uniq -d
: 1481041046:0;grep "ALTER TABLE" changes*.sql  | wc -l 
: 1481041053:0;grep "ALTER TABLE" changes*.sql  | uniq -c
: 1481041071:0;grep "ALTER TABLE" changes*.sql  | grep -v "--" 
: 1481041079:0;grep "ALTER TABLE" changes*.sql  | grep -v "^--" 
: 1481041083:0;grep "ALTER TABLE" changes*.sql  | grep -v "^--" | wc -l 
: 1481041095:0;grep "ALTER TABLE" changes*.sql  | grep -v "--" | wc -l 
: 1481041100:0;grep "ALTER TABLE" changes*.sql  | grep -v "\--" | wc -l 
: 1481041108:0;grep "ALTER TABLE" utf8mb4_logs/wd_parallel_changes*.log  | wc -l 
: 1481041137:0;grep "ALTER TABLE" utf8mb4_logs/wd_parallel_changes*.log  
: 1481041163:0;grep "ALTER TABLE" utf8mb4_logs/wd_parallel_changes*.log  | awk -F ":" '{print $2}'
: 1481041168:0;grep "ALTER TABLE" utf8mb4_logs/wd_parallel_changes*.log  | awk -F ":" '{print $2}' > list
: 1481041188:0;grep "ALTER TABLE" changes*.sql 
: 1481041223:0;grep "ALTER TABLE" changes*.sql| grep -v "\--" | awk -F ":" '{print $2}' > list2
: 1481041226:0;more list2
: 1481041234:0;wc -l list list2
: 1481041247:0;cat list2 
: 1481041255:0;cat list2 | sort 
: 1481041261:0;cat list2 | sort > list2_sorted
: 1481041269:0;diff -u list list2_sorted
: 1481041286:0;cat list1 | sort > list1_sorted
: 1481041297:0;cat list | sort > list1_sorted
: 1481041303:0;diff -u list1_sorted ls
: 1481041333:0;vi list*_sorted
: 1481041357:0;diff -u list1_sorted list2_sorted
: 1481041368:0;vi list1_sorted
: 1481041415:0;paste list1_sorted list2_sorted | more
: 1481041468:0;more list1_sorted
: 1481041511:0;cat list1_sorted | awk -F "." '{print $2}'
: 1481041526:0;cat list1_sorted | awk -F "." '{print $2}' | awk '{print $1}'
: 1481041548:0;cat list1_sorted | awk -F "." '{print $2}' | awk '{print $1 " " print $2}'
: 1481041558:0;cat list1_sorted | awk -F "." '{print $2}' | awk '{print $1 ' ' print $2}'
: 1481041566:0;cat list1_sorted | awk -F "." '{print $2}' | awk '{print $1, ' ', print $2}'
: 1481041572:0;cat list1_sorted | awk -F "." '{print $2}' | awk '{print $1  print $2}'
: 1481041584:0;cat list1_sorted | awk -F "." '{print $2}' | awk '{print $1 " " $2}'
: 1481041589:0;cat list1_sorted | awk -F "." '{print $2}' | awk '{print $1 " " $3}'
: 1481041601:0;cat list1_sorted | awk -F "." '{print $2}' | awk '{print $1 " " $3}' > list1
: 1481041612:0;cat list2_sorted | awk -F "." '{print $2}' | awk '{print $1 " " $3}' > list2
: 1481041706:0;grep serial_number changes*.sql 
: 1481041742:0;cp list1 list1_with_devices_serial_number
: 1481041745:0;vi list1
: 1481041756:0;paste list1 list2
: 1481041763:0;wc -l list1 list2
: 1481041783:0;mv list* utf8mb4_logs
: 1481041818:0;ls -l *convert*
: 1481041832:0;grep 2016 wd_parallel_convert*.log
: 1481041855:0;more Step1-script-for-WD-maint-shutdown.sh
: 1481041869:0;grep mysql *.sh
: 1481041897:0;cp wd-deploy-action-tracking.sh wd-utf8mb4-char-conversion.sh 
: 1481042019:0;ls -l changes*sql
: 1481042034:0;ls -l changes*sql | awk '{print $9}'
: 1481042049:0;ls -l changes*sql | awk '{print $9}' >> wd-utf8mb4-char-conversion.sh
: 1481042659:0;more changes_1_user_events.sql
: 1481042691:0;cat changes1_email_logs.sql changes1_email_logs_2.sql
: 1481042717:0;more list1 | grep email_logs
: 1481042728:0;more utf8mb4_logs/list1 | grep email_logs
: 1481042762:0;grep email_logs wd_parallel_changes*.log
: 1481042771:0;vi wd_parallel_changes1.log
: 1481042901:0;grep email_logs changes*.sql 
: 1481042911:0;grep likes changes*.sql 
: 1481042980:0;mv changes5_a.sql changes5_a_user_cards_other.sql 
: 1481043096:0;mv changes5_c.sql changes3_users_other.sql 
: 1481043149:0;grep user changes*.sql 
: 1481043170:0;grep net_promoter_scores changes*.sql 
: 1481043210:0;mv changes3_users_other.sql changes_3_users_other.sql
: 1481043218:0;mv changes5_sms_logs_2.sql changes_5_sms_logs_2.sql
: 1481043230:0;mv changes5_sms_logs.sql changes_5_sms_logs.sql
: 1481043240:0;mv changes5_a_user_cards_other.sql changes_5_a_user_cards_other.sql
: 1481043282:0;mv changes4_likes.sql changes_4_likes.sql
: 1481043296:0;mv changes1_email_logs.sql changes_1_email_logs.sql
: 1481043306:0;mv changes1_email_logs_2.sql changes_1_email_logs_2.sql
: 1481043323:0;mv changes5_b.sql changes_5_b.sql
: 1481043330:0;ls -ltra changes*.sql 
: 1481043350:0;ls -ltra changes*.sql | awk '{ print $9}'
: 1481043379:0;ls -ltra changes*.sql | awk '{ print $9}' >> wd-utf8mb4-char-conversion.sh
: 1481043449:0;more changes3.sql
: 1481043459:0;grep experiences changes*.sql 
: 1481043478:0;more changes4.sql
: 1481043484:0;more changes4*.sql
: 1481043506:0;more changes4.sql changes_4_likes.sql
: 1481043534:0;more changes6.sql
: 1481043540:0;vi changes6.sql
: 1481043573:0;grep " users " changes*.sql
: 1481043583:0;grep " `users` " changes*.sql
: 1481043589:0;grep "`users` " changes*.sql
: 1481043592:0;grep "`users` " change*.sql
: 1481043609:0;grep ".\`users` " change*.sql
: 1481043613:0;grep ".\`users " change*.sql
: 1481043617:0;grep ".\`users" change*.sql
: 1481043655:0;grep ".\`users" wd_parallel_changes*.log
: 1481043666:0;vi wd_parallel_changes6.log
: 1481049729:0;cp wd-utf8mb4-char-conversion.sh wd-utf8mb4-char-conversion2.sh
: 1481049827:0;ssh -A -L 9981:127.0.0.1:9981 ubuntu@54.152.155.24 ssh -L 9981:walkadoo-production2-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1481049847:0;mysql -u meyouhealth  -P 9981 -h 127.0.0.1
: 1481049939:0;ssh -A -L 9980:127.0.0.1:9980 ubuntu@54.152.155.24 ssh -L 9980:wd-utf8mb4-test-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1481049948:0;mysql -u meyouhealth  -P 9980 -h 127.0.0.1
: 1481050166:0;grep -i database change*.sql
: 1481050174:0;grep -i utf8 change*.sql
: 1481050400:0;sh -xv wd-utf8mb4-char-conversion.sh
: 1481050430:0;cp wd-utf8mb4-char-conversion.sh wd-utf8mb4-char-conversion1.sh
: 1481050481:0;vi wd-utf8mb4-char-conversion1a.sh
: 1481050505:0;cat wd-utf8mb4-char-conversion*.sh
: 1481050554:0;cat wd-utf8mb4-char-conversion1.sh
: 1481050585:0;vi wd-utf8mb4-char-conversion1b.sh
: 1481050604:0;cat wd-utf8mb4-char-conversion.sh
: 1481050623:0;cat wd-utf8mb4-char-conversion1a.sh
: 1481050629:0;cat wd-utf8mb4-char-conversion1b.sh
: 1481050647:0;cp wd-utf8mb4-char-conversion.sh wd-utf8mb4-char-conversion1a.sh
: 1481050650:0;cp wd-utf8mb4-char-conversion.sh wd-utf8mb4-char-conversion1b.sh
: 1481050653:0;cp wd-utf8mb4-char-conversion.sh wd-utf8mb4-char-conversion1c.sh
: 1481050656:0;cp wd-utf8mb4-char-conversion.sh wd-utf8mb4-char-conversion1d.sh
: 1481050659:0;cp wd-utf8mb4-char-conversion.sh wd-utf8mb4-char-conversion1e.sh
: 1481050662:0;cp wd-utf8mb4-char-conversion.sh wd-utf8mb4-char-conversion1f.sh
: 1481050834:0;cp wd-utf8mb4-char-conversion.sh wd-utf8mb4-char-conversion1g.sh
: 1481050837:0;cp wd-utf8mb4-char-conversion.sh wd-utf8mb4-char-conversion1h.sh
: 1481050841:0;cp wd-utf8mb4-char-conversion.sh wd-utf8mb4-char-conversion1i.sh
: 1481050846:0;vi wd-utf8mb4-char-conversion1*.sh
: 1481050936:0;cp wd-utf8mb4-char-conversion.sh wd-utf8mb4-char-conversion1j.sh
: 1481050951:0;cat wd-utf8mb4-char-conversion1i.sh
: 1481050960:0;vi wd-utf8mb4-char-conversion1j.sh
: 1481051014:0;sh -xv wd-utf8mb4-char-conversion1a.sh
: 1481051027:0;sh -xv wd-utf8mb4-char-conversion1b.sh
: 1481051043:0;sh -xv wd-utf8mb4-char-conversion1c.sh
: 1481051079:0;sh -xv wd-utf8mb4-char-conversion1d.sh
: 1481051097:0;sh -xv wd-utf8mb4-char-conversion1e.sh
: 1481051165:0;sh -xv wd-utf8mb4-char-conversion1f.sh
: 1481051186:0;sh -xv wd-utf8mb4-char-conversion1g.sh
: 1481051217:0;sh -xv wd-utf8mb4-char-conversion1h.sh
: 1481051242:0;sh -xv wd-utf8mb4-char-conversion1i.sh
: 1481051266:0;sh -xv wd-utf8mb4-char-conversion1j.sh
: 1481051306:0;more wd-utf8mb4-char-conversion.sh-20161206-1481050400.log
: 1481052405:0;sh -xv wd-utf8mb4-char-conversion2.sh
: 1481052470:0;more wd-utf8mb4-char-conversion2.sh
: 1481052483:0;cp wd-utf8mb4-char-conversion2.sh wd-utf8mb4-char-conversion2a.sh 
: 1481052487:0;cp wd-utf8mb4-char-conversion2.sh wd-utf8mb4-char-conversion2b.sh 
: 1481052490:0;cp wd-utf8mb4-char-conversion2.sh wd-utf8mb4-char-conversion2c.sh 
: 1481052492:0;cp wd-utf8mb4-char-conversion2.sh wd-utf8mb4-char-conversion2d.sh 
: 1481052500:0;vi wd-utf8mb4-char-conversion2*.sh
: 1481052550:0;rm wd-utf8mb4-char-conversion2d.sh
: 1481052559:0;cat wd-utf8mb4-char-conversion2*.sh
: 1481052662:0;sh -xv wd-utf8mb4-char-conversion2a.sh 
: 1481052709:0;sh -xv wd-utf8mb4-char-conversion2b.sh
: 1481052738:0;sh -xv wd-utf8mb4-char-conversion2c.sh
: 1481052793:0;more wd-utf8mb4-char-conversion1a.sh
: 1481052828:0;tail wd-utf8mb4-char-conversion1i.sh-20161206-1481051242.log
: 1481052866:0;grep Bye wd-utf8mb4-char-conversion*.log > quickest_run
: 1481056387:0;cat wd-utf8mb4-char-conversion2c.sh
: 1481056409:0;grep Bye wd-utf8mb4-char-conversion*.log 
: 1481056436:0;vi quickest_run
: 1481056449:0;grep Bye wd-utf8mb4-char-conversion*.log >> quickest_run
: 1481056896:0;bg 
: 1481056954:0;ls -ltra | tail -100
: 1481056977:0;ls -ltra | tail -100 > list_as_returning_starting_2PM_list.txt
: 1481056998:0;date >> list_as_returning_starting_2PM_list.txt
: 1481057019:0;grep Bye wd-utf8mb4-char-conversion1d.sh-20161206-1481051079.log
: 1481057101:0;grep varchar changes*.sql 
: 1481057124:0;grep -v "varchar(191)" changes*.sql 
: 1481057199:0;more wd-utf8mb4-char-conversion1c.sh-20161206-1481051043.log
: 1481057300:0;more wd-utf8mb4-char-conversion1b.sh
: 1481057310:0;more changes2.sql
: 1481126868:0;ssh -A -L 9979:127.0.0.1:9979 ubuntu@54.152.155.24 ssh -L 9979:wd-utf8mb4-test-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1481126882:0;mysql -u meyouhealth  -P 9979 -h 127.0.0.1
: 1481127377:0;grep versions convert*
: 1481127548:0;grep versions change*.sql
: 1481127999:0;more quickest_run
: 1481128034:0;ls -ltra wd-utf8mb4-char-conversion1*
: 1481128066:0;more list_as_returning_starting_2PM_list.txt
: 1481128092:0;vi wd-utf8mb4-char-conversion*.sh
: 1481128268:0;vi wd-utf8mb4-char-conversion2.sh
: 1481128299:0;mv wd-utf8mb4-char-conversion*.log utf8mb4_logs
: 1481128350:0;sh -xv wd-utf8mb4-char-conversion.sh 9979 
: 1481128378:0;more wd-utf8mb4-char-conversion.sh
: 1481128410:0;vi wd-utf8mb4-char-conversion.sh
: 1481128445:0;vi wd-utf8mb4-char-conversion1.sh
: 1481128462:0;more wd-utf8mb4-char-conversion1.sh
: 1481128491:0;sh -xv wd-utf8mb4-char-conversion.sh 9979
: 1481128503:0;more wd-utf8mb4-char-conversion.sh-20161207-1481128491.log
: 1481128518:0;sh -xv wd-utf8mb4-char-conversion.sh 9979 & 
: 1481128535:0;sh -x wd-utf8mb4-char-conversion.sh 9979 & 
: 1481128547:0;sh  wd-utf8mb4-char-conversion.sh 9979 & 
: 1481128597:0;cat wd-utf8mb4-char-conversion1.sh wd-utf8mb4-char-conversion.sh
: 1481128621:0;mv wd-utf8mb4-char-conversion.sh wd-utf8mb4-char-conversion1o.sh 
: 1481128645:0;rm wd-utf8mb4-char-conversion.sh-20161207-14*.log
: 1481128660:0;ls -l wd-utf8mb4-char-conversion1*
: 1481128670:0;ls -l wd-utf8mb4-char-conversion1* | awk '{print $9}'
: 1481128975:0;for i in `ls -l wd-utf8mb4-char-conversion1* | awk '{print $9}'`\
do \
sh ${i} 9979 & \
done 
: 1481128996:0;more wd-utf8mb4-char-conversion1.sh-20161207-1481128975.log
: 1481129006:0;more wd-utf8mb4-char-conversion1g.sh-20161207-1481128975.log
: 1481129111:0;rm wd-utf8mb4-char-conversion*log
: 1481130028:0;more wd-utf8mb4-char-conversion1o.sh
: 1481130033:0;more changes.sq
: 1481130066:0;mv wd-utf8mb4-char-conversion1.sh wd-utf8mb4-char-conversion1_a.sh
: 1481130077:0;mv wd-utf8mb4-char-conversion1o.sh wd-utf8mb4-char-conversion1.sh
: 1481130114:0;more wd-utf8mb4-char-conversion1_a.sh
: 1481130726:0;more walkadoo-production-june-03-2016-2000-hr-search-all-nodes.txt
: 1481130730:0;rm walkadoo-production-june-03-2016-2000-hr-search-all-nodes.txt
: 1481130737:0;more hmmm
: 1481130741:0;rm hmm
: 1481130744:0;rm hmmm
: 1481130778:0;ls -l *.txt
: 1481130796:0;ls -l chef
: 1481130806:0;mkdir chef
: 1481130815:0;mv knife*.txt chef/
: 1481130823:0;rm ip_list*
: 1481130841:0;rm aws-elasticache*.txt
: 1481130853:0;rm aws-elasticache-desc-cache-clusters-show-node-info.0
: 1481130855:0;rm aws-elasticache-desc-cache-clusters-show-node-info.0*
: 1481130863:0;rm early_http_409s_06032016-2009.txt
: 1481130888:0;mv chef-* chef/
: 1481130895:0;mv learn_chef_io.log chef/
: 1481130914:0;rm postgresql.log.2015-12-10-18
: 1481130935:0;rm dc_investigation_notes_121415.rtf
: 1481130949:0;more history*
: 1481130969:0;more list-patching-done
: 1481130974:0;rm list-patching-done
: 1481131732:0;ssh -A -L 9978:127.0.0.1:9978 ubuntu@54.152.155.24 ssh -L 9978:wd-utf8mb4-test-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1481131745:0;mysql -u meyouhealth  -P 9978 -h 127.0.0.1
: 1481131820:0;for i in `ls -l wd-utf8mb4-char-conversion1* | awk '{print $9}'`\
do \
sh ${i} 9978 & \
done 
: 1481138612:0;grep Bye wd-utf8mb4-char-conversion*.log
: 1481138636:0;ssh -A -L 9977:127.0.0.1:9977 ubuntu@54.152.155.24 ssh -L 9977:wd-utf8mb4-test-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1481138649:0;mysql -u meyouhealth  -P 9977 -h 127.0.0.1
: 1481138747:0;jobs~/Documents/walkadoo â€¹2.1.5â€º  $ Wed Dec  7 12:32:47 EST 2016
: 1481138747:0;[6]    36989 done       sh ${i} 9978
: 1481138747:0;~/Documents/walkadoo â€¹2.1.5â€º  $ Wed Dec  7 12:43:19 EST 2016
: 1481138747:0;[12]  - 37005 done       sh ${i} 9978
: 1481138747:0;~/Documents/walkadoo â€¹2.1.5â€º  $ ERRORERROR 2013 (HY000) 2013 (HY000) at line 5 at line 9ERROR: Lost connection to MySQL server during query
: 1481138748:0;ERROR 2013 (HY000) at line 15: Lost connection to MySQL server during query
: 1481138748:0;ERROR 2013 (HY000) at line 26: Lost connection to MySQL server during query
: 1481138748:0;ERROR 2013 (HY000) at line 16: Lost connection to MySQL server during query
: 1481138748:0;ERROR 2013 (HY000) at line 5: Lost connection to MySQL server during query
: 1481138748:0;ERRORERROR 2013 (HY000) 2013 (HY000) at line 12 at line 5: Lost connection to MySQL server during query
: 1481138748:0;: Lost connection to MySQL server during query
: 1481138748:0;Wed Dec  7 14:22:35 EST 201Wed Dec  7 14:22:35 EST 201Wed Dec  7 14:22:35 EST 201Wed Dec  1]  - 37004 done       sh ${i} 9978
: 1481138748:0;Wed Dec  7 14:22:35 EST 2016
: 1481138880:0;more convert*
: 1481138924:0;mysql -u meyouhealth  -P 9977 -h 127.0.0.1 < convert.sql > convert.log
: 1481139050:0;grep -i Warnings wd-utf8mb4-char-conversion*.log 
: 1481139063:0;grep -i Warnings wd-utf8mb4-char-conversion*.log | grep -v " Warnings: 0"
: 1481139105:0;grep sms_logs change*
: 1481139134:0;grep changes_5_sms_logs *.sh
: 1481139543:0;grep admin_users change*.sql 
: 1481139582:0;grep changes.sql *.sh 
: 1481139594:0;more wd-utf8mb4-char-conversion1.sh*.log
: 1481139610:0;more changes.sql 
: 1481139691:0;mysql -u meyouhealth  -P 9977 -h 127.0.0.1 -vvv < changes.sql > changes.log 
: 1481139721:0;more wd-utf8mb4-char-conversion1c.sh-20161207-1481131820.log
: 1481139752:0;more wd-utf8mb4-char-conversion1d.sh
: 1481139757:0;more changes_3_user_cards.sql
: 1481139771:0;more wd-utf8mb4-char-conversion1e.sh-20161207-1481131820.log
: 1481139802:0;grep last_page_visited changes_3_users_other.sql
: 1481139805:0;cat changes_3_users_other.sql
: 1481139858:0;vi changes.sql 
: 1481139896:0;more changes.log
: 1481139914:0;more convert.sql 
: 1481140042:0;convert.logjobs
: 1481149221:0;more wd-utf8mb4-char-conversion2.sh-20161207-1481138688.log
: 1481149239:0;more wd-utf8mb4-char-conversion2c.sh-20161207-1481138688.log
: 1481149343:0;history | grep convert.sql
: 1481160101:0;grep source_object_type changes*.sql
: 1481160154:0;grep changes2.sql wd-utf8mb4-char-conversion2*
: 1481160158:0;grep changes2.sql wd-utf8mb4-char-conversion*
: 1481160185:0;mysql -u meyouhealth  -P 9977 -h 127.0.0.1 -vvv 
: 1481161446:0;more convert.log
: 1481161612:0;grep order_number changes*.sql
: 1481161938:0;grep tracking_number changes*.sql 
: 1481161981:0;grep payment_confirmations changes*.sql 
: 1481162049:0;grep phone_numbers changes*.sql 
: 1481162107:0;vi convert1.sql
: 1481162125:0;grep push_logs changes*.sql 
: 1481162154:0;mysql -u meyouhealth  -P 9977 -h 127.0.0.1 -vvv < changes5.sql > changes5.log
: 1481162317:0;mysql -u meyouhealth  -P 9977 -h 127.0.0.1 -vvv < convert1.sql > convert1.log
: 1481162854:0;mysql -u meyouhealth  -P 9977 -h 127.0.0.1 -vvv < convert2.sql > convert2.log
: 1481162947:0;ls -l conver*
: 1481162968:0;mv *.log utf8mb4_logs
: 1481162996:0;for i in `ls -l wd-utf8mb4-char-conversion1* | awk '{print $9}'`\
do \
sh ${i} 9977 & \
done 
: 1481163150:0;for i in `ls -l wd-utf8mb4-char-conversion2* | awk '{print $9}'`\
do \
sh ${i} 9977 & \
done 
: 1481164989:0;ssh -A -L 9976:127.0.0.1:9976 ubuntu@54.152.155.24 ssh -L 9976:wd-utf8mb4-test-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1481165033:0;grep "ALTER DATABASE" changes*.sql 
: 1481165042:0;head -10 changes.sql
: 1481165081:0;for i in `ls -l wd-utf8mb4-char-conversion1* | awk '{print $9}'`\
do \
sh ${i} 9976 & \
done 
: 1481165106:0;vi wd-utf8mb4-char-conversion1g.sh
: 1481165139:0;more wd-utf8mb4-char-conversion1j.sh-20161207-1481162996.log
: 1481165186:0;grep 2016 wd-utf8mb4-char-conversion1g.sh-20161207*.log
: 1481165206:0;grep 2016 wd-utf8mb4-char-conversion*.log
: 1481165258:0;more wd-utf8mb4-char-conversion1i.sh-20161207-1481162996.log
: 1481165281:0;grep "TCP port" wd-utf8mb4-char-conversion*.log
: 1481165311:0;mv *sh-20161207-148116*.log utf8mb4_logs
: 1481165331:0;mv *sh-20161207-148116*.log ../
: 1481165375:0;ls -ltra *sh-20161207-1481165*.log utf8mb4_logs
: 1481166642:0;for i in `ls -l wd-utf8mb4-char-conversion2* | awk '{print $9}'`\
do \
sh ${i} 9976 & \
done 
: 1481166659:0;more wd-utf8mb4-char-conversion2b.sh-20161207-1481163150.log
: 1481166728:0;ps -ef | grep sh
: 1481167374:0;mysql -u meyouhealth  -P 9977 -h 127.0.0.1 -vvv < convert.sql > convert.log
: 1481211609:0;ls -l convert*
: 1481211633:0;mysql -u meyouhealth  -P 9976 -h 127.0.0.1 -vvv < convert1.sql > convert1.log
: 1481211952:0;more convert1.log
: 1481211963:0;mysql -u meyouhealth  -P 9976 -h 127.0.0.1 -vvv < convert2.sql > convert2.log
: 1481211972:0;more convert2.log
: 1481212026:0;grep admin_users convert*
: 1481212094:0;mysql -u meyouhealth  -P 9976 -h 127.0.0.1 -vvv < convert.sql > convert.log
: 1481212148:0;mysql -u meyouhealth  -P 9976 -h 127.0.0.1
: 1481213945:0;find . -name "*.sh" -exec grep ' -schema ' {}\;
: 1481213950:0;find . -name "*.sh" -exec grep ' -schema ' {} \;
: 1481213980:0;ls -l *.sh
: 1481214000:0;cp mysqldump_schema.sh ../walkadoo/mysqldump_schema.sh
: 1481215750:0;mv mysqldump-schema-\{EXPORT_NAME\}-20161208.mysql mysqldump-schema-wd-20161208.mysql
: 1481215758:0;cp mysqldump-schema-wd-20161208.mysql mysqldump-schema-wd-20161208.mysql.orig
: 1481216270:0;cp mysqldump-schema-wd-20161208.mysql.orig mysqldump-schema-wd-20161208.mysql.test
: 1481216275:0;vi mysqldump-schema-wd-20161208.mysql.test
: 1481216375:0;diff mysqldump-schema-wd-20161208.mysql mysqldump-schema-wd-20161208.mysql.test
: 1481216395:0;rm mysqldump-schema-wd-20161208.mysql.test
: 1481216427:0;vi mysqldump-schema-wd-20161208.mysql
: 1481216456:0;mysql -u meyouhealth  -P 9976 -h 127.0.0.1 -vvv < mysqldump-schema-wd-20161208.mysql > mysqldump-schema-wd-20161208.new-table-test.log
: 1481216490:0;more mysqldump-schema-wd-20161208.new-table-test.log
: 1481225229:0;vi reset_auto_increment_wd.sql 
: 1481225302:0;cat reset_auto_increment_wd.sql| sed s/\|//g
: 1481225322:0;cat reset_auto_increment_wd.sql| sed s/\|//g > wd_reset_auto_increment_for_import.sql 
: 1481225328:0;rm reset_auto_increment_wd.sql
: 1481225498:0;vi wd_reset_auto_increment_for_import.sql
: 1481225541:0;cat wd_reset_auto_increment_for_import.sql | sed s/\| //g
: 1481225548:0;cat wd_reset_auto_increment_for_import.sql | sed s/\|//g
: 1481225566:0;cat wd_reset_auto_increment_for_import.sql | sed s/\|//g > wd_reset_new_cp_auto_increment_for_import.sql
: 1481225571:0;rm wd_reset_auto_increment_for_import.sql
: 1481225609:0;vi wd_reset_new_cp_auto_increment_for_import.sql
: 1481226259:0;vi inserts_old_into_new.sql
: 1481226362:0;cat inserts_old_into_new.sql | sed s/\|//g
: 1481226380:0;cat inserts_old_into_new.sql | sed s/\|//g > wd_inserts_from_old_into_new.sql 
: 1481226385:0;rm inserts_old_into_new.sql
: 1481226475:0;split -l 5 wd_inserts_from_old_into_new.sql 
: 1481226484:0;more xat
: 1481226491:0;more xas
: 1481226503:0;rm xat
: 1481226526:0;rm xaa
: 1481226692:0;mysql -u meyouhealth  -P 9976 -h 127.0.0.1 -vvv -D walkadoo_production -A 
: 1481226699:0;for i in `ls -l xa* | awk '{print $9}'`\
do \
 mysql -u meyouhealth  -P 9976 -h 127.0.0.1 -D walkadoo_production -A -vvv < ${i} > wd-${i}-insert-into-new.log  \
done 
: 1481226784:0;cp wd-utf8mb4-char-conversion1.sh wd-utf8mb4-insert-into-new.sh
: 1481227075:0;ps -ef | grep mysql 
: 1481227084:0;kill 74839
: 1481227244:0;vi truncate_new_tables_for_testing.log
: 1481227261:0;cat truncate_new_tables_for_testing.log| sed s/\|//g 
: 1481227273:0;cat truncate_new_tables_for_testing.log| sed s/\|//g > wd-truncate-tbl-new-testing.sql 
: 1481227407:0;vi wd-truncate-tbl-new-testing.
: 1481227411:0;vi wd-truncate-tbl-new-testing.sql
: 1481227454:0;mysql -u meyouhealth  -P 9976 -h 127.0.0.1 -vvv < wd_reset_new_cp_auto_increment_for_import.sql > wd_reset_new_cp_auto_increment_for_import.log
: 1481227489:0;for i in `ls -l xa* | awk '{print $9}'`\
do \
 mysql -u meyouhealth  -P 9976 -h 127.0.0.1 -D walkadoo_production -A -vvv < ${i} > wd-${i}-insert-into-new.log  2>&1 & \
done 
: 1481227562:0;more wd-xap-insert-into-new.log
: 1481227615:0;kill %2
: 1481227619:0;kill %3
: 1481227623:0;kill %4
: 1481227626:0;kill %5
: 1481227628:0;kill %6
: 1481227631:0;kill %7
: 1481227636:0;kill %8
: 1481227644:0;kill %10
: 1481227648:0;kill %11
: 1481227652:0;kill %12
: 1481227654:0;kill %13
: 1481227657:0;kill %17
: 1481227661:0;kill %18
: 1481227769:0;vi wd-utf8mb4-insert-into-new.sh
: 1481227793:0;rm -rf web-log
: 1481227818:0;mysql -u meyouhealth  -P 9976 -h 127.0.0.1 -vvv 
: 1481229503:0;mysql -u meyouhealth  -P 9976 -h 127.0.0.1 -vvv < wd-truncate-tbl-new-testing.sql > wd-truncate-tbl-new-testing.log
: 1481229511:0;mysql -u meyouhealth  -P 9976 -h 127.0.0.1 -vvv -D walkadoo_production
: 1481229570:0;mysql -u meyouhealth  -P 9976 -h 127.0.0.1 -vvv -D walkadoo_production -A < wd_reset_new_cp_auto_increment_for_import.sql > wd_reset_new_cp_auto_increment_for_import.log
: 1481229592:0;more wd-truncate-tbl-new-testing.log
: 1481229690:0;for i in `ls -l xa* | grep -v '.log' | awk '{print $9}'`\
do \
 mysql -u meyouhealth  -P 9976 -h 127.0.0.1 -D walkadoo_production -A -vvv < ${i} > utf8mb4_logs/wd-${i}-insert-into-new.log  2>&1 & \
done 
: 1481229762:0;mysql -u meyouhealth  -P 9976 -h 127.0.0.1 -vvv -D walkadoo_production -A
: 1481229849:0;vi utf8mb4_logs/start_insert_time_est
: 1481229874:0;ls -l utf8mb4_logs/wd-x*insert* >> utf8mb4_logs/start_insert_time_est
: 1481229997:0;ls -l utf8mb4_logs/wd-x*insert* | wc -l 
: 1481231602:0;more walkadoo-prod-read1-full-dump.061716-907.sql
: 1481231698:0;cat mysqldump_schema.sh
: 1481231742:0;more mysqldump-schema-wd-20161208.mysql.orig
: 1481231774:0;more mysqldump-schema-wd-20161208.mysql.orig | sed s/DROP TABLE IF EXISTS/ -- -- /g
: 1481231783:0;more mysqldump-schema-wd-20161208.mysql.orig | sed s/^DROP TABLE IF EXISTS/ -- -- /g
: 1481231797:0;cat mysqldump-schema-wd-20161208.mysql.orig | sed s/^DROP TABLE IF EXISTS/-- -- /g
: 1481231804:0;cat mysqldump-schema-wd-20161208.mysql.orig | sed s/DROP TABLE IF EXISTS/-- -- /g
: 1481231813:0;cat mysqldump-schema-wd-20161208.mysql.orig | sed s/DROP/-- -- /g
: 1481231856:0;cat mysqldump-schema-wd-20161208.mysql.orig | sed s/DROP/-- /g
: 1481231878:0;cat mysqldump-schema-wd-20161208.mysql.orig | sed s/` (/-- /g
: 1481231893:0;cat mysqldump-schema-wd-20161208.mysql.orig | sed s/\` (/-- /g
: 1481231900:0;cat mysqldump-schema-wd-20161208.mysql.orig 
: 1481231920:0;cat mysqldump-schema-wd-20161208.mysql.orig | sed s/Dump completed/ HI THERE/g
: 1481231945:0;cat mysqldump-schema-wd-20161208.mysql.orig | sed s/  //g
: 1481231953:0;cat mysqldump-schema-wd-20161208.mysql.orig | sed -s s/  //g
: 1481231969:0;cat mysqldump-schema-wd-20161208.mysql.orig | sed s/Dump completed/ HI THERE//g
: 1481231975:0;history | grep sed
: 1481232000:0;cat mysqldump-schema-wd-20161208.mysql.orig | sed s/\|//g
: 1481232013:0;cat mysqldump-schema-wd-20161208.mysql.orig | sed s/\DROP//g
: 1481232020:0;cat mysqldump-schema-wd-20161208.mysql.orig | sed s/\DROP/-- -- /g
: 1481232025:0;cat mysqldump-schema-wd-20161208.mysql.orig | sed s/\DROP/-- /g
: 1481232044:0;cat mysqldump-schema-wd-20161208.mysql.orig | sed s/\DROP TABLE/\--/g
: 1481232049:0;cat mysqldump-schema-wd-20161208.mysql.orig | sed s/\DROP/\--/g
: 1481232062:0;cat mysqldump-schema-wd-20161208.mysql.orig | sed s/\DROP/\--/g > new
: 1481232104:0;cat new | sed s/\` ($/_new\` (/g
: 1481232119:0;cat new | sed s/\` ($/ HI/g
: 1481232127:0;cat new | sed s/\\` ($/ HI/g
: 1481232136:0;cat new | sed s/` ($/ HI/g\
'\
`
: 1481232152:0;vi new
: 1481232729:0;ls -l utf8mb4_logs/wd-x*insert*
: 1481296820:0;mkdir utf8_conv_1
: 1481296840:0;mv wd-utf8mb4-char-conversion* utf8_conv_1
: 1481296856:0;mv changes*.sql utf8_conv_1
: 1481296875:0;mv convert*.sql utf8_conv_1
: 1481296886:0;mv quickest_run utf8mb4_logs
: 1481296905:0;mv wd_mods_utf8mb4_all_old.sql utf8_conv_1
: 1481296955:0;more new
: 1481296960:0;rm new
: 1481296984:0;mv wd_alter_table_char_set_collate_utf8mb4.sql utf8_conv_1
: 1481297000:0;mv list_as_returning_starting_2PM_list.txt utf8mb4_logs
: 1481297031:0;mv wd_mods_utf8mb4_ordered_all.sql utf8_conv_1
: 1481297048:0;mv wd_leftovers* utf8_conv_1
: 1481297073:0;mv wd_alter_*utf8mb4*.sql utf8_conv_1
: 1481297105:0;rm dclog_pg_grants.sql
: 1481297113:0;more snippet_timings.sql
: 1481297156:0;mv snippet_timings.sql dc_optimize_tbl_no_binlog.sql 
: 1481297161:0;mv dc_optimize_tbl_no_binlog.sql ../dc/
: 1481297167:0;more test.sql
: 1481297178:0;rm test.sql
: 1481316898:0;ssh -A -L 9974:127.0.0.1:9974 ubuntu@54.152.155.24 ssh -L 9974:wd-utf8mb4-test-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1481316918:0;vi mysqldump_schema.sh
: 1481316965:0;sh -xv mysqldump_schema.sh
: 1481316974:0;sh -xv mysqldump_schema.sh 9974
: 1481317051:0;cp mysqldump-schema-wd-20161209.mysql mysqldump-schema-wd-20161209.mysql.orig
: 1481317309:0;diff -u mysqldump-schema-wd-20161209.mysql mysqldump-schema-wd-20161209.mysql.orig
: 1481317378:0;grep "CREATE DATABASE" mysqldump-schema-wd-20161209.mysql*
: 1481317405:0;grep DATABASE utf8mb4_logs/changes.log
: 1481317418:0;grep DATABASE utf8_conv_1/changes.sql
: 1481317573:0;vi mysqldump-schema-wd-20161209.mysql
: 1481317967:0;mysql -u meyouhealth  -P 9974 -h 127.0.0.1 -vvv -D walkadoo_production -A 
: 1481317978:0;mysql -u meyouhealth  -P 9974 -h 127.0.0.1 -vvv -D walkadoo_production -A < mysqldump-schema-wd-20161209.mysql > mysqldump-schema-wd-20161209.mysql.log 
: 1481318119:0;more mysqldump-schema-wd-20161209.mysql.log
: 1481318142:0;tail mysqldump-schema-wd-20161209.mysql.log
: 1481318216:0;mysql -u meyouhealth  -P 9974 -h 127.0.0.1 -vvv -D walkadoo_production -A < wd_reset_new_cp_auto_increment_for_import.sql > wd_reset_new_cp_auto_increment_for_import.log
: 1481318229:0;more wd_reset_new_cp_auto_increment_for_import.log
: 1481318303:0;for i in `ls -l xa* | grep -v '.log' | awk '{print $9}'`\
do \
 mysql -u meyouhealth  -P 9974 -h 127.0.0.1 -D walkadoo_production -A -vvv < ${i} > utf8mb4_logs/wd-${i}-insert-into-new.120916.log  2>&1 & \
done 
: 1481318376:0;mysql -u meyouhealth  -P 9974 -h 127.0.0.1 -vvv -D walkadoo_production -A
: 1481318577:0;diff -u mysqldump-schema-wd-20161208.mysql mysqldump-schema-wd-20161209.mysql
: 1481318793:0;diff -u mysqldump-schema-wd-20161208.mysql mysqldump-schema-wd-20161209.mysql | more
: 1481557084:0;diff -u mysqldump-schema-wd-20161208.mysql mysqldump-schema-wd-20161209.mysql | grep -v "AUTO_INCRE" 
: 1481557094:0;diff -u mysqldump-schema-wd-20161208.mysql mysqldump-schema-wd-20161209.mysql | grep -v "AUTO_INCRE"  | grep -v "^  "
: 1481557115:0;diff -u mysqldump-schema-wd-20161208.mysql mysqldump-schema-wd-20161209.mysql | grep -v "AUTO_INCRE"  | grep -v "^  " | grep -v "^ \/"
: 1481557219:0;diff -u mysqldump-schema-wd-20161208.mysql mysqldump-schema-wd-20161209.mysql | grep -v "AUTO_INCRE"  | grep -v "^  " | grep -v "^ \/" | grep -v "^--"
: 1481557225:0;diff -u mysqldump-schema-wd-20161208.mysql mysqldump-schema-wd-20161209.mysql | grep -v "AUTO_INCRE"  | grep -v "^  " | grep -v "^ \/" | grep -v "^ --"
: 1481557242:0;diff -u mysqldump-schema-wd-20161208.mysql mysqldump-schema-wd-20161209.mysql | grep -v "AUTO_INCRE"  | grep -v "^  " | grep -v "^ \/" | grep -v "^ -- --"
: 1481557246:0;diff -u mysqldump-schema-wd-20161208.mysql mysqldump-schema-wd-20161209.mysql | grep -v "AUTO_INCRE"  | grep -v "^  " | grep -v "^ \/" | grep -v "^-- --"
: 1481557254:0;diff -u mysqldump-schema-wd-20161208.mysql mysqldump-schema-wd-20161209.mysql | grep -v "AUTO_INCRE"  | grep -v "^  " | grep -v "^ \/" | grep -v "-- --"
: 1481557259:0;diff -u mysqldump-schema-wd-20161208.mysql mysqldump-schema-wd-20161209.mysql | grep -v "AUTO_INCRE"  | grep -v "^  " | grep -v "^ \/" | grep -v "\-- --"
: 1481557292:0;diff -u mysqldump-schema-wd-20161208.mysql mysqldump-schema-wd-20161209.mysql | grep -v "AUTO_INCRE"  | grep -v "^  " | grep -v "^ \/" | grep -v "\-- --"  | head
: 1481557343:0;diff -u mysqldump-schema-wd-20161208.mysql mysqldump-schema-wd-20161209.mysql | grep -v "AUTO_INCRE"  | grep -v "^  " | grep -v "^ \/" | grep -v "\-- --"  > final-diff-new-schema-wd-120916.log
: 1481557349:0;more final-diff-new-schema-wd-120916.log
: 1481557461:0;grep  body ../final-diff-new-schema-wd-120916.log | more
: 1481557473:0;cat  ../final-diff-new-schema-wd-120916.log | more
: 1481557499:0;grep active_admin_comments_new *.log
: 1481557513:0;vi wd-xab-insert-into-new.120916.log
: 1481557725:0;ssh -A -L 9973:127.0.0.1:9973 ubuntu@54.152.155.24 ssh -L 9973:wd-utf8mb4-test-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1481558140:0;mv "spool\ rename_and_switch_tables.sql" rename-and-switch-tables.sql 
: 1481558151:0;mv spool\ rename_and_switch_tables.sql rename-and-switch-tables.sql 
: 1481558154:0;vi rename-and-switch-tables.sql
: 1481558312:0;cat rename-and-switch-tables.sql| grep -v "\|" 
: 1481558323:0;cat rename-and-switch-tables.sql| sed s/\|//g 
: 1481558339:0;cat rename-and-switch-tables.sql| sed s/\|//g > wd-rename-and-switch-new-tbls.sql 
: 1481558345:0;rm rename-and-switch-tables.sql
: 1481558349:0;vi wd-rename-and-switch-new-tbls.sql
: 1481558519:0;cat rename_current_to_old.sql | sed s/\|//g 
: 1481558536:0;rm rename_current_to_old.sql
: 1481558641:0;cat rename_current_to_old.sql | sed s/\|//g  > wd-rename-current-to-old.sql
: 1481558747:0;head -5  wd-rename-current-to-old.sql
: 1481558755:0;mysql -u meyouhealth -P 9973 -h 127.0.0.1 -D walkadoo_production -A
: 1481558864:0;mysql -u meyouhealth -P 9973 -h 127.0.0.1 -D walkadoo_production -A -vvv < wd-rename-current-to-old.sql > wd-rename-current-to-old.121216.log 
: 1481558966:0;more wd-rename-current-to-old.121216.log
: 1481559267:0;mysql -u meyouhealth -P 9973 -h 127.0.0.1 -D walkadoo_production -A -vvv 
: 1481559599:0;ls -l db
: 1481559605:0;more schema.rb
: 1481560095:0;grep Warnings wd-x*-insert-into-new.120916*.log
: 1481560110:0;grep Warnings wd-x*-insert-into-new.120916*.log | grep -v "Duplicates: 0  Warnings: 0"
: 1481560151:0;grep Warnings wd-x*-insert-into-new.120916*.log > no_warnings_120916_run.log
: 1481560164:0;grep -i min wd-x*-insert-into-new.120916*.log
: 1481560183:0;grep -i min wd-x*-insert-into-new.120916*.log | grep -v "INSERT"
: 1481560189:0;grep -i min wd-x*-insert-into-new.120916*.log | grep -v "INSERT" | wc -l 
: 1481560215:0;grep -i sec wd-x*-insert-into-new.120916*.log | grep -v "INSERT" | wc -l 
: 1481560231:0;grep -i "Query OK" wd-x*-insert-into-new.120916*.log | grep -v "INSERT" | wc -l 
: 1481560254:0;more ../xab
: 1481560275:0;grep INSERT ../x*
: 1481560278:0;grep INSERT ../x* | wc -l 
: 1481560308:0;grep INSERT ../x* | awk '{print $3}'
: 1481560311:0;grep INSERT ../x* | awk '{print $4}'
: 1481560316:0;grep INSERT ../x* | awk '{print $5}'
: 1481560320:0;grep INSERT ../x* | awk '{print $7}'
: 1481560323:0;grep INSERT ../x* | awk '{print $8}'
: 1481560331:0;grep INSERT ../x* | awk '{print $8}' > x_list
: 1481560339:0;mv x_list ../
: 1481560353:0;cat wd-rename-current-to-old.sql
: 1481560367:0;cat wd-rename-current-to-old.sql | grep RENAME | awk '{print $3}'
: 1481560372:0;cat wd-rename-current-to-old.sql | grep RENAME | awk '{print $3}' r_list
: 1481560382:0;more r_list
: 1481560398:0;wc -l r_list
: 1481560414:0;diff -u x_list r_list
: 1481560426:0;paste x_list r_list
: 1481560448:0;grep active_admin_comments x*
: 1481560478:0;cat wd-rename-current-to-old.sql | grep RENAME | awk '{print $3}' 
: 1481560506:0;cat wd-rename-current-to-old.sql | grep RENAME | awk '{print $3}'  | more
: 1481560517:0;cat wd-rename-current-to-old.sql | grep RENAME | awk '{print $3}'  > r_list
: 1481560520:0;vi r_list
: 1481560529:0;diff x_list r_list
: 1481560666:0;grep feeds mysqldump-schema-wd-20161209.mysql.orig
: 1481560743:0;grep feeds wd_inserts_from_old_into_new.sql
: 1481560756:0;vi wd_inserts_from_old_into_new.sql
: 1481560785:0;wc -l wd_inserts_from_old_into_new.sql
: 1481560794:0;grep INSERT wd_inserts_from_old_into_new.sql | wc -l 
: 1481560804:0;grep INSERT wd_inserts_from_old_into_new.sql 
: 1481560808:0;> i_list
: 1481560816:0;grep INSERT wd_inserts_from_old_into_new.sql > i_list
: 1481560838:0;cat i_list | awk '{print $3'
: 1481560842:0;cat i_list | awk '{print $3}'
: 1481560853:0;cat i_list | awk '{print $3}' | sed s/_new//g
: 1481560858:0;cat i_list | awk '{print $3}' | sed s/_new//g > I_list
: 1481560862:0;rm i_list
: 1481560869:0;mv I_list i_list
: 1481560882:0;more I_list
: 1481560912:0;grep INSERT wd_inserts_from_old_into_new.sql | awk '{print $3}' | sed s/_new//g >  i_list
: 1481560915:0;more i_list
: 1481560919:0;wc -l i_list
: 1481560923:0;wc -l i_list x_list
: 1481560928:0;wc -l i_list x_list r_list
: 1481560953:0;more xa*
: 1481560981:0;vi xab
: 1481561403:0;vi table_list.log
: 1481561430:0;cat table_list.log| sed s/\|//g
: 1481561436:0;cat table_list.log| sed s/\| //g 
: 1481561442:0;cat table_list.log| sed s/\|//g 
: 1481561874:0;cat table_list.log| sed s/\|//g  > t_list
: 1481561878:0;vi t_list
: 1481561896:0;wc -l x_list
: 1481561899:0;wc -l x_list r_list
: 1481561903:0;wc -l x_list r_list t_list
: 1481561911:0;more x_list
: 1481561919:0;grep feeds x_list
: 1481561936:0;grep INSERT x*
: 1481561948:0;grep INSERT x* | awk '{print $3}'
: 1481561952:0;grep INSERT x* | awk '{print $4}'
: 1481561958:0;grep INSERT x* | awk '{print $4}' | sort
: 1481561966:0;grep INSERT x* | awk '{print $4}' | sort | sed s/_new//g
: 1481561973:0;grep INSERT x* | awk '{print $4}' | sort | sed s/_new//g > new_x_list
: 1481561978:0;wc -l new_x_list
: 1481561989:0;vi i_list
: 1481561996:0;wc -l new_x_list i_list t_list
: 1481562003:0;grep feeds x*
: 1481562114:0;mysql -u meyouhealth -P 9973 -h 127.0.0.1 -D walkadoo_production -A -vvv
: 1481562200:0;grep conv *.sql
: 1481567441:0;shipit list walkadoo-production
: 1481567463:0;shipit environment help
: 1481567468:0;shipit environment list
: 1481567512:0;shipit environment show wellbeingtracker-production
: 1481567526:0;shipit logs 4332
: 1481567539:0;shipit deployment logs 4332
: 1481567765:0;	1. Shipit ssh deployment-api
: 1481568522:0;git log | more
: 1481568591:0;git log | grep d57c7d3a6d1dacb9e4a98bc31695efae6e1a83bd
: 1481569066:0;cd spectacle
: 1481569079:0;cd myh-aws
: 1481569102:0;cd dockerbook-code
: 1481569113:0;cd hes_docker_mysql
: 1481569121:0;cd ../chef-hes
: 1481569133:0;cd dotfiles
: 1481569140:0;cd ../quitnet-server
: 1481569234:0;cd ../walkadoo-ios
: 1481569385:0;cd guides
: 1481569393:0;cd ../myh-monitoring
: 1481569401:0;cd ../surge_introduction_to_chef
: 1481569470:0;mv .env old_env
: 1481569548:0;git stash branch lifecycle-management-prototype
: 1481569618:0;cd ../chef-meyouhealth
: 1481569633:0;cd ../sourcecode
: 1481569646:0;cd chef-walkadoo
: 1481569666:0;cd ../myh-terraform
: 1481569680:0;git checkout environments/staging/quitnet/terraform.tfstate
: 1481569744:0;rm -rf myh-terraform.oops
: 1481569846:0;sudo pip install --upgrade awscli
: 1481570161:0;history | grep brew | grep upgrade
: 1481570227:0;rm @
: 1481570253:0;more wd-truncate-tbl-new-testing.sql
: 1481570281:0;more wd-rename-and-switch-new-tbls.sql
: 1481570290:0;rm wd-rename-and-switch-new-tbls.sql
: 1481570304:0;more rename_current_to_old.sql
: 1481570319:0;mv rename_current_to_old.sql rename_current_to_old.sql.orig
: 1481643351:0;cd ~Documents/walkadoo
: 1481643356:0;cd ~/Documents/walkadoo
: 1481643381:0;ls -l utf8mb4_logs
: 1481643404:0;ls -l wd-xa*-insert-into-new.120916.log
: 1481643413:0;grep sec wd-xa*-insert-into-new.120916.log
: 1481643424:0;grep min wd-xa*-insert-into-new.120916.log
: 1481643641:0;grep experiences wd-xa*-insert-into-new.120916.log
: 1481643703:0;grep user_events wd-xa*-insert-into-new.120916.log
: 1481643707:0;more wd-xaq-insert-into-new.120916.log
: 1481643737:0;grep email_logs wd-xa*-insert-into-new.120916.log
: 1481643743:0;more wd-xae-insert-into-new.120916.log
: 1481643790:0;grep likes wd-xa*-insert-into-new.120916.log
: 1481643795:0;more wd-xah-insert-into-new.120916.log
: 1481643821:0;grep sms_logs
: 1481643830:0;grep sms_logs wd-xa*-insert-into-new.120916.log
: 1481643836:0;more wd-xag-insert-into-new.120916.log
: 1481643846:0;more wd-xal-insert-into-new.120916.log
: 1481644723:0;more wd-utf8mb4-insert-into-new.sh
: 1481644739:0;ls -l *insert*
: 1481644744:0;more wd_inserts_from_old_into_new.sql
: 1481644756:0;cp wd_inserts_from_old_into_new.sql wd_inserts_from_old_into_new_bare.sql 
: 1481644761:0;vi wd_inserts_from_old_into_new_bare.sql
: 1481644794:0;mkdir wd_script_5 
: 1481644801:0;ls x*
: 1481644811:0;mv x* wd_script_5
: 1481644828:0;man split
: 1481644884:0;man split 
: 1481644911:0;split -l 1 wd_inserts_from_old_into_new_bare.sql -a wd-
: 1481644926:0;split -l 1 wd_inserts_from_old_into_new_bare.sql -p wd
: 1481644936:0;split -l 1 -p wd wd_inserts_from_old_into_new_bare.sql 
: 1481644993:0;split -l 1 wd_inserts_from_old_into_new.sql 
: 1481645003:0;more xdt
: 1481645006:0;rm x*
: 1481645031:0;split -p wd -l 1 wd_inserts_from_old_into_new_bare.sql 
: 1481645037:0;split -l 1 wd_inserts_from_old_into_new_bare.sql 
: 1481645078:0;grep experience x*
: 1481645138:0;mv xar xaa6
: 1481645145:0;grep user_cards x*
: 1481645161:0;mv xcz xaa1
: 1481645175:0;grep user_events x*
: 1481645196:0;mv xdb xaa5
: 1481645215:0;grep email_logs x*
: 1481645230:0;mv xaq xaa2
: 1481645248:0;grep likes x*
: 1481645273:0;mv xbe xaa3
: 1481645279:0;grep sms_logs x*
: 1481645293:0;mv xbz xaa4
: 1481645316:0;more x*
: 1481647009:0;grep experiences wd-utf8mb4-char-conversion*
: 1481647021:0;more wd-utf8mb4-char-conversion1b.sh-20161206-1481051027.log
: 1481647032:0;more wd-utf8mb4-char-conversion1b.sh-20161207-1481131820.log
: 1481647046:0;more changes5.log
: 1481647081:0;grep user_cards wd-utf8mb4-char-conversion*.log
: 1481647102:0;more wd-utf8mb4-char-conversion1d.sh-20161207-1481131820.log
: 1481647121:0;grep min wd-utf8mb4-char-conversion1d*.log
: 1481647132:0;more wd-utf8mb4-char-conversion1d.sh-20161206-1481051079.log
: 1481647158:0;more wd-utf8mb4-char-conversion2b.sh-20161207-1481138688.log
: 1481653381:0;cd db
: 1481653386:0;vi schema.rb
: 1481655228:0;cd utf8mb4_logs
: 1481655253:0;more wd-utf8mb4-char-conversion1_a.sh-20161207-1481131820.log
: 1481655273:0;more wd-utf8mb4-char-conversion2b.sh-20161206-1481052709.log
: 1481655285:0;ls -l wd*conv*
: 1481655291:0;more wd-utf8mb4-char-conversion1i.sh-20161206-1481051242.log
: 1481655358:0;grep -i min wd-utf8mb4-char-conversion*.log 
: 1481655374:0;grep -i min wd-utf8mb4-char-conversion*.log  | grep -v Uptime | more
: 1481655400:0;grep -i min wd-utf8mb4-char-conversion*.log  | grep -v Uptime | grep -v "ALTER TABLE"
: 1481655966:0;more wd-utf8mb4-char-conversion2a.sh-20161206-1481052662.log
: 1481656128:0;more wd_parallel_convert.log
: 1481659922:0;ssh -A -L 9972:127.0.0.1:9972 ubuntu@54.152.155.24 ssh -L 9972:wd-utf8mb4-test-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1481660487:0;mysql -u meyouhealth  -P 9972 -h 127.0.0.1 -vvv -D walkadoo_production -A
: 1481729930:0;cat wd-varchar-columns-used-in-indexes.txt
: 1481729939:0;cat wd-varchar-columns-used-in-indexes.txt | grep -v "_old"
: 1481729961:0;cat wd-varchar-columns-used-in-indexes.txt | grep -v "_old" | grep -v "_retest"
: 1481729977:0;cat wd-varchar-columns-used-in-indexes.txt | grep -v "_old" | grep -v "_retest" > wd-varchar-columns-used-in-indexes_complete.txt
: 1481729982:0;vi wd-varchar-columns-used-in-indexes_complete.txt
: 1481730275:0;cat wd-varchar-columns-used-in-indexes_complete.txt | awk -F "\|" '{print $2, $3}'
: 1481730283:0;cat wd-varchar-columns-used-in-indexes_complete.txt | awk -F "\|" '{print $3, $4}'
: 1481730332:0;cat wd-varchar-columns-used-in-indexes_complete.txt | awk -F "\|" '{print "ALTER TABLE ", $3, $4}'
: 1481730364:0;more changes.sql
: 1481730390:0;cat wd-varchar-columns-used-in-indexes_complete.txt | awk -F "\|" '{print "ALTER TABLE ", $3, " CHANGE ", $4}'
: 1481730450:0;cat wd-varchar-columns-used-in-indexes_complete.txt | awk -F "\|" '{print "ALTER TABLE ", $3, " CHANGE `", $4, "` varchar(191) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci; " }'
: 1481730473:0;cat wd-varchar-columns-used-in-indexes_complete.txt | awk -F "\|" '{print "ALTER TABLE ", $3, " CHANGE `", $4, "` varchar(191) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci; " }' > core_47_to_change.sql 
: 1481730505:0;grep _old wd-varchar-columns-used-in-indexes.txt
: 1481730518:0;vi core_47_to_change.sql
: 1481730748:0;cat core_47_to_change.sql
: 1481731212:0;mv core_47_to_change.sql wd_core_47_to_change.sql
: 1481766908:0;grep -v "_old" wd-varchar-columns-used-in-indexes.txt
: 1481766922:0;grep -v "_old" wd-varchar-columns-used-in-indexes.txt | grep new
: 1481766927:0;grep -v "_old" wd-varchar-columns-used-in-indexes.txt | wc -l 
: 1481766937:0;grep -v "_old" wd-varchar-columns-used-in-indexes.txt | grep -v "retest"
: 1481766942:0;grep -v "_old" wd-varchar-columns-used-in-indexes.txt | grep -v "retest" | wc -l 
: 1481766973:0;more wd-varchar-columns-used-in-indexes_complete.txt
: 1481766982:0;wc -l wd-varchar-columns-used-in-indexes_complete.txt
: 1481766987:0;wc -l wd_core_47_to_change.sql
: 1481767008:0;grep ALTER mysqldump-schema-wd-20161209.mysql
: 1481767026:0;vi wd-alter-db.sql 
: 1481767056:0;mkdir wd-utf8-inserts
: 1481767064:0;ls xa*
: 1481767074:0;mv xa* wd-utf8-inserts
: 1481767087:0;mv xb* wd-utf8-inserts
: 1481767101:0;mv xc* wd-utf8-inserts
: 1481767112:0;mv xd* wd-utf8-inserts
: 1481767126:0;ls -l wd_script_5
: 1481767143:0;mv wd_script_5 wd-utf8-inserts-5
: 1481767224:0;ssh -A -L 9971:127.0.0.1:9971 ubuntu@54.152.155.24 ssh -L 9971:wd-utf8mb4-test-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1481767241:0;mkdir wd-utf8-alter-47
: 1481767258:0;cp wd_core_47_to_change.sql wd-utf8-alter-47
: 1481767266:0;cp wd-alter-db.sql wd-utf8-alter-47
: 1481767311:0;split -l 1 wd_core_47_to_change.sql
: 1481767334:0;mv wd-alter-db.sql xaa1
: 1481767403:0;for i in `ls -l xa* | grep -v '.log' | awk '{print $9}'`\
do \
 mysql -u meyouhealth  -P 9971 -h 127.0.0.1 -D walkadoo_production -A -vvv < ${i} > utf8mb4_logs/wd-${i}-change-varchar-timing-47-core.log  2>&1 & \
done 
: 1481767458:0;for i in `ls -l xa* | grep -v '.log' | awk '{print $9}'`\
do \
 mysql -u meyouhealth  -P 9971 -h 127.0.0.1 -D walkadoo_production -A -vvv < ${i} > wd-${i}-change-varchar-timing-47-core.log  2>&1 & \
done 
: 1481767475:0;more wd-xaa-change-varchar-timing-47-core.log
: 1481767538:0;grep active_admin *.sql 
: 1481767673:0;vi x*
: 1481768549:0;grep CHANGE x*
: 1481768591:0;mv xaa xaa2
: 1481768603:0;mv xaa1 xaa 
: 1481768651:0;for i in `ls -l x* | grep -v '.log' | awk '{print $9}'`\
do \
 mysql -u meyouhealth  -P 9971 -h 127.0.0.1 -D walkadoo_production -A -vvv < ${i} > wd-${i}-change-varchar-timing-47-core.log  2>&1 & \
done 
: 1481768655:0;jobs 
: 1481769089:0;more wd-xbi-change-varchar-timing-47-core.log
: 1481769115:0;grep Warnings *.log | wc -l 
: 1481769160:0;mysql -u meyouhealth  -P 9971 -h 127.0.0.1 -vvv -D walkadoo_production -A
: 1481770159:0;more wd-xad-change-varchar-timing-47-core.log
: 1481770167:0;more wd-xaj-change-varchar-timing-47-core.log
: 1481770178:0;more wd-xbn-change-varchar-timing-47-core.log
: 1481771194:0;grep Warnings *.log
: 1481771288:0;shipit env concierge-production
: 1481771320:0;shipit env show concierge-production
: 1481814612:0;ls -l *conv*
: 1481814652:0;wc -l wd-xb*-change-varchar-timing-47-core.log
: 1481814672:0;ls -ltra wd-xb*-change-varchar-timing-47-core.log
: 1481814678:0;ls -ltra wd-xb*-change-varchar-timing-47-core.log | wc -l
: 1481814697:0;ls -ltra wd-x*-change-varchar-timing-47-core.log | wc -l
: 1481814811:0;more wd-xam-change-varchar-timing-47-core.log
: 1481814833:0;more wd-xbl-change-varchar-timing-47-core.log
: 1481814838:0;more wd-xal-change-varchar-timing-47-core.log
: 1481815415:0;grep ALTER wd-x*-change-varchar-timing-47-core.log 
: 1481815442:0;grep ALTER wd-x*-change-varchar-timing-47-core.log | awk -F ":" '{print $2}'
: 1481815449:0;grep ALTER wd-x*-change-varchar-timing-47-core.log | awk -F ":" '{print $2}' > 1 
: 1481815458:0;grep sec wd-x*-change-varchar-timing-47-core.log 
: 1481815477:0;grep sec wd-x*-change-varchar-timing-47-core.log | awk -F ":" '{print $2}'
: 1481815481:0;grep sec wd-x*-change-varchar-timing-47-core.log | awk -F ":" '{print $2}' > 2
: 1481815533:0;paste 1 2 > 3
: 1481815550:0;cp 3 alter_tbl_col_with_timings.txt
: 1481816466:0;cd wd-utf8-inserts
: 1481816478:0;cd ../utf8mb4_logs
: 1481816504:0;grep INSERT wd-x*insert*120916*.log
: 1481816528:0;grep INSERT wd-x*insert*120916*.log | awk -F ":" '{print $2}'
: 1481816532:0;grep INSERT wd-x*insert*120916*.log | awk -F ":" '{print $2}' > 1
: 1481816545:0;grep sec wd-x*insert*120916*.log
: 1481816559:0;grep sec wd-x*insert*120916*.log | awk -F ":" '{print $2}'
: 1481816562:0;grep sec wd-x*insert*120916*.log | awk -F ":" '{print $2}' > 2 
: 1481816594:0;cp 3 insert_all_utf8mb4_timings.txt
: 1481817556:0;grep -i convert *.sql 
: 1481817581:0;cat verification_post_assessment_pre_sql.sql 
: 1481817593:0;cat verification_post_assessment_pre_sql.sql | grep CONVERT
: 1481817600:0;cat verification_post_assessment_pre_sql.sql | grep CONVERT | sed s/\|//g
: 1481817623:0;cat verification_post_assessment_pre_sql.sql | grep CONVERT | sed s/\|//g > convert_all_tables.sql 
: 1481817663:0;ssh -A -L 9970:127.0.0.1:9970 ubuntu@54.152.155.24 ssh -L 9970:wd-utf8mb4-test-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1481819729:0;tail -20 convert_all_tables.sql
: 1481819767:0;grep "--" convert_all_tables.sql
: 1481820236:0;vi convert_all_tables.sql
: 1481820254:0;mysql -u meyouhealth  -P 9970 -h 127.0.0.1 -vvv -D walkadoo_production -A < convert_all_tables.sql > wd-convert-all-tables.121516.log 
: 1481820263:0;more wd-convert-all-tables.121516.log
: 1481820326:0;grep "^-- " convert_all_tables.sql
: 1481825131:0;cp convert_all_tables.sql wd-utf8-alter-47
: 1481825135:0;xs wd-utf8-alter-47
: 1481829907:0;mysql -u meyouhealth  -P 9970 -h 127.0.0.1 -vvv -D walkadoo_production -A
: 1481830185:0;for i in `ls -l x* | grep -v '.log' | awk '{print $9}'`\
do \
 mysql -u meyouhealth  -P 9969 -h 127.0.0.1 -D walkadoo_production -A -vvv < ${i} > wd-${i}-end-2-end-change-varchar-timing-47-core.log  2>&1 & \
done 
: 1481830348:0;mysql -u meyouhealth  -P 9969 -h 127.0.0.1 -vvv -D walkadoo_production -A
: 1481830475:0;more xaa2
: 1481830561:0;more wd-xaa-end-2-end-change-varchar-timing-47-core.log
: 1481830600:0;ssh -A -L 9969:127.0.0.1:9969 ubuntu@54.152.155.24 ssh -L 9969:wd-utf8mb4-test-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1481830613:0;rm *.log
: 1481830616:0;l s-ltra
: 1481830655:0;cat xaa
: 1481830670:0;mv xaa alter_db_utf8mb4.sql 
: 1481830678:0;vi replication_stop.sql 
: 1481832387:0;ssh -A -L 9960:127.0.0.1:9960 ubuntu@54.152.155.24 ssh -L 9960:wd-utf8mb4-test-read2.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1481832437:0;mysql -u meyouhealth  -P 9960 -h 127.0.0.1 -vvv -D walkadoo_production -A < replication_stop.sql > replication_stop.log
: 1481832445:0;more replication_stop.log
: 1481832463:0;cat alter_db_utf8mb4.sql
: 1481832474:0;mysql -u meyouhealth  -P 9960 -h 127.0.0.1 -vvv -D walkadoo_production -A < alter_db_utf8mb4.sql > alter_db_utf8mb4.log
: 1481832480:0;more alter_db_utf8mb4.log
: 1481832490:0;cat xaa2
: 1481832495:0;mv xaa2 xaa
: 1481832499:0;ls -l x*
: 1481832502:0;ls -l x* | wc -l 
: 1481832526:0;for i in `ls -l x* | grep -v '.log' | awk '{print $9}'`\
do \
 mysql -u meyouhealth  -P 9960 -h 127.0.0.1 -D walkadoo_production -A -vvv < ${i} > wd-${i}-end-2-end-change-varchar-timing-47-core.log  2>&1 & \
done 
: 1481832547:0;mysql -u meyouhealth  -P 9960 -h 127.0.0.1 -vvv -D walkadoo_production -A
: 1481832635:0;more convert_all_tables.sql | grep -v "^-- " 
: 1481832644:0;more convert_all_tables.sql | grep -v "^-- "  > remaining-conversion.sql 
: 1481832656:0;mkdir conversion
: 1481832663:0;mv remaining-conversion.sql conversion
: 1481832681:0;split -l 1 remaining-conversion.sql
: 1481832860:0;jobs | wc -l
: 1481838666:0;grep sec wd-x*end*log
: 1481838670:0;grep sec wd-x*end*log | wc -l 
: 1481838745:0;for i in `ls -l x* | grep -v '.log' | awk '{print $9}'`\
do \
 mysql -u meyouhealth  -P 9960 -h 127.0.0.1 -D walkadoo_production -A -vvv < ${i} > wd-${i}-end-2-end-change-conversion-timing-47-core.log  2>&1 & \
done 
: 1481901988:0;more wd-xae-end-2-end-change-conversion-timing-47-core.log
: 1481902551:0;grep ALTER wd-x*-end-2-end-change-conversion-timing-47-core.log
: 1481902567:0;grep ALTER wd-x*-end-2-end-change-conversion-timing-47-core.log | awk -F ":" '{print $2}'
: 1481902571:0;grep ALTER wd-x*-end-2-end-change-conversion-timing-47-core.log | awk -F ":" '{print $2}' > 1 
: 1481902585:0;grep sec wd-x*-end-2-end-change-conversion-timing-47-core.log | awk -F ":" '{print $2}' > 2
: 1481902589:0;paste 1 2 
: 1481902596:0;paste 1 2 > 3 
: 1481902631:0;cp 3 conversion-all-utf8mb4-timings.txt
: 1481903657:0;wc -l 3
: 1481903682:0;wc -l remaining-conversion.sql
: 1481904987:0;ssh -A -L 9959:127.0.0.1:9959 ubuntu@54.152.155.24 ssh -L 9959:wd-utf8mb4-test-read2.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1481905001:0;mysql -u meyouhealth  -P 9959 -h 127.0.0.1 -vvv -D walkadoo_production -A
: 1481905557:0;grep active x*
: 1481905634:0;vi xacd
: 1482162346:0;more pg_grants_iris_stg.sql
: 1482162381:0;cp pg_grants_concierge_prod.sql pg_grants_wd_prod_migration_test.sql 
: 1482162386:0;vi pg_grants_wd_prod_migration_test.sql
: 1482163903:0;deis info
: 1482164355:0;pip --help
: 1482164373:0;history | grep pip | more
: 1482164574:0;deis login https://deis.meyouhealth.com --ssl-verify=true
: 1482164682:0;deis config:get iris-production
: 1482164696:0;deis apps --help
: 1482164702:0;deis apps:list
: 1482164745:0;deis apps:logs concierge-production
: 1482164752:0;deis apps:logs --help
: 1482164761:0;deis apps:logs -a concierge-production -n 100
: 1482165216:0;kops export kubecfg k8s.myhstg.com
: 1482165237:0;kops export kubecfg k8s.meyouhealth.com
: 1482165246:0;grep s3 .zshrc
: 1482172269:0;cp config conifg-original 
: 1482172445:0;grep server config
: 1482175554:0;ssh -A -L 9958:127.0.0.1:9958 ubuntu@54.152.155.24 ssh -L 9959:mysql-shared-production.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.37
: 1482175580:0;ssh -A -L 9955:127.0.0.1:9955 ubuntu@54.152.155.24 ssh -L 9955:mysql-shared-production.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.37
: 1482175606:0;mysql -u meyouhealth  -P 9955 -h 127.0.0.1 -vvv 
: 1482175675:0;grep DATABASE *
: 1482175840:0;grep new mysqldump-schema-wd-20161209.mysql
: 1482175870:0;cp mysqldump-schema-wd-20161209.mysql mysqldump-schema-wd-DMS-only.sql 
: 1482175875:0;vi mysqldump-schema-wd-DMS-only.sql
: 1482175978:0;mysql -u meyouhealth  -p -P 9955 -h 127.0.0.1 -vvv -D walkadoo_production < mysqldump-schema-wd-DMS-only.sql > TESTING-DMS-ONLY-mysqldump-schema-wd-DMS-only.log 
: 1482176058:0;more TESTING-DMS-ONLY-mysqldump-schema-wd-DMS-only.log
: 1482177100:0;deis loging https://deis.myhstg.com --ssl-verify=true
: 1482177503:0;kubectl --namespace iris-prod get pods
: 1482177512:0;kubectl --namespace iris-production get pods
: 1482199841:0;mysql -u meyouhealth -p -P 9955 -h 127.0.0.1 -vvv 
: 1482199924:0;ssh -A -L 9954:127.0.0.1:9954 ubuntu@54.152.155.24 ssh -L 9954:mysql-shared-production.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.37
: 1482199935:0;mysql -u meyouhealth -p -P 9954 -h 127.0.0.1 -vvv 
: 1482258243:0;ssh -A -L 9953:127.0.0.1:9953 ubuntu@54.152.155.24 ssh -L 9953:mysql-shared-production.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.37
: 1482259093:0;grep CONCAT *.sql 
: 1482259313:0;grep "ADD INDEX" *.sql 
: 1482259320:0;grep -i "ADD INDEX" *.sql 
: 1482259328:0;grep -i "ADD INDEX" ../dc/*.sql 
: 1482259339:0;grep -i " INDEX" ../dc/*.sql 
: 1482259381:0;cd ../dc/
: 1482259391:0;ls -ltra db-research-objects-to-create.sql 
: 1482259399:0;more db-research-objects-to-create.sql
: 1482260884:0;ssh -A -L 9950:127.0.0.1:9950 ubuntu@54.152.155.24 ssh -L 9950:walkadoo-production-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1482261197:0;grep "CREATE INDEX " dbsolo-schema-scripting.sql
: 1482261207:0;more dbsolo-schema-scripting.sql
: 1482261226:0;more dbdbsolo-schema-scripting.sql\
: 1482261245:0;cat dbsolo-schema-scripting.sql\\
: 1485181125:0;\\
: 1482261252:0;cat dbsolo-schema-scripting.sq*\
\\
: 1485181125:0;'
: 1482261258:0;cat dbsolo-schema-scripting.sq*
: 1482261272:0;cat dbsolo-schema-scripting.sq* > db-solo5-wd-schema-scripting.sql 
: 1482261278:0;rm dbsolo-schema-scripting.s*
: 1482261298:0;grep "CREATE INDEX " db-solo5-wd-schema-scripting.sql
: 1482261332:0;grep "CREATE INDEX " db-solo5-wd-schema-scripting.sql > wd-create-idx-all.sql 
: 1482261356:0;cp mysqldump-schema-wd-DMS-only.sql mysqldump-schema-wd-DMS-only-no-idx.sql 
: 1482261645:0;more wd-create-idx-all.sql | grep -i id
: 1482261682:0;grep index_user_events_on_user_id mysqldump-schema-wd-DMS-only.sql
: 1482261824:0;vi mysqldump-schema-wd-DMS-only-no-idx.sql
: 1482263776:0;mv mysqldump-schema-wd-DMS-only-no-idx.sql test-mysqldump-schema-wd-DMS-only-no-idx.sql
: 1482264400:0;mysql -u meyouhealth -p -P 9953 -h 127.0.0.1 -vvv 
: 1482265324:0;mysql -u meyouhealth -p -P 9953 -h 127.0.0.1 -vvv < test-mysqldump-schema-wd-DMS-only-no-idx.sql > test-mysqldump-schema-wd-DMS-only-no-idx.log 
: 1482265492:0;more test-mysqldump-schema-wd-DMS-only-no-idx.log
: 1482265505:0;mysql -u meyouhealth -p -P 9953 -h 127.0.0.1 -vvv
: 1482265673:0;mysql -u meyouhealth -p -P 9953 -h 127.0.0.1 -vvv -D walkadoo_production
: 1482341695:0;ssh -A -L 9945:127.0.0.1:9945 ubuntu@54.152.155.24 ssh -L 9945:wd-utf8mb4-dms-test1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1482341719:0;cd../
: 1482341736:0;mysql -u meyouhealth -p -P 9945 -h 127.0.0.1 -vvv -D walkadoo_production
: 1482343180:0;ssh -A -L 9963:127.0.0.1:9963 ubuntu@54.152.155.24 ssh -L 9963:mysql-shared-production.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.37
: 1482343204:0;mysql -u meyouhealth -p -P 9963 -h 127.0.0.1 -vvv 
: 1482365840:0;ssh -A -L 9944:127.0.0.1:9944 ubuntu@54.152.155.24 ssh -L 9944:wd-utf8mb4-dms-test2.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1482365870:0;mysql -u meyouhealth -p -P 9944 -h 127.0.0.1 -vvv -D walkadoo_production
: 1482436855:0;vi test-mysqldump-schema-wd-DMS-only-no-idx.sql
: 1482507134:0;mysql -u meyouhealth -p -P 9963 -h 127.0.0.1 -vvv -D walkadoo_production
: 1482507260:0;mysql -u meyouhealth -p -P 9963 -h 127.0.0.1 -vvv -D walkadoo_production < test-mysqldump-schema-wd-DMS-only-no-idx.sql > test-mysqldump-schema-wd-DMS-only-no-idx.122316.log 
: 1482508046:0;man trace
: 1482508055:0;man strace
: 1482508227:0;kubectl --context --- help
: 1482508230:0;kubectl --context --help
: 1482508237:0;kubectl -context --help
: 1482508288:0;kubectl --context ... exec -ti ... /runner/init "bundle exec rails console" 
: 1482508452:0;deis login https://deis.myhstg.com --ssl-verify=true
: 1482508498:0;deis -- help
: 1482509976:0;ls -l /usr/local/bin/
: 1482509981:0;ls -l /usr/local/bin/ | grep deis
: 1482510159:0;DEIS_PROFILE=staging deis ps -a helloworld
: 1482512990:0;deis info -a iris-staging 
: 1482513136:0;deis shortcuts
: 1482548074:0;mkdir wd-create-idx
: 1482548080:0;cp wd-create-idx-all.sql wd-create-idx
: 1482548097:0;split -1 wd-create-idx-all.sql
: 1482548108:0;ls -ltra x* | wc -l
: 1482548187:0;vi .my.cnf
: 1482548409:0;mysql -u meyouhealth  -P 9963 -h 127.0.0.1 -D walkadoo_production -A
: 1482548444:0;more xab
: 1482548483:0;for i in `ls -l x* | grep -v '.log' | awk '{print $9}'`\
do \
 mysql -u meyouhealth  -P 9963 -h 127.0.0.1 -D walkadoo_production -A -vvv < ${i} > wd-${i}-all-utf8mb4-idx-create-timing.log  2>&1 & \
done 
: 1482548833:0;ls -ltra | grep "  0 Dec 23 22:"
: 1482548844:0;ls -ltrah | wc -l 
: 1482548853:0;ls -ltra | grep "  0 Dec 23 22:" | wc -l
: 1482548871:0;ls -ltrah | grep -v "  0 Dec 23 22:" | wc -l
: 1482549603:0;vi start_time_est
: 1482550455:0;more wd-xbw-all-utf8mb4-idx-create-timing.log
: 1482550468:0;grep sec *.log
: 1482551083:0;ls -l x* |wc -l
: 1482556972:0;grep user_cards *.log
: 1482557279:0;ls -ltra test-mysqldump-schema-wd-DMS-only-no-idx.122316.log
: 1483457207:0;cd Documents/dc
: 1483457216:0;history | grep dc-production3
: 1483457231:0;history | grep ssh |grep dc
: 1483457330:0;ssh -A -L 9907:dc-production3.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 heidischmidt@ec2-54-242-121-10.compute-1.amazonaws.com -N
: 1483457399:0;mysql -u meyouhealth -p -P 9907 -h 127.0.0.1 -D meyouhealth_production -A
: 1483458119:0;ssh -A -L 9934:127.0.0.1:9934 ubuntu@54.152.155.24 ssh -L 9934:wd-utf8mb4-dms-test2.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1483458176:0;mysql -u meyouhealth -p -P 9934 -h 127.0.0.1 -vvv -D walkadoo_production
: 1483458335:0;mysql -u meyouhealth -p -P 9943 -h 127.0.0.1 -vvv 
: 1483458556:0;history |grep mysqldump
: 1483458704:0;more mysqldump-schema-wd-20170103.mysql
: 1483458710:0;mv 
: 1483458710:0;--
: 1483458734:0;mv mysqldump-schema-wd-20170103.mysql DMS-CDC-working-schema-mysqldump-schema-wd-20170103.mysql
: 1483458794:0;more test-mysqldump-schema-wd-DMS-only-no-idx.122316.log
: 1483458817:0;ls -l mysqldump-schema-wd-201*.mysql
: 1483458937:0;mysql -u meyouhealth -p -P 9943 -h 127.0.0.1 -vvv < mysqldump-schema-wd-20161209.mysql > recreate-wd-clean-schema-for-dms-cdc-test-all.log 
: 1483459113:0;more wd-rename-current-to-old.sql
: 1483459147:0;mysql -u meyouhealth -p -P 9943 -h 127.0.0.1 -vvv < wd-rename-current-to-old.sql > dmd-cdc-clean-wd-rename-current-to-old.log
: 1483459174:0;mysql -u meyouhealth -p -P 9943 -h 127.0.0.1 -vvv -D walkadoo_production < wd-rename-current-to-old.sql > dmd-cdc-clean-wd-rename-current-to-old.log
: 1483459207:0;more dmd-cdc-clean-wd-rename-current-to-old.log
: 1483459222:0;vi wd-rename-current-to-old.sql
: 1483459247:0;mysql -u meyouhealth -p -P 9943 -h 127.0.0.1 -vvv -D walkadoo_production < wd-rename-current-to-old.sql > dmd-cdc-clean-wd-rename-current-to-old-rest.log
: 1483459404:0;ls -l *schema*.sh
: 1483459424:0;sh -x mysqldump_schema.sh 9943 
: 1483459618:0;mv mysqldump-schema-wd-20170103.mysql dms-cdc-reset-itself-to-ucs2-while-recreating-it-mysqldump-schema-wd-20170103.mysql
: 1483459693:0;more dmd-cdc-clean-wd-rename-current-to-old-rest.log
: 1483459958:0;mv dmd-cdc-clean-wd-rename-current-to-old.log dms-cdc-clean-wd-rename-current-to-old.log
: 1483459964:0;more dms-cdc-clean-wd-rename-current-to-old.log
: 1483460113:0;more mysqldump-schema-wd-20161209.mysql
: 1483460140:0;history | grep mysqldump-schema-wd-20161209.mysql
: 1483460165:0;ls -l recreate-wd-clean-schema-for-dms-cdc-test-all.log
: 1483460178:0;more recreate-wd-clean-schema-for-dms-cdc-test-all.log
: 1483460205:0;mysql -u meyouhealth -p -P 9943 -h 127.0.0.1 -vvv -D walkadoo_production 
: 1483460251:0;mysql -u meyouhealth -p -P 9943 -h 127.0.0.1 -vvv -D walkadoo_production -A
: 1483469292:0;cat wd_core_47_to_change.sql
: 1483469305:0;mysql -u meyouhealth -p -P 9941 -h 127.0.0.1 -vvv -D walkadoo_production -A 
: 1483469346:0;vi wd_core_47_to_change.sql
: 1483469368:0;mv wd_core_47_to_change.sql broken_wd_core_47_to_change.sql
: 1483469393:0;more wd-action-tracking-during-deploy.sql
: 1483469424:0;cat broken_wd_core_47_to_change.sql
: 1483469442:0;cat broken_wd_core_47_to_change.sql | awk '{print $1}'
: 1483469473:0;cat broken_wd_core_47_to_change.sql | awk '{print $1, " ", $2, " ", $3, " ", $4, " ", $5 }'
: 1483469493:0;cat broken_wd_core_47_to_change.sql | awk '{print $1, " ", $2, " ", $3, " ", $4, " ", $5, " ", $5, " "  }'
: 1483469507:0;cat broken_wd_core_47_to_change.sql | awk '{print $1, " ", $2, " ", $3, " ", $4, " ", $5, " ", $5, " ", $6, $7  }'
: 1483469521:0;cat broken_wd_core_47_to_change.sql | awk '{print $1, " ", $2, " ", $3, " ", $4, " ", $5, " ", $5, " ", $6, $7, $8, $9, $10  }'
: 1483469526:0;cat broken_wd_core_47_to_change.sql | awk '{print $1, " ", $2, " ", $3, " ", $4, " ", $5, " ", $5, " ", $6, $7, $8, $9, $10, $11  }'
: 1483469538:0;cat broken_wd_core_47_to_change.sql | awk '{print $1, " ", $2, " ", $3, " ", $4, " ", $5, " ", $5, " ", $6, $7, $8, $9, $10, $11  }' > wd_core_47_to_change.sql
: 1483469553:0;mysql -u meyouhealth -p -P 9941 -h 127.0.0.1 -vvv -D walkadoo_production -A < wd_core_47_to_change.sql > dms-cdc-load-stop-chg-column-b4-add-indexes-wd_core_47_to_change.log 
: 1483469608:0;ls -l *idx*
: 1483469611:0;ls -l *idx*.sql
: 1483469633:0;ls -l | grep idx
: 1483469803:0;ls -l wd*
: 1483469837:0;more after-core-47-alter-and-conversion.log
: 1483473958:0;cat dms-cdc-load-stop-chg-column-b4-add-indexes-wd_core_47_to_change.log | grep ALTER | wc -l 
: 1483474044:0;more dms-cdc-load-stop-chg-column-b4-add-indexes-wd_core_47_to_change.log
: 1483475270:0;ls -ltr *idx*
: 1483475337:0;more xacd
: 1483475350:0;CREATE INDEX index_active_admin_comments_on_author_type_and_author_id ON active_admin_comments (author_type ASC, author_id ASC) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci
: 1483475451:0;mysql -u meyouhealth -p -P 9941 -h 127.0.0.1 -vvv -D walkadoo_production -A
: 1483475539:0;mkdir log 
: 1483475543:0;mv *.log log/
: 1483475561:0;mv start_time_est test1_start_time_est
: 1483475597:0;for i in `ls -l x* | grep -v '.log' | awk '{print $9}'`\
do \
 mysql -u meyouhealth  -P 9941 -h 127.0.0.1 -D walkadoo_production -A -vvv < ${i} > wd-${i}-all-utf8mb4-idx-create-timing.log  2>&1 & \
done 
: 1483475618:0;more wd-xac-all-utf8mb4-idx-create-timing.log
: 1483475643:0;ls -l x* | grep -v log 
: 1483475647:0;ls -l x* | grep -v log  | wc -l 
: 1483478045:0;grep admin_users;
: 1483478052:0;grep admin_users *.log
: 1483478072:0;grep admin x*
: 1483478093:0;grep admin_users x*
: 1483478105:0;grep derbies x*
: 1483478194:0;more wd-create-idx-all.sql | grep derbies
: 1483478339:0;ls -l *solo*
: 1483478345:0;more db-solo5-wd-schema-scripting.sql
: 1483478371:0;cp db-solo5-wd-schema-scripting.sql db-solo5-wd-schema-scripting.sql.original
: 1483478391:0;cp db-solo5-wd-schema-scripting.sql db-solo5-wd-schema-scripting-modded-utf8mb4.sql
: 1483478396:0;vi db-solo5-wd-schema-scripting-modded-utf8mb4.sql
: 1483478525:0;jobs | wc -l 
: 1483479022:0;grep CONSTRAINT db-solo5-wd-schema-scripting.sql
: 1483479063:0;vi wd-create-constraints.sql 
: 1483479152:0;mysql -u meyouhealth -p -P 9941 -h 127.0.0.1 -vvv -D walkadoo_production -A < wd-create-constraints.sql > dms-cdc-add-constraints-after-indexes.log  
: 1483479213:0;more dms-cdc-add-constraints-after-indexes.log
: 1483479242:0;mysql -u meyouhealth -p -P 9941 -h 127.0.0.1 -vvv -D walkadoo_production -A < wd-create-constraints.sql > dms-cdc-add-constraints-after-indexes-rest.log  
: 1483492220:0;more dms-cdc-add-constraints-after-indexes-rest.log
: 1483492234:0;vi wd-create-constraints.sql
: 1483492322:0;mysql -u meyouhealth -p -P 9941 -h 127.0.0.1 -vvv -D walkadoo_production -A < wd-create-constraints.sql > dms-cdc-add-constraints-after-indexes-rest2.log
: 1483494584:0;more dms-cdc-add-constraints-after-indexes-rest2.log
: 1483494638:0;mysql -u meyouhealth -p -P 9940 -h 127.0.0.1 -vvv -D walkadoo_production -A
: 1483544013:0;grep user_cards wd-x*.log
: 1483544023:0;ls -ltr4a
: 1483544069:0;more state-of-db-after-constraints-utf8mb4-col-varchar-chgs-and-dms-cdc-changes.log
: 1483544182:0;more wd_reset_new_cp_auto_increment_for_import.sql
: 1483544484:0;grep -i id wd-varchar-columns-used-in-indexes.txt
: 1483544497:0;more wd-varchar-columns-used-in-indexes.txt
: 1483544816:0;vi select-max-id-for-auto-increment.log
: 1483544859:0;grep select select-max-id-for-auto-increment.log > wd-select-max-id.sql 
: 1483544883:0;grep SELECT select-max-id-for-auto-increment.log
: 1483544892:0;grep SELECT select-max-id-for-auto-increment.log | sed s/\|//g 
: 1483544899:0;grep SELECT select-max-id-for-auto-increment.log | sed s/\|//g  > wd-select-max-id.sql
: 1483544903:0;vi wd-select-max-id.sql
: 1483544942:0;mysql -u meyouhealth -p -P 9931 -h 127.0.0.1 -vvv -D walkadoo_production -A < wd-select-max-id.sql > wd-select-max-id.log 
: 1483544988:0;mysql -u meyouhealth -p -P 9931 -h 127.0.0.1 -s -D walkadoo_production -A < wd-select-max-id.sql > wd-select-max-id.log 
: 1483544999:0;more wd-select-max-id.log
: 1483545341:0;mysql -u meyouhealth -p -P 9931 -h 127.0.0.1 -s -D walkadoo_production -A < wd-select-max-id.sql > wd-select-max-id-delta.log
: 1483545359:0;diff -u wd-select-max-id-delta.log wd-select-max-id.log
: 1483545391:0;cp wd_reset_new_cp_auto_increment_for_import.sql frame-auto-inc
: 1483545394:0;vi frame-auto-inc
: 1483545580:0;paste frame-auto-inc wd-select-max-id-delta.logÂƒ€> auto-increment-set-1059AM-010417.sql 
: 1483545594:0;paste frame-auto-inc wd-select-max-id-delta.log
: 1483545599:0;paste frame-auto-inc wd-select-max-id-delta.log >> test
: 1483545612:0;mv test auto-increment-set-1059AM-010417.sql
: 1483545615:0;vi auto-increment-set-1059AM-010417.sql
: 1483545660:0;head wd-select-max-id.sql
: 1483545685:0;cat wd-select-max-id.sql | awk '{print $3}'
: 1483545689:0;cat wd-select-max-id.sql | awk '{print $4}'
: 1483545697:0;cat wd-select-max-id.sql | awk '{print $4}' > select1 
: 1483545708:0;cat auto-increment-set-1059AM-010417.sql
: 1483545721:0;cat auto-increment-set-1059AM-010417.sql | awk '{print $2}'
: 1483545723:0;cat auto-increment-set-1059AM-010417.sql | awk '{print $3}'
: 1483545730:0;cat auto-increment-set-1059AM-010417.sql | awk '{print $3}' > paste1 
: 1483545747:0;vi select1
: 1483545756:0;vi paste1
: 1483545763:0;diff -u select1 paste1
: 1483545783:0;rm auto-increment-set-1059AM-010417.sql
: 1483545800:0;more past1
: 1483545804:0;more paste1
: 1483545812:0;rm paste1
: 1483545844:0;more select-max-id-for-auto-increment.log
: 1483545875:0;ls -l *auto*
: 1483545880:0;head wd_reset_new_cp_auto_increment_for_import.sql
: 1483545917:0;cat select1 | awk '{print "ALTER TABLE ", $1}'
: 1483545944:0;cat select1 | awk '{print "ALTER TABLE ", $1, " AUTO_INCREMENT  = "}'
: 1483545979:0;cat select1 | awk '{print "ALTER TABLE ", $1, " AUTO_INCREMENT  = "}' > auto-inc-by-table
: 1483545986:0;more wd-select-max-id-delta.log
: 1483545995:0;paste auto-inc-by-table wd-select-max-id-delta.log
: 1483546015:0;mysql -u meyouhealth -p -P 9931 -h 127.0.0.1 -s -D walkadoo_production -A < wd-select-max-id.sql > wd-select-max-id-final-check.log
: 1483546039:0;mysql -u meyouhealth -p -P 9931 -h 127.0.0.1 -D walkadoo_production -A < wd-select-max-id.sql > wd-select-max-id-final-check.log
: 1483546053:0;more wd-select-max-id-final-check.log
: 1483546069:0;mysql -u meyouhealth -p -P 9931 -h 127.0.0.1 -D walkadoo_production -A -s < wd-select-max-id.sql > wd-select-max-id-final-check.log
: 1483546089:0;diff -u wd-select-max-id-delta.log wd-select-max-id-final-check.log
: 1483546097:0;more auto-inc-by-table
: 1483546113:0;paste auto-inc-by-table wd-select-max-id-final-check.log
: 1483546133:0;paste auto-inc-by-table wd-select-max-id-final-check.log >> wd-auto-inc-by-table-010417-1108AM.sql 
: 1483546140:0;vi wd-auto-inc-by-table-010417-1108AM.sql
: 1483546173:0;mysql -u meyouhealth -p -P 9931 -h 127.0.0.1 -D walkadoo_production -A 
: 1483546673:0;cat wd-set-autoincrement-from-what-is-in-innodb.sql
: 1483546691:0;cat wd-set-autoincrement-from-what-is-in-innodb.sql | sed s/\|//g | grep ALTER
: 1483546704:0;cat wd-set-autoincrement-from-what-is-in-innodb.sql | sed s/\|//g | grep ALTER > wd-alter-tbl-set-auto-inc.sql 
: 1483546709:0;vi wd-alter-tbl-set-auto-inc.sql
: 1483546747:0;mysql -u meyouhealth -p -P 9931 -h 127.0.0.1 -D walkadoo_production -A -vvv < wd-alter-tbl-set-auto-inc.sql > wd-alter-tbl-set-auto-inc.log 
: 1483558273:0;pip install --upgrade pip
: 1483559264:0;mv /Users/heidischmidt/deis /Users/heidischmidt/deis-v2.8.0
: 1483559286:0;cd /Users/heidischmidt
: 1483559446:0;brew install kubernetes-helm
: 1483559505:0;helm upgrade deis/workflow
: 1483559668:0;rm /usr/local/bin/deis
: 1483559704:0;ls -l /usr/local/bin/deis
: 1483560011:0;disown %1
: 1483561207:0;mysql -u meyouhealth -p -P 9931 -h 127.0.0.1 -vvv -D walkadoo_production -A
: 1483577824:0;tail wd-alter-tbl-set-auto-inc.log
: 1483578740:0;vi comparison-cdc-322pm
: 1483648730:0;sl -ltra
: 1483648764:0;cd wd-utf8-alter-47
: 1483648780:0;cd conversion
: 1483648781:0;grep provider_id *
: 1483648824:0;grep provider_id *.sql
: 1483729545:0;grep ALTER ucs2_all_alters.log
: 1483729553:0;grep ALTER ucs2_all_alters.log | sed s/\|//g
: 1483729580:0;grep ALTER ucs2_all_alters.log | sed s/\|//g > wd-alters-all-ucs2-to-utf8mb4.sql 
: 1483729585:0;vi wd-alters-all-ucs2-to-utf8mb4.sql
: 1483729625:0;mkdir wd-dms-cdc-chgs
: 1483729635:0;cp wd-alters-all-ucs2-to-utf8mb4.sql wd-dms-cdc-chgs/
: 1483729647:0;history | grep split
: 1483729669:0;split -l 1 wd-alters-all-ucs2-to-utf8mb4.sql
: 1483729731:0;for i in `ls -l x* | grep -v '.log' | awk '{print $9}'`\
do \
 mysql -u meyouhealth  -P 9930 -h 127.0.0.1 -D walkadoo_production -A -vvv < ${i} > wd-${i}-all-ucs2-to-utf8mb4-alter-191-timing.log  2>&1 & \
done 
: 1483737424:0;mysql -u meyouhealth -p -P 9930 -h 127.0.0.1 -vvv -D walkadoo_production -A
: 1483756845:0;ssh -A -L 9949:127.0.0.1:9949 ubuntu@54.152.155.24 ssh -L 9949:walkadoo-production2-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1483756886:0;ssh -A -L 9919:127.0.0.1:9919 ubuntu@54.152.155.24 ssh -L 9919:walkadoo-production2-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1483756931:0;mysql -u meyouhealth -p -P 9919 -h 127.0.0.1 -vvv -D walkadoo_production -A
: 1483757030:0;ssh -A -L 9909:127.0.0.1:9909 ubuntu@54.152.155.24 ssh -L 9909:wd-utf8mb4-dms-test2.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1483760779:0;mysql -u meyouhealth -p -P 9909 -h 127.0.0.1 -vvv -D walkadoo_production -A
: 1483760795:0;mysql -u meyouhealth  -P 9930 -h 127.0.0.1 -vvv -D walkadoo_production -A
: 1483760958:0;while true ; do echo "$(date)" ; jobs | wc -l  ; sleep 300;  done
: 1483761194:0;while true ; do echo "$(date)" ; jobs | wc -l  ; sleep 30;  done
: 1483982173:0;ssh -A -L 9908:127.0.0.1:9908 ubuntu@54.152.155.24 ssh -L 9908:wd-utf8mb4-dms-test2.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1483982187:0;mysql -u meyouhealth -p -P 9908 -h 127.0.0.1 -vvv -D walkadoo_production -A
: 1483982632:0;mysql -u walkadoo_prd_app -p  -P 9929 -h 127.0.0.1 -vvv -D walkadoo_production -A
: 1483982693:0;mysql -u meyouhealth -P 9929 -h 127.0.0.1 -vvv -D walkadoo_production -A
: 1483997747:0;mysql -u meyouhealth -P 9928 -h 127.0.0.1 -vvv -D walkadoo_production -A
: 1484060814:0;ssh -A -L 9907:127.0.0.1:9907 ubuntu@54.152.155.24 ssh -L 9907:wd-utf8mb4-dms-test2.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1484060853:0;mysql -u meyouhealth -P 9927 -h 127.0.0.1 -vvv -D walkadoo_production -A
: 1484065990:0;history |Âƒ€grep awscli | grep dms
: 1484065995:0;history |Âƒ€grep awscli 
: 1484066007:0;awscli --help
: 1484066015:0;history | grep "aws"
: 1484066026:0;history | grep "aws dms"
: 1484074796:0;aws version
: 1484074842:0;brew list aws
: 1484074851:0;brew list pip
: 1484074965:0;pip --version
: 1484075423:0;locate dms
: 1484075438:0;ls -l /usr/local/lib/python2.7/site-packages/awscli/examples/dms
: 1484075454:0;more /usr/local/lib/python2.7/site-packages/awscli/examples/dms/create-endpoint.rst
: 1484076294:0;ls -lo /usr/local/lib/python2.7/site-packages/awscli/
: 1484076302:0;ls -l .aws
: 1484076306:0;ls -l .aws/config
: 1484076313:0;ls -l .aws/models
: 1484076317:0;ls -l .aws/models/dms
: 1484076323:0;ls -l .aws/models/dms/2015-12-25/
: 1484076337:0;cat .aws/models/dms/2015-12-25/dms-cli.json
: 1484078713:0;java --version
: 1484078718:0;java -V
: 1484078721:0;java -version
: 1484078761:0;locate postgres | grep jar
: 1484078774:0;cd /usr/local/bin/jdbc-drivers
: 1484078825:0;cp ~/Downloads/postgresql-9.4.1212.jar .
: 1484078845:0;java --help
: 1484078848:0;java -help
: 1484078870:0;java -jar postgresql-9.4.1212.jar -tvf
: 1484078919:0;echo $JAVA_HOME
: 1484078932:0;which java
: 1484078952:0;ls -l /usr/bin | grep java
: 1484080090:0;cp ~/Downloads/mysql-connector-java-5.1.40/mysql-connector-java-5.1.40-bin.jar .
: 1484080460:0;ls -ls | grep deis
: 1484080502:0;which kubectl 
: 1484080509:0;ls -l /usr/local/bin/kubectl
: 1484080520:0;ls -l /usr/local/bin/hel*
: 1484149299:0;grep -i security *.tf
: 1484149434:0;teraform --version
: 1484149449:0;teraform -version
: 1484149456:0;terraform -version
: 1484149518:0;more walkadoo.tf
: 1484149698:0;cd ../iris
: 1484149714:0;grep ssl_certificate
: 1484149717:0;grep ssl_certificate *
: 1484151926:0;cp environments/production/iris/elasticache.tf environments/production/iris/elasticache.tf.old
: 1484151932:0;git checkout environments/production/iris/elasticache.tf
: 1484151998:0;git status .
: 1484152224:0;cd staging/iris
: 1484152245:0;diff variables.tf ../../production/iris/variables.tf
: 1484152271:0;grep ssl_cert *.tf
: 1484152284:0;grep ssl_cert ../../production/iris/*.tf
: 1484152572:0;git checkout environments/staging/iris/terraform.tfstate
: 1484152594:0;cd environments/staging/iris
: 1484152693:0;grep ssl_cert
: 1484152696:0;grep ssl_cert *
: 1484152707:0;more shipit.tf
: 1484152717:0;vi shipit.tf
: 1484152796:0;grep sec *.tf
: 1484152864:0;cp ec2.tf ../wellbeingid/iris_ec2.tf
: 1484152878:0;diff iris_ec2.tf ec2.tf
: 1484152902:0;mv ec2.tf wbid-ec2-old.tf
: 1484152909:0;cp iris_ec2.tf ec2.tf
: 1484152935:0;more wbid-ec2-old.tf
: 1484152951:0;git diff .
: 1484152999:0;rm iris_ec2.tf
: 1484153006:0;rm wbid-ec2-old.tf
: 1484153095:0;vi ~/Documents/wbid/old-ec2-terraform-security-group-non-vpc.txt
: 1484153122:0;history | grep "test.plan"
: 1484153183:0;mv ec2.tf ec2.tf.new
: 1484153204:0;diff ec2.tf ec2.tf.new
: 1484153363:0;grep publicly variables.tf
: 1484153417:0;grep aws_db_instance *.tf
: 1484160408:0;cd ../iris/
: 1484160421:0;cd ../hello200
: 1484160425:0;grep -i subnet *.tf
: 1484160557:0;mkdir shared-postgres-staging
: 1484160590:0;cp rds.tf ../shared-postgres-staging
: 1484160701:0;grep access_key *
: 1484160720:0;grep access_key ../shared-mysql-staging/*.tf
: 1484160729:0;more ../shared-mysql-staging/provider.tf
: 1484160734:0;cp ../shared-mysql-staging/provider.tf .
: 1484160764:0;cd shared-mysql-staging
: 1484160770:0;grep access *.tf
: 1484160814:0;cp variables.tf ../shared-postgres-staging
: 1484160839:0;cd ../shared-mysql-staging
: 1484160879:0;cp terraform.tfvars ../shared-postgres-staging
: 1484160890:0;cd ../shared-postgres-staging
: 1484160948:0;git add shared-postgres-staging
: 1484161036:0;cd shared-postgres-staging
: 1484161044:0;;tfp
: 1484162595:0;ssh -A -L 9906:127.0.0.1:9906 ubuntu@54.86.62.65 ssh -L 9906:wbid-staging.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:5432 -N heidischmidt@10.150.68.133
: 1484162727:0;ssh -A -L 9906:127.0.0.1:9906 ubuntu@54.86.62.65 ssh -L 9906:wbid-staging.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:5432 -N heidischmidt@54.204.40.159
: 1484163067:0;vi /Users/heidischmidt/.ssh/known_hosts
: 1484163403:0;cd staging/shared-postgres-staging
: 1484163622:0;git add terraform.tfstate
: 1484163697:0;psql -E -h 127.0.0.1 -p 9906 -U meyouhealth  --dbname=postgres "TvGqrXY7Dli0Db
: 1484163711:0;psql -E -h 127.0.0.1 -p 9906 -U meyouhealth  --dbname=postgres
: 1484163803:0;psql -E -h 127.0.0.1 -p 9906 -U meyouhealth  --dbname=wellbeingid_staging
: 1484236962:0;terraform plan -out=test.plan --target=aws_security_group.wbid_staging
: 1484237210:0;ssh -A -L 9906:wbid-staging.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:5432 ubuntu@54.204.40.159 -N
: 1484237223:0;psql -E -h 127.0.0.1 -p 9906 -U wellbeingid_staging_app  --dbname=wellbeingid_staging
: 1484237649:0;git checkout .
: 1484237889:0;cp ec2.tf ec2.tf.old
: 1484237978:0;more ../staging/wellbeingid/ec2.tf
: 1484237998:0;more ../../staging/wellbeingid/ec2.tf
: 1484238261:0;grep self ec2.tf.old
: 1484238278:0;more ec2.tf.old
: 1484238349:0;rm terraform.tfstate.backup
: 1484238375:0;mv ec2.tf.old ~/Documents/wbid/terraform-ec2.tf-old-work
: 1484238426:0;grep aws_db_subnet *.tf
: 1484238478:0;git add wellbeingid
: 1484239729:0;mv ec2.tf ec2.tf.broken
: 1484239740:0;cp ../../staging/wellbeingid/ec2.tf .
: 1484239951:0;git add ec2,tf
: 1484239954:0;git add ec2.tf
: 1484241070:0;vi ec2.tfr
: 1484241109:0;cp ../walkadoo/variables.tf variables.tf.wd
: 1484241117:0;diff variables.tf.wd variables.tf
: 1484241157:0;mv variables.tf variables.tf.wbid.stg
: 1484241164:0;cp variables.tf.wd variables.tf
: 1484241327:0;grep arn ../*/variables.tf
: 1484241391:0;git add variables.tf
: 1484241906:0;terraform plan -out=test.plan --target=aws_subnet.wellbeingid_production_rds_us_east_1b
: 1484241932:0;terraform plan -out=test.plan --target=aws_subnet.wellbeingid_production_rds_us_east_1c
: 1484244773:0;grep sg-8e4006f5 *.tf
: 1484244777:0;vi ec2.tf
: 1484244944:0;git checkout ec2.tf
: 1484245077:0;grep 90 *.tf
: 1484245535:0;mkdir shared-postgres-prod
: 1484245549:0;rm rds.tf
: 1484245558:0;git checkout rds.tf
: 1484245564:0;git rm rds.tf
: 1484245623:0;rm ec2.tf.broken
: 1484245631:0;rm variables.tf.wbid.stg
: 1484245654:0;cp ../walkadoo/terraform.tfvars .
: 1484245661:0;cp ../walkadoo/variables.tf .
: 1484245683:0;cp ../../staging/shared-postgres-staging/ec2.tf .
: 1484245694:0;ls -l ../../staging/shared-postgres-staging/
: 1484245707:0;more ../../staging/shared-postgres-staging/rds.tf
: 1484245724:0;cp  ../../staging/shared-postgres-staging/rds.tf . 
: 1484245812:0;vi variables.tf
: 1484245848:0;vi terraform.tfvars
: 1484245959:0;git add shared-postgres-prod/
: 1484246012:0;cd shared-postgres-prod
: 1484246031:0;grep provider.aws.region ../*/*.tf
: 1484246040:0;grep region ../*/*.tf
: 1484246061:0;ls -l ../*/provider.tf
: 1484246066:0;more ../walkadoo/provider.tf
: 1484246075:0;cp ../walkadoo/provider.tf .
: 1484246078:0;git add provider.tf
: 1484246584:0;terraform plan -out=test.plan --target=
: 1484246602:0;terraform plan -out=test.plan --target=aws_subnet.shared_postgres_production_rds_us_east_1b
: 1484246637:0;terraform plan -out=test.plan --target=aws_subnet.shared_postgres_production_rds_us_east_1c
: 1484246659:0;terraform plan -out=test.plan 
: 1484246918:0;ls -l ../../staging/shared-postgres-staging
: 1484246935:0;grep security ../../staging/shared-postgres-staging/*.tf
: 1484246947:0;grep security ../../staging/wellbeingid/*.tf
: 1484247596:0;cd ../shared-postgres-prod
: 1484247602:0;more ec2
: 1484249772:0;history | tail -20 
: 1484250825:0;find . -name "*" -print -exec grep -i snapshot {} \;
: 1484250836:0;find . -name "*.tf" -print -exec grep -i snapshot {} \;
: 1484250874:0;more ./production/reporting-dashboard/reporting-dashboard.tf
: 1484250906:0;cd production/
: 1484251101:0;grep subnet *.tf
: 1484251197:0;more ../shared-postgres-prod
: 1484251203:0;more ../shared-postgres-prod/*.tf
: 1484251271:0;more ../shared-postgres-prod/rds.tf
: 1484251592:0;rm variables.tf.wd
: 1484251640:0;echo "figure out the security group for wellbeingid prod and especially subnet group to use shared prod postgres"
: 1484325691:0;more ../walkadoo/rds.tf
: 1484325846:0;cat ../walkadoo/rds.tf >> rds.tf
: 1484584453:0;cd rds.tf
: 1484594988:0;grep security *.tf
: 1484600115:0;diff 1 rds.tf
: 1484600136:0;grep aws_security_group *.tf
: 1484600148:0;cat ec2.tf
: 1484600312:0;grep aws_db_subnet_group *.tf
: 1484600707:0;grep wbid_database_password *.tf
: 1484600856:0;terraform plan -out=test.plan --target=aws_db_subnet_group.wellbeingid_production
: 1484600950:0;terraform plan -out=test.plan --target=aws_security_group.wbid_production
: 1484600988:0;terraform plan -out=test.plan --target=aws_db_subnet_group.wbid_production
: 1484665678:0;terraform plan -out=test.plan --target aws_db_instance.wbid_production
: 1484667973:0;ssh -A -L 9992:127.0.0.1:9992 ubuntu@54.152.155.24 ssh -L 9992:wbid-production-tf-test.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:5432 -N heidischmidt@54.225.238.195 
: 1484669245:0;cd myh-terraform/environments/staging
: 1484669467:0;ssh -A -L 9905:wbid-production-tf-test.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:5432 ubuntu@54.225.238.195 -N
: 1484669538:0;history | grep psql | tail
: 1484669606:0;psql -E -h 127.0.0.1 -p 9905 -U wellbeingid_production_app --dbname=wellbeingid_production 
: 1484671879:0;ssh ec2-54-234-3-109.compute-1.amazonaws.com
: 1484672159:0;ssh 172.24.60.122
: 1484672753:0;ls -trla
: 1484680585:0;aws subnet help
: 1484680957:0;aws rds describe-db-instances --db-instance-identifier=wbid-production-sg-test.cduvhvvpw6oo.us-east-1.rds.amazonaws.com
: 1484680973:0;aws rds describe-db-instances --db-instance-identifier wbid-production-sg-test.cduvhvvpw6oo.us-east-1.rds.amazonaws.com
: 1484680982:0;aws rds describe-db-instances help 
: 1484681004:0;aws rds describe-db-instances --db-instance-identifier wbid-production-sg-test
: 1484682254:0;grep -i parame ../hello200/*.tf
: 1484682259:0;grep -i param ../hello200/*.tf
: 1484682269:0;grep -i param ../*/*.tf
: 1484682291:0;grep -i param ../../staging/*/*.tf
: 1484682317:0;grep -i parameter_group_name ../../staging/*/*.tf
: 1484682346:0;ls -l staging/*/*rds*.tf
: 1484682352:0;more staging/shared-postgres-staging/rds.tf
: 1484682573:0;terraform apply test.plan
: 1484682634:0;cat rds.tf
: 1484758034:0;cd .deis
: 1484758038:0;more client.json
: 1484758077:0;deis auth:login https://deis.myhstg.com --ssl-verify=true
: 1484758186:0;which deis
: 1484758200:0;cd /usr/local/bin
: 1484758205:0;ls -l | grep deis
: 1484758227:0;brew uninstall deis
: 1484758248:0;ls -ltra /usr/local/bin/ | grep deis
: 1484758262:0;curl -sSL http://deis.io/deis-cli/install-v2.sh | bash
: 1484758272:0;./deis --version
: 1484758901:0;kubectl --namespace help
: 1484759104:0;history | grep deis | grep meyouhealth
: 1484759122:0;deis logout https://deis.meyouhealth.com
: 1484759139:0;deis auth:logout https://deis.meyouhealth.com
: 1484759148:0;deis auth:logout --help
: 1484759153:0;deis auth:logout
: 1484759202:0;kubectl context --help
: 1484759210:0;kubectl use-context --help
: 1484759296:0;deis run -a iris-staging -- bundle exec rake -T
: 1484759341:0;deis run --help
: 1484759362:0;deis run --app iris-staging -- bundle exec rake -T
: 1484760295:0;deis --username=scott.gardner --password=Sec23et!
: 1484760356:0;history | grep deis.myhstg.com
: 1484760376:0;deis auth:register https://deis.myhstg.com --username=scott.gardner --password=Sec23et!
: 1484760421:0;history | grep login
: 1484760501:0;deis users list
: 1484760610:0;cat .zshrc
: 1484760800:0;export KOPS_STATE_STORE=s3://myh-kops
: 1484760825:0;deis apps | awk '{print $1}'
: 1484760838:0;deis apps | grep staging | awk '{print $1}' 
: 1484760853:0;for i in `deis apps | grep staging | awk '{print $1}' `
: 1484760881:0;deis perms:create scott.gardner --app hello200-staging 
: 1484760898:0;for i in `deis apps | grep staging | awk '{print $1}' `\
do \
deis perms:create scott.gardner --app $i\
done
: 1484761355:0;more .kube/conifg-original
: 1484761525:0;history | grep aws | grep register
: 1484761533:0;history | grep aws | grep config
: 1484761541:0;aws config --help
: 1484761589:0;history | grep aws-cli
: 1484762284:0;aws --version
: 1484762296:0;aws configure --help
: 1484762299:0;aws configure 
: 1484762894:0;aws iam list-access-keys --help
: 1484762904:0;aws iam list-access-keys help
: 1484762919:0;aws iam list-access-keys --user-name scott.gardner
: 1484763603:0;more .ssh/config
: 1484763618:0;more .ssh/config | grep 52.149 
: 1484763635:0;more .ssh/config | grep 54.159
: 1484764137:0;deis auth:logout 
: 1484764160:0;;dprdl
: 1484764198:0;history | grep kube 
: 1484764606:0;deis auth:register https://deis.meyouhealth.com --username=scott.gardner --password=Pl4yful1
: 1484765347:0;kubectl config use-context k8s.meyouhealth.com 
: 1484765357:0;for i in `deis apps | grep production | awk '{print $1}' `\
do \
deis perms:create scott.gardner --app $i\
done
: 1484767257:0;kube config current-context
: 1484769000:0;which openvpn
: 1484796743:0;ls -ltr wellbeingid
: 1484796750:0;more wellbeingid/rds.tf
: 1484796778:0;cp wellbeingid/rds.tf walkadoo/rds-tf.tf
: 1484797112:0;grep ssd ../*/rds.tf
: 1484797127:0;grep disk ../*/rds.tf
: 1484797135:0;grep general ../*/rds.tf
: 1484797157:0;grep instance ../*/rds.tf
: 1484797181:0;grep storage ../*/rds.tf
: 1484797196:0;more ../concierge/rds.tf
: 1484797308:0;rm rds-tf.tf
: 1484797364:0;grep wd-production-memcached
: 1484797369:0;grep wd-production-memcached *
: 1484797476:0;vi elasticache.tf
: 1484797658:0;grep 22222a5a terraform.tfstate
: 1484797665:0;vi terraform.tfstate
: 1484798869:0;cd production/wellbeingid
: 1484798880:0;cd environments/production/wellbeingid
: 1484798932:0;more rds.tf
: 1484799326:0;terraform plan -out=test.plan --target=aws_db_instance.walkadoo_production-utf8
: 1484799473:0;terraform plan -out=test.plan --target=aws_db_instance.wbid_production
: 1484799621:0;terraform plan -out=test.plan --target=aws_db_instance.walkadoo_production3
: 1484799637:0;terraform apply test.plan 
: 1484854256:0;brew list | grep ku
: 1484854288:0;brew list kubernetes-helm
: 1484855338:0;aws s3 ls myh-k0ps
: 1484855604:0;history
: 1484856089:0;kubectl context
: 1484856102:0;kubectl --context 
: 1484935412:0;git add rds.tf
: 1484935429:0;git add terraform.tfstate 
: 1484935436:0;git status | head
: 1484935442:0;;tfg
: 1484935553:0;vi rds.tf
: 1484941117:0;history |grep ssh
: 1484943385:0;ssh heidischmidt@10.0.14.114
: 1484947832:0;ssh -A -L 9910:127.0.0.1:9910 ubuntu@54.152.155.24 ssh -L 9910:wd-prd-to-wd-prd3-utf8mb4.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1484947850:0;mysql -u meyouhealth -P 9910 -h 127.0.0.1 -vvv -D walkadoo_production -A
: 1484947888:0;mysql -u meyouhealth -P 9910 -p -h 127.0.0.1 -vvv -D walkadoo_production -A
: 1484949853:0;ssh -A -L 9911:walkadoo-production3.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 admin@54.237.232.212 -N
: 1484949947:0;mysql -u meyouhealth -P 9911 -p  -vvv -D walkadoo_production -A
: 1484950093:0;mysql -u meyouhealth -P 9911 -p -h 127.0.0.1 -vvv -D walkadoo_production -A
: 1485180360:0;git status | moer
: 1485180363:0;git status | more
: 1485180763:0;cd wdoo
: 1485180779:0;ls -l wd-dms-cdc-chgs
: 1485180790:0;more xda
: 1485180845:0;ls -l wd-create-idx
: 1485180856:0;more wd-create-idx/xg*
: 1485180868:0;more 1
: 1485180889:0;mv 1 table-statistics-dms-010417.loh
: 1485180971:0;grep convert *.sql
: 1485181018:0;mkdir wd-dms-from-scratch
: 1485181031:0;mkdir old
: 1485181038:0;mv wd_reset_new_cp_auto_increment_for_import.sql old/
: 1485181048:0;mv wd_proc_analyse_columns.sql old/
: 1485181055:0;mv wd_inserts_from_old_into_new.sql old/
: 1485181061:0;mv wd_inserts_from_old_into_new_bare.sql old/
: 1485181122:0;cp wd-alters-all-ucs2-to-utf8mb4.sql wd-dms-from-scratch
: 1485181131:0;more broken_wd_core_47_to_change.sql
: 1485181136:0;mv broken_wd_core_47_to_change.sql old/
: 1485181157:0;cp check_charset_collation.sql wd-dms-from-scratch
: 1485181176:0;cp convert_all_tables.sql wd-dms-from-scratch
: 1485181187:0;ls -l *sql 
: 1485181212:0;more wd-alter-db.sql
: 1485181220:0;cp wd-alter-db.sql wd-dms-from-scratch
: 1485181228:0;ls logs
: 1485181263:0;more db-research-objects-to-create-orig.sql
: 1485181282:0;mv db-research-objects-to-create-orig.sql old/
: 1485181285:0;ls -l *sql
: 1485181291:0;more events.sql
: 1485181308:0;mv events.sql old/research_events.sql 
: 1485181316:0;ls *.sql 
: 1485182005:0;more wd-select-max-id.sql
: 1485182013:0;mv wd-select-max-id.sql old/
: 1485182015:0;ls *.sql
: 1485182025:0;more test-mysqldump-schema-wd-DMS-only-no-idx.sql
: 1485182042:0;mv test-mysqldump-schema-wd-DMS-only-no-idx.sql old/
: 1485182067:0;more wd-alter-tbl-set-auto-inc.sql
: 1485182084:0;cp wd-set-autoincrement-from-what-is-in-innodb.sql wd-dms-from-scratch
: 1485182095:0;more preTableInfo*
: 1485182110:0;ls *list
: 1485182116:0;mv *list old/
: 1485182135:0;mv mysqldump-schema* old/
: 1485182149:0;more script-for-WD-startup.sh
: 1485182157:0;more script-for-WD-startup.sh-OOW
: 1485182163:0;mv script-for-WD-startup.sh-OOW old/
: 1485182200:0;ls *.sh
: 1485182208:0;mv Step*.sh old/
: 1485182216:0;more wd-auto-inc-by-table-010417-1108AM.sql
: 1485182221:0;mv wd-auto-inc-by-table-010417-1108AM.sql old/
: 1485182233:0;more verification_post_assessment_pre_sql.sql
: 1485182259:0;more select1
: 1485182264:0;mv select1 old/
: 1485182276:0;more wd-deploy-action-tracking.sh
: 1485182289:0;more dms-cdc-reset-itself-to-ucs2-while-recreating-it-mysqldump-schema-wd-20170103.mysql
: 1485182301:0;mv dms-cdc-reset-itself-to-ucs2-while-recreating-it-mysqldump-schema-wd-20170103.mysql old/
: 1485182312:0;mv wd-rename-current-to-old.sql old/
: 1485182323:0;mv auto-inc-by-table old/
: 1485182327:0;more frame-auto-inc
: 1485182333:0;mv frame-auto-inc old/
: 1485182359:0;more comparison-cdc-322pm
: 1485182365:0;mv comparison-cdc-322pm old/
: 1485182375:0;mv argh-master-status-again-re-dump old/
: 1485182380:0;mv shipit-wd-stg-from-github-chef-walkadoo-recipes-application-rb-vars.txt.old old/
: 1485182388:0;more stage
: 1485182395:0;rm stage
: 1485182397:0;more prod
: 1485182400:0;rm prod
: 1485182452:0;more DMS-CDC-working-schema-mysqldump-schema-wd-20170103.mysql
: 1485182461:0;mv DMS-CDC-working-schema-mysqldump-schema-wd-20170103.mysql old/
: 1485182470:0;more db-solo5-wd-schema-scripting-modded-utf8mb4.sql
: 1485182522:0;cp  db-solo5-wd-schema-scripting-modded-utf8mb4.sql wd-dms-from-scratch
: 1485182537:0;ls -l wd-dms-from-scratch
: 1485183455:0;cd ../h200
: 1485183501:0;more augh_s3_bucket_iam_policy_hello_nurse
: 1485183505:0;mv augh_s3_bucket_iam_policy_hello_nurse logs/
: 1485183514:0;more hello200_new_prod_users.sql
: 1485183532:0;mow aws-elasticache-create-snapshot-new-staging.sh
: 1485183539:0;more aws-elasticache-create-snapshot-new-staging.sh
: 1485183914:0;mkdir postgres
: 1485183936:0;more concierge_prod_stats_sleuthing.log
: 1485183942:0;rm concierge_prod_stats_sleuthing.log
: 1485183952:0;vi postgres_db_size.sql 
: 1485184056:0;vi check-last-vacuum-stats.sql 
: 1485184142:0;vi table-checks.sql 
: 1485184395:0;vi iris-prod-tbl-messages-checks.log
: 1485184507:0;mv rds_ext_9* logs/
: 1485184522:0;ls -l pgdata_wbid
: 1485184527:0;rm -rf pgdata_wbid
: 1485184538:0;mv *.txt logs/
: 1485184559:0;more pg_dump_wbid-original-data-dump-inserts.sql
: 1485184590:0;more pg_dump_ex.sql
: 1485194622:0;ssh -A -L 9913:127.0.0.1:9913 ubuntu@54.152.155.24 ssh -L 9913:wd-prd-to-wd-prd3-utf8mb4.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1485196766:0;more postgres_db_size.sql
: 1485197545:0;ssh 54.204.40.159
: 1485197803:0;ubuntu-cloud-images
: 1485199321:0;cd .ssh.
: 1485199335:0;more id_rsa.pub
: 1485199369:0;ssh admin@172.24.33.24
: 1485199389:0;ssh ubuntu@172.24.33.24
: 1485200892:0;ssh ubuntu@172.16.1.134
: 1485272599:0;history | grep ssh | grep wellbeing
: 1485278653:0;ssh -A -L 9800:wellbeingid-production.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:5432 heidischmidt@54.225.238.195 -N
: 1485278709:0;ls -l wbid
: 1485279014:0;vi table-checks.sql
: 1485279159:0;more typical_session_activity.log
: 1485279308:0;more check-last-vacuum-stats.sql
: 1485279312:0;rm check-last-vacuum-stats.sql
: 1485279318:0;more *.log
: 1485279333:0;more stats-check-all.log
: 1485279361:0;rm stats-check-all.log
: 1485279394:0;cp pg_stat_activity.sql pg_stat_activity.sql.orig
: 1485279408:0;psql -E -h 127.0.0.1 -p 9800 -U wellbeingid_production_app --dbname=wellbeingid_production  --file=pg_stat_activity.sql > pg_stat_activity.log 
: 1485279417:0;more pg_stat_activity.log
: 1485279431:0;more all-table-checks.log
: 1485279436:0;rm all-table-checks.log
: 1485279459:0;psql -E -h 127.0.0.1 -p 9800 -U wellbeingid_production_app --dbname=wellbeingid_production  --file=postgres_db_size.sql > postgres_db_size.log 
: 1485279491:0;psql -E -h 127.0.0.1 -p 9800 -U wellbeingid_production_app --dbname=wellbeingid_production  --file=table-checks.sql > table-checks.log 
: 1485279551:0;vi pg-stats-tables.sql
: 1485279619:0;vi pg-stats-tables-all.sql 
: 1485279834:0;psql -E -h 127.0.0.1 -p 9800 -U wellbeingid_production_app --dbname=wellbeingid_production  --file=pg-stats-tables-all.sql > pg-stats-tables-all.log 
: 1485279846:0;more pg-stats-tables-all.
: 1485279897:0;psql -E -h 127.0.0.1 -p 9800 -U wellbeingid_production_app --dbname=wellbeingid_production 
: 1485279942:0;more table-list
: 1485279974:0;vi table-list-in-schema.sql 
: 1485280021:0;psql -E -h 127.0.0.1 -p 9800 -U wellbeingid_production_app --dbname=wellbeingid_production --file=table-list-in-schema.sql > table-list-in-schema.log 
: 1485280029:0;more table-list-in-schema.log
: 1485280053:0;cat table-list-in-schema.log | awk -F "\|" '{print $2}'
: 1485280305:0;cat table-list-in-schema.log | awk -F "\|" '{print "ANALYZE TABLE ", $2, " ;"}' > analyze-table.sql  
: 1485280505:0;more postgres_db_size.log
: 1485282568:0;vi vacuum-freeze-info.sql 
: 1485282618:0;psql -E -h 127.0.0.1 -p 9800 -U wellbeingid_production_app --dbname=wellbeingid_production --file=vacuum-freeze-info.sql > vacuum-freeze-info.log 
: 1485282625:0;more vacuum-freeze-info.log
: 1485282656:0;more vacuum-freeze-info.log | grep 70534637
: 1485282660:0;more vacuum-freeze-info.log | grep 70534637 | wc -l 
: 1485282664:0;more vacuum-freeze-info.log | grep -v 70534637 | wc -l 
: 1485282671:0;wc -l vacuum-freeze-info.log
: 1485282698:0;cat vacuum-freeze-info.log | grep -v "pg_"
: 1485282709:0;cat vacuum-freeze-info.log | grep -v "pg_" | grep 70534637
: 1485282735:0;cat vacuum-freeze-info.log | grep -v "pg_"| wc -l 
: 1485282742:0;cat vacuum-freeze-info.log | grep -v "pg_" | grep 70534637 | wc -l 
: 1485286003:0;mv pg-stats-tables-all.log wbid-pg-stats-tables-all.log
: 1485286012:0;mv table-list-in-schema.log wbid-table-list-in-schema.log
: 1485286022:0;mv vacuum-freeze-info.log wbid-vacuum-freeze-info.log
: 1485286039:0;more table-checks.log
: 1485286045:0;mv table-checks.log wbid-table-checks.log
: 1485286056:0;mv postgres_db_size.log wbid-postgres_db_size.log
: 1485286075:0;mv pg_stat_activity.log wbid-pg_stat_activity.log
: 1485287165:0;vi analyze-table.sql
: 1485289271:0;vi pg-settings.sql 
: 1485289324:0;psql -E -h 127.0.0.1 -p 9800 -U wellbeingid_production_app --dbname=wellbeingid_production --file=pg-settings.sql > pg-settings.log 
: 1485289332:0;more pg-settings.log
: 1485289359:0;mv pg-settings.log wbid-9314-pg-settings.log 
: 1485309781:0;psql -E -h 127.0.0.1 -p 9800 -U wellbeingid_production_app --dbname=wellbeingid_production --file=pg-settings.sql > pg-settings2.log
: 1485309805:0;ssh -A -L 9700:wellbeingid-production.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:5432 heidischmidt@54.225.238.195 -N
: 1485309817:0;psql -E -h 127.0.0.1 -p 9700 -U wellbeingid_production_app --dbname=wellbeingid_production --file=pg-settings.sql > pg-settings2.log
: 1485309828:0;more pg-settings2.log
: 1485309886:0;export PASS="dYjyq3fHUWfEv3V"
: 1485309890:0;echo $PASS
: 1485309902:0;psql -E -h 127.0.0.1 -p 9700 --password=${PASS} -U wellbeingid_production_app --dbname=wellbeingid_production --file=pg-settings.sql > pg-set.log
: 1485309946:0;psql -E -h 127.0.0.1 -p 9700 -U wellbeingid_production_app --dbname=wellbeingid_production --file=pg-settings.sql > pg-set.log
: 1485309992:0;rm table-list
: 1485310005:0;rm pg-settings2.log pg-set.log
: 1485311404:0;more wbid-pg_stat_activity.log
: 1485311477:0;psql -E -h 127.0.0.1 -p 9700 -U wellbeingid_production_app --dbname=wellbeingid_production 
: 1485311508:0;cat pg_stat_activity.sql
: 1485311777:0;more pg-stats-activity.log
: 1485311873:0;cat pg-stats-tables-all.sql
: 1485335434:0;ls *.log
: 1485335515:0;for i in `ls -l | grep '.log' `\
do\
mv $i baseline-${i}\
done
: 1485335541:0;more 30
: 1485335546:0;rm 30
: 1485335594:0;mkdir pgdump
: 1485335602:0;mv pg_dump*.sql pgdump/
: 1485335615:0;mkdir pggrants
: 1485335627:0;mv pg_grants*.sql pggrants/
: 1485335660:0;mkdir sql
: 1485335667:0;mv pg_test_crypt.sql sql/
: 1485335675:0;mv postgres_*.sql sql/
: 1485335684:0;mv insight_pg_grants.sql sql/
: 1485335742:0;more table-list-in-schema.sql
: 1485335754:0;mv table-list-in-schema.sql sql/
: 1485335758:0;ls -ltrah *.sql
: 1485336076:0;ls -tlr
: 1485336112:0;tail pg-stats-activity.log
: 1485337971:0;shipit environment show wellbeingid-production
: 1485340268:0;ssh -A -L 9701:wellbeingid-production.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:5432 heidischmidt@54.225.238.195 -N
: 1485340275:0;psql -E -h 127.0.0.1 -p 9701 -U wellbeingid_production_app --dbname=wellbeingid_production
: 1485340519:0;psql -E -h 127.0.0.1 -p 9700 -U wellbeingid_production_app --dbname=wellbeingid_production --file=pg-settings.sql > after-reboot-middle-upgrade-pg-settings.log 
: 1485340529:0;psql -E -h 127.0.0.1 -p 9701 -U wellbeingid_production_app --dbname=wellbeingid_production --file=pg-settings.sql > after-reboot-middle-upgrade-pg-settings.log 
: 1485340538:0;more after-reboot-middle-upgrade-pg-settings.log
: 1485340557:0;psql -E -h 127.0.0.1 -p 9701 -U wellbeingid_production_app --dbname=wellbeingid_production --file=
: 1485340600:0;psql -E -h 127.0.0.1 -p 9701 -U wellbeingid_production_app --dbname=wellbeingid_production --file=pg-stats-tables-all.sql > after-reboot-middle-upgrade-pg-stats-all.log 
: 1485340608:0;more after-reboot-middle-upgrade-pg-stats-all.log
: 1485340843:0;psql -E -h 127.0.0.1 -p 9701 -U wellbeingid_production_app --dbname=wellbeingid_production 
: 1485340991:0;psql -E -h 127.0.0.1 -p 9701 -U wellbeingid_production_app --dbname=wellbeingid_production --file=pg-settings.sql > after-reboot-middle-upgrade-pg-settings2.log 
: 1485341271:0;psql -E -h 127.0.0.1 -p 9701 -U wellbeingid_production_app --dbname=wellbeingid_production --file=analyze-table.sql > analyze-table.log 
: 1485341339:0;psql -E -h 127.0.0.1 -p 9701 -U wellbeingid_production_app --dbname=wellbeingid_production --file=pg-stats-tables-all.sql > pg-stats-tables-all.log 
: 1485341350:0;more pg-stats-tables-all.log
: 1485341560:0;ssh -A -L 9702:wellbeingid-production.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:5432 heidischmidt@54.225.238.195 -N
: 1485341565:0;psql -E -h 127.0.0.1 -p 9702 -U wellbeingid_production_app --dbname=wellbeingid_production 
: 1485341639:0;psql -E -h 127.0.0.1 -p 9702 -U wellbeingid_production_app --dbname=wellbeingid_production --file=analyze-table.sql > analyze-table.log 
: 1485341767:0;psql -E -h 127.0.0.1 -p 9702 -U wellbeingid_production_app --dbname=wellbeingid_production --file=pg-stats-tables-all.sql > pg-stats-tables-all-after-failover.log 
: 1485341779:0;more pg-stats-tables-all-after-failover.log
: 1485341903:0;psql -E -h 127.0.0.1 -p 9702 -U wellbeingid_production_app --dbname=wellbeingid_production --file=pg-settings.sql > pg-settings-after-dms-cdc-param.log 
: 1485341941:0;more table-checks.sql
: 1485342055:0;psql -E -h 127.0.0.1 -p 9702 -U wellbeingid_production_app --dbname=wellbeingid_production --file=vacuum-freeze-info.sql > vacuum-freeze-info-after-upgrade.log 
: 1485375846:0;more               table              | table scans | tuples scanned | index lookups | tuples fetched via index
: 1485375846:0;---------------------------------+-------------+----------------+---------------+--------------------------
: 1485375879:0;more pg-stats-tables-all
: 1485377301:0;ls -l *.sql 
: 1485377350:0;scp *.sql ubuntu@172.16.1.17:/home/ubuntu/dms/walkadoo/
: 1485377394:0;scp *.sql ubuntu@172.16.1.117:/home/ubuntu/dms/walkadoo/
: 1485377879:0;cd utf8_conv_1
: 1485377890:0;more wd_alter_columns_modify_varchar191_utf8mb4.sql
: 1485377906:0;more wd_alter_table_char_set_collate_utf8mb4.sql
: 1485377920:0;more convert1.sql
: 1485377933:0;more wd_mods_utf8mb4_ordered_all.sql
: 1485876942:0;cd Documents/
: 1485876949:0;mkdir deis-k8s
: 1485876967:0;vi user-list
: 1485877326:0;mv user-list iam-user-list
: 1485877352:0;deis users:list
: 1485877371:0;deis users:list | sort > deis-users-list
: 1485877412:0;ls -lt4ra
: 1485877433:0;diff -u iam-user-list deis-users-list
: 1485877492:0;vi deis
: 1485877551:0;paste iam-user-list deis-users-list
: 1485877700:0;ssh heidi.schmidt@54.175.188.41
: 1485877721:0;ssh heidischmidt@172.24.66.124
: 1485877776:0;history | grep shipit | grep ssh
: 1485877821:0;cd Documents/deis-k8s
: 1485877848:0;in/zsh
: 1485877848:0;newrelic:x:999:998:New Relic daemons:/opt/newrelic:/bin/false
: 1485877848:0;sensu:x:998:997:Sensu Monitoring Framework:/opt/sensu:/bin/false
: 1485877865:0;vi ec2-user-gecos-list
: 1485877886:0;cat ec2-user-gecos-list| awk -F ":" '{print $1}'
: 1485877895:0;cat ec2-user-gecos-list| awk -F ":" '{print $1}' > ec2-user-list
: 1485877935:0;cat ec2-user-list| sort
: 1485877943:0;cat ec2-user-list| sort > ec2-user-gecos-list
: 1485877949:0;mv ec2-user-gecos-list ec2-user-list
: 1485877971:0;wc -l *list
: 1485877983:0;diff -u ec2-user-list iam-user-list
: 1485878161:0;vi ec2-user-list
: 1485878185:0;vi iam-user-list
: 1485878192:0;paste iam-user-list ec2-user-list
: 1485878311:0;vi deis-users-list
: 1485878317:0;paste deis-users-list ec2-user-list
: 1485878352:0;paste iam-user-list deis-users-list ec2-user-list
: 1485878434:0;grep antaresmeketaec2-user-list
: 1485878441:0;grep antaresmeketa ec2-user-list
: 1485879406:0;vi openvpn-user
: 1485880098:0;more deis-users-list
: 1485965837:0;vi deis-user-create-per-env.sh 
: 1485966217:0;ls -rtla
: 1485966242:0;chmod 0744 deis-user-create-per-env.sh
: 1485966598:0;./deis-user-create-per-env.sh
: 1485966660:0;cat deis-user-create-per-env.sh
: 1485966721:0;deis auth:register https://deis.meyouhealth.com --username=maggie.concannon password=AsioURBaNdIAMIGr
: 1485966769:0;history | grep gardner
: 1485966954:0;vi deis-user-create-per-env.sh
: 1485967021:0;sh -xv deis-user-create-per-env.sh
: 1485967103:0;history | grep login | grep heidi 
: 1485967151:0;for i in `deis apps | grep staging | awk '{print $1}' `\
do \
deis perms:create maggie.concannon --app $i\
done
: 1485967194:0;history |Âƒ€grep deis | greo heidi
: 1485967197:0;history |Âƒ€grep deis | grep heidi
: 1485967204:0;history |Âƒ€grep deis 
: 1485967247:0;deis auth:register https://deis.meyouhealth.com --username=maggie.concannon --password=AsioURBaNdIAMIGr
: 1485967272:0;for i in `deis apps | grep production | awk '{print $1}' `\
do \
deis perms:create maggie.concannon --app $i\
done
: 1485967340:0;for i in `deis apps | grep production | awk '{print $1}' `\
do \
deis perms:create guillermo.guerini --app $i\
done
: 1485967379:0;for i in `deis apps | grep staging | awk '{print $1}' `\
do \
deis perms:create guillermo.guerini --app $i\
done
: 1485976664:0;vi maggie
: 1485977088:0;cat maggie
: 1485977184:0;aws configure
: 1485977837:0;brew list | grep python
: 1485977995:0;mroe g-accessKeys.csv
: 1485977998:0;more g-accessKeys.csv
: 1485978219:0;python --version
: 1485978257:0;which python
: 1485978262:0;ls -l /usr/local/bin/python
: 1486062426:0;history | grep wellbeingid-production
: 1486062445:0;ssh -A -L 9703:wellbeingid-production.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:5432 heidischmidt@54.225.238.195 -N
: 1486062539:0;grep grant *.sql
: 1486062561:0;more pg_grants_concierge_prod.sql
: 1486062675:0;vi grants-mod-dms.sql 
: 1486062844:0;cat grants-mod-dms.sql
: 1486063319:0;vi dms-table-audit.sql 
: 1486063543:0;vi dms-intercept-ddl.sql 
: 1486064165:0;mv wbid-schema-non-vpc-create-statements.sql*.sql wbid-schema-non-vpc-create-statements.txt
: 1486064174:0;mv wbid-schema-non-vpc-create-statements.txt wbid-schema-non-vpc-create-statements.sql
: 1486064250:0;grep -i aws DROP-wbid-for-INFO-ONLY.sql
: 1486064396:0;vi wbid-migration-schema-non-vpc.sql
: 1486064519:0;vi grants-mod-dms.sql
: 1486064528:0;vi dms-table-audit.sql
: 1486064539:0;cat dms-table-audit.sql
: 1486064580:0;cat dms-intercept-ddl.sql
: 1486064655:0;vi wbid-dms-tbl-function-creation-log.log
: 1486065485:0;vi dms-cleanup-tbl-trigger.sql 
: 1486065542:0;vi dms-*.sql
: 1486065651:0;more pg_stat_activity.sql
: 1486131827:0;aws dms
: 1486132050:0;aws dms describe-endpoints > aws-dms-endpoints
: 1486132057:0;vi aws-dms-endpoints
: 1486132076:0;cat aws-dms-endpoints
: 1486132853:0;pgdump --help
: 1486132865:0;pg_dump --version
: 1486132933:0;pgdump -E -h 127.0.0.1 -p 9703 -U wellbeingid_production_app --dbname=wellbeingid_production  --schema-only --file=wbid-existing-schema.sql 
: 1486132941:0;pg_dump -E -h 127.0.0.1 -p 9703 -U wellbeingid_production_app --dbname=wellbeingid_production  --schema-only --file=wbid-existing-schema.sql 
: 1486132951:0;pg_dump  -h 127.0.0.1 -p 9703 -U wellbeingid_production_app --dbname=wellbeingid_production  --schema-only --file=wbid-existing-schema.sql 
: 1486133387:0;head -250 wbid-existing-schema.sql
: 1486152124:0;ssh -A -L 9704:wbid-production.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:5432 heidischmidt@54.225.238.195 -N
: 1486152141:0;ssh heidischmidt@54.225.238.195
: 1486152854:0;psql -E -h 127.0.0.1 -p 9704 -U wellbeingid_production_app --dbname=wellbeingid_production 
: 1486153430:0;nslookup 54.45.69.84
: 1486167484:0;grep -v "\|" alter-tbl-wbid-old.sql
: 1486167498:0;cat alter-tbl-wbid-old.sql
: 1486167568:0;cat alter-tbl-wbid-old.sql | awk -F "\|" '{"ALTER TABLE rename ", print $2, " to ", print $2,"_old;"}' 
: 1486167597:0;cat alter-tbl-wbid-old.sql | awk -F "\|" '{'ALTER TABLE rename ', print $2, ' to ', print $2,'_old;'}' 
: 1486167612:0;cat alter-tbl-wbid-old.sql | awk -F "\|" '{print $2}'
: 1486167619:0;cat alter-tbl-wbid-old.sql | awk -F "\|" '{"ALTER",print $2}'
: 1486167628:0;cat alter-tbl-wbid-old.sql | awk -F "\|" '{'ALTER',print $2}'
: 1486167640:0;cat alter-tbl-wbid-old.sql | awk -F "\|" '{ 'ALTER', print $2}'
: 1486167649:0;cat alter-tbl-wbid-old.sql | awk -F "\|" '{ print $2}'
: 1486167655:0;cat alter-tbl-wbid-old.sql | awk -F "\|" '{ print $2}' > 1
: 1486167664:0;vi 1
: 1486167710:0;cat alter-tbl-wbid-old.sql | awk -F "\|" '{ print $2}' > 2
: 1486167728:0;paste 1 2 > alter-tbl-wbid-old.sql
: 1486167731:0;vi alter-tbl-wbid-old.sql
: 1486168054:0;scp alter-tbl-wbid-old.sql ubuntu@172.16.1.117:
: 1486168209:0;;ll
: 1486168271:0;scp wbid-schema-non-vpc-create-statements.sql ubuntu@172.16.1.117:
: 1486168433:0;more DROP-wbid-for-INFO-ONLY.sql | grep TABLE
: 1486168506:0;rm 1
: 1486168530:0;cd postgres
: 1486168555:0;ls -l *schema*
: 1486168577:0;more wbid-migration-schema-non-vpc.sql
: 1486168587:0;ls -l wbid-migration-schema-non-vpc.sql
: 1486168595:0;ls -l wbid-migration-schema-non-vpc.sql\  
: 1486168608:0;mv wbid-migration-schema-non-vpc.sql\  wbid-migration-schema
: 1486168625:0;cd wbid-migration-schema
: 1486168629:0;more wbid-migration-schema
: 1486168639:0;rm wbid-migration-schema
: 1486168693:0;scp wbid-existing-schema.sql ubuntu@172.16.1.117:
: 1486169056:0;grep -i json wbid-existing-schema.sql
: 1486170667:0;vi wbid-existing-schema.sql
: 1486170693:0;head -4000 wbid-existing-schema.sql
: 1486170704:0;head -4000 wbid-existing-schema.sql | head -500
: 1486170717:0;head -4000 wbid-existing-schema.sql  |more
: 1486170728:0;head -4000 wbid-existing-schema.sql  | tail -500 
: 1486170991:0;pg_dump  -h 127.0.0.1 -p 9703 -U wellbeingid_production_app --dbname=wellbeingid_production  --schema-only --superuser=meyouhealth --clean --create --schema=public --file=wbid-test-flag-schema.sql 
: 1486171031:0;more wbid-test-flag-schema.sql
: 1486394998:0;mv wbid-test-flag-schema.sql wbid-src-test-flag-schema.sql
: 1486395006:0;more wbid-existing-schema.sql
: 1486395031:0;mv wbid-existing-schema.sql wbid-src-existing-schema.sql
: 1486396206:0;more DROP-wbid-for-INFO-ONLY.sql| grep aws
: 1486396229:0;more DROP-wbid-for-INFO-ONLY.sql| grep aws > aws-dms-objects-drop.sql 
: 1486396307:0;vi aws-dms-objects-drop.sql
: 1486396348:0;grep -i awsdms wbid-existing-schema.sql
: 1486396358:0;grep -i awsdms wbid-src-existing-schema.sql
: 1486396436:0;cp wbid-src-existing-schema.sql aws-dms-create-objects.sql
: 1486396839:0;cd ../postgres
: 1486396853:0;more pg_grants.sql
: 1486396873:0;vi aws-dms-create-objects.sql
: 1486404341:0;cat aws-dms-objects-drop.sql
: 1486404458:0;vi checks-after-drop.log
: 1486405078:0;vi eligibility_records_src_wbid.log
: 1486405756:0;more aws-dms-objects-drop.sql
: 1486405762:0;more aws-dms-create-objects.sql
: 1486411569:0;history | grep pg_dump
: 1486414676:0;grep -i grant wbid-src-test-flag-schema.sql
: 1486414690:0;grep -i grant wbid-src-test-flag-schema.sql | grep wellbeingid_production_app
: 1486414730:0;grep -i grant wbid-src-test-flag-schema.sql | grep wellbeingid_production_app > work-around-grants-meyouhealth.sql
: 1486414735:0;vi work-around-grants-meyouhealth.sql
: 1486414832:0;psql -E -h 127.0.0.1 -p 9703 -U meyouhealth  --dbname=wellbeingid_production 
: 1486414882:0;psql -E -h 127.0.0.1 -p 9703 -U wellbeingid_production_app  --dbname=wellbeingid_production 
: 1486416110:0;more work-around-grants-meyouhealth.sql
: 1486416131:0;mv work-around-grants-meyouhealth.sql src-wbid-work-around-grants-meyouhealth.sql
: 1486416145:0;mv checks-after-drop.log src-wbid-checks-after-drop.log
: 1486416157:0;more DROP-wbid-for-INFO-ONLY.sql
: 1486416169:0;rm DROP-wbid-for-INFO-ONLY.sql
: 1486416180:0;more alter-tbl-wbid-old.sql
: 1486416194:0;mv alter-tbl-wbid-old.sql target-ONLY-alter-tbl-wbid-old.sql
: 1486416215:0;mv aws-dms-objects-drop.sql src-wbid-aws-dms-objects-drop.sql
: 1486416224:0;mv aws-dms-create-objects.sql src-wbid-aws-dms-create-objects.sql
: 1486416235:0;more wbid-schema-non-vpc-create-statements.sql
: 1486416261:0;mv wbid-schema-non-vpc-create-statements.sql src-wbid-db-solo-created-schema-non-vpc-create-statements.sql
: 1486573003:0;vi aws-dms-postgres-support-ticket-020817.txt
: 1486656157:0;grep "ALTER TABLE" *.sql 
: 1486675932:0;grep "ALTER TABLE" *.sql | grep 191
: 1486675950:0;grep "ALTER TABLE" wd-alters-all-ucs2-to-utf8mb4.sql | grep 191
: 1486675954:0;grep "ALTER TABLE" wd-alters-all-ucs2-to-utf8mb4.sql | grep 191 | wc -l 
: 1486676875:0;nslookup walkadoo-production3.cduvhvvpw6oo.us-east-1.rds.amazonaws.com
: 1486676884:0;telnet 172.24.0.117 3306
: 1486676927:0;ssh -A -L 9911:127.0.0.1:9911 ubuntu@54.86.62.65 ssh -L 9911:walkadoo-production3.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@54.237.232.212
: 1486677003:0;history  | grep mysql | grep 127.0.0.1
: 1486677011:0;mysql -u meyouhealth -P 9913 -p -h 127.0.0.1 -vvv -D walkadoo_production -A
: 1486677064:0;nslookup vpn.meyouhealth.com
: 1486677536:0;ssh 172.24.0.117
: 1486678296:0;IF
: 1486748742:0;grep shipit *
: 1486748753:0;cd old
: 1486748762:0;more shipit-wd-stg-from-github-chef-walkadoo-recipes-application-rb-vars.txt.old
: 1486748818:0;grep POOL shipit-wd-stg-from-github-chef-walkadoo-recipes-application-rb-vars.txt.old
: 1486868567:0;brew upgrade 
: 1486868900:0;brew list terraform
: 1486868977:0;history | grep wellbeingid-production.cduvhvvpw6oo.us-east-1.rds.amazonaws.com
: 1486869032:0;pg_dump --help
: 1486869395:0;pg_dump  -h 127.0.0.1 -p 9708 -U wellbeingid_production_app  --dbname=wellbeingid_production --file=wbid-data-dump-src.sql --inserts --quote-all-identifiers --data-only --jobs=5
: 1486869435:0;pg_dump  -h 127.0.0.1 -p 9708 -U wellbeingid_production_app  --dbname=wellbeingid_production --file=wbid-data-dump-src.sql --inserts --quote-all-identifiers --data-only 
: 1486869476:0;pg_dump  -h 127.0.0.1 -p 9708 -U meyouhealth  --dbname=wellbeingid_production --file=wbid-data-dump-src.sql --inserts --quote-all-identifiers --data-only 
: 1486869526:0;head wbid-data-dump-src.sql
: 1486869547:0;tail wbid-data-dump-src.sql
: 1486869774:0;vi test1-dump1-inserts 
: 1486870052:0;ps -ef | grep pg_dump
: 1486945007:0;ls -lrta wbid-data-dump-src.sql
: 1486999346:0;history | grep shared-staging-mysql.cduvhvvpw6oo.us-east-1.rds.amazonaws.com
: 1486999370:0;history | grep ssh | grep shared
: 1486999388:0;ssh -A -L 9997:127.0.0.1:9997 ubuntu@52.86.252.208 ssh -L 9997:staging-shared-mysql.cduvhvvpw6oo.us-east-1.rds.amazonaws.com :3306 -N ubuntu@10.145.46.88
: 1486999454:0;ssh ubuntu@10.145.46.88
: 1486999617:0;ssh -A -L 9997:127.0.0.1:9997 ubuntu@52.86.252.208 ssh -L 9997:shared-staging-mysql.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N ubuntu@10.145.46.88
: 1486999803:0;ssh -A -L 9997:127.0.0.1:9997 ubuntu@52.86.252.208 ssh -L 9997:shared-staging-mysql.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N ubuntu@10.0.19.161
: 1486999844:0;ssh ubuntu@10.0.19.161
: 1486999892:0;ls -ltrah wbid-data-dump-src.sql >> test1-dump1-inserts
: 1486999999:0;ssh -A -L 9997:127.0.0.1:9997 ubuntu@52.86.252.208 ssh -L 9997:shared-staging-mysql.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N ubuntu@10.150.68.133
: 1487000006:0;mysql -u meyouhealth -P 9997 -p -h 127.0.0.1 -vvv -D walkadoo_staging -A
: 1487000029:0;ssh ubuntu@10.150.68.133
: 1487000103:0;ssh heidischmit@10.150.68.133
: 1487001086:0;ssh heidischmidt@ec2-54-81-235-76.compute-1.amazonaws.com
: 1487001209:0;nslookup 52.86.252.208
: 1487001319:0;ssh -A -L 9997:127.0.0.1:9997 ubuntu@52.86.252.208 ssh -L 9997:shared-staging-mysql.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N ubuntu@10.186.173.88
: 1487001337:0;ssh -A -L 9997:127.0.0.1:9997 ubuntu@52.86.252.208 ssh -L 9997:shared-staging-mysql.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.186.173.88
: 1487001354:0;ssh heidischmidt@10.186.173.88
: 1487001413:0;ssh heidischmidt@54.81.235.76
: 1487001515:0;grep "ALTER DATABASE" *.sql 
: 1487001593:0;more convert_all_tables.sql
: 1487001706:0;more convert_all_tables.sql | grep -v "^--"
: 1487001712:0;more convert_all_tables.sql | grep -v "^--" > staging
: 1487001715:0;vi staging
: 1487001733:0;cat staging
: 1487001873:0;more convert_all_tables.sql | grep  "^--" > staging2
: 1487001876:0;more staging2
: 1487002093:0;more check_charset_collation.sql
: 1487002212:0;cd wd-create-idx
: 1487002221:0;more wd-create-constraints.sql
: 1487002237:0;ls -l *.sql
: 1487002244:0;more wd-create-idx-all.sql
: 1487003576:0;cat check_charset_collation.sql
: 1487004981:0;more wd-alters-all-ucs2-to-utf8mb4.sql
: 1487004987:0;grep schema wd-alters-all-ucs2-to-utf8mb4.sql
: 1487005165:0;more wd_core_47_to_change.sql
: 1487018552:0;shipit ssh walkadoo_staging
: 1487087331:0;ALTER   TABLE   authentications   CHANGE   `provider_code`   `provider_code`   varchar(191) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;
: 1487087342:0;shipit env update --console-host 10.0.17.71 walkadoo-staging
: 1487089494:0;rbenv 2.1.5
: 1487089497:0;ced
: 1487089498:0;cde
: 1487089538:0;history | grep shipit | grep -i config
: 1487089610:0;shipit env config_set walkadoo-staging DATABASE_URL mysql2://walkadoo_stg_app:LBay1GTZgD081Y@shared-staging-mysql.cduvhvvpw6oo.us-east-1.rds.amazonaws.com/walkadoo_staging?reconnect=true&encoding=utf8mb4&pool=15
: 1487089630:0;shipit env config_set walkadoo-staging DATABASE_URL "mysql2://walkadoo_stg_app:LBay1GTZgD081Y@shared-staging-mysql.cduvhvvpw6oo.us-east-1.rds.amazonaws.com/walkadoo_staging?reconnect=true&encoding=utf8mb4&pool=15"
: 1487089643:0;shipit env config_list walkadoo-staging | grep DATABASE
: 1487089730:0;history | grep shipit | grep deploy
: 1487089763:0;shipit deployment create walkadoo-staging 6d7b1c3b380fe052323f869b128f7005c0cf6fdc
: 1487092145:0;history | grep config_set
: 1487100069:0;shipit deploy create walkadoo-staging 1ebe4bd95e74e4c785809c7267b6bf575999f318 
: 1487100443:0;brew install minikube
: 1487100471:0;brew install Caskroom/cask/minikube
: 1487101059:0;which minikube
: 1487101067:0;mkdir minikube 
: 1487101070:0;cd minikube
: 1487101375:0;minikube ip
: 1487101392:0;vi start-log-n-ip
: 1487101475:0;kubectl run hello-minikube --image=gcr.io/google_containers/echoserver:1.4 --port=8080
: 1487101495:0;kubectl expose deployment hello-minikube --type=NodePort
: 1487101498:0;telnet localhost 8080
: 1487101537:0;minikube service hello-minikube
: 1487101578:0;curl $(minikube service hello-minikube --url)
: 1487101622:0;minikube docker-env
: 1487101958:0;minikube start
: 1487102262:0;ps -ef | grep docker
: 1487102280:0;netstat -rn | grep "192.168.99.100"
: 1487102300:0;eval $(minikube docker-env)
: 1487102345:0;minkube ssh 
: 1487102350:0;minikube ssh 
: 1487102437:0;telnet 192.168.99.100 8080
: 1487102441:0;telnet 192.168.99.100 80
: 1487102491:0;curl https://192.168.99.100:30776
: 1487102506:0;curl http://192.168.99.100:30776
: 1487102529:0;curl http://10.0.0.8:30776
: 1487102691:0;netstat -rn | grep 30776
: 1487102696:0;netstat -rn | grep 8080
: 1487102700:0;netstat -rn | grep docker
: 1487102704:0;netstat -rn | grep kube
: 1487102715:0;netstat -rn | grep 192
: 1487102916:0;more .kube/config
: 1487103039:0;kubectl get pods --context=minikube
: 1487103065:0;shell minikube dashboard
: 1487103106:0;minikube dashboard
: 1487172938:0;kubectl get 
: 1487173086:0;kubectl get pods namespace kube-system
: 1487173092:0;kubectl get pods --namespace kube-system
: 1487173252:0;kubectl describe pod --namespace kube-system kubernetes-dashboard-75skb
: 1487173287:0;kubectl describe pod --namespace kube-system kube-dns-v20-nhjqf
: 1487173707:0;kubectl get replicationcontrollers
: 1487173741:0;kubectl get --namespaces kube-system rc
: 1487173881:0;kubectl get --namespaces kube-system deployments
: 1487173888:0;kubectl get --namespace kube-system deployments
: 1487173895:0;kubectl get --namespace kube-system rc
: 1487173904:0;kubectl get
: 1487174074:0;kubectl get --namespaces default rc
: 1487174081:0;kubectl get --namespace default rc
: 1487174175:0;kubectl get pods --namespace default
: 1487174944:0;kubectl current config
: 1487175066:0;kubectl cluster 
: 1487175635:0;vi ~/.kube/config
: 1487177019:0;deis config -a hello200-staging | grep TEST
: 1487177050:0;deis config:set -a hello200-staging TEST=heidi
: 1487182476:0;ls -l /data
: 1487182489:0;ls -l /var/lib/localkube
: 1487182497:0;ls -l /var/lib/docker
: 1487182502:0;locate localkube
: 1487182524:0;ls -l /Users
: 1487183913:0;brew list  | grep helm
: 1487183970:0;which deis 
: 1487183984:0;mv /usr/local/bin/deis /usr/local/bin/deis-hand-install-2.10.0 
: 1487183989:0;brew install deis
: 1487184522:0;kubectl config 
: 1487184581:0;kubectl config current-context 
: 1487184674:0;brew list | grep -i tiller
: 1487184679:0;brew install tiller
: 1487184848:0;helm init
: 1487184854:0;ls -l ~/.helm
: 1487184860:0;ls -l ~/.helm/*
: 1487184928:0;tiller --help
: 1487185063:0;helm repo update
: 1487185099:0;helm install stable/mysql
: 1487185259:0;kubectl get secret --namespace default cranky-lemur-mysql -o jsonpath="{.data.mysql-root-password}" | base64 --decode; echo
: 1487185359:0;helm delete cranky-lemur
: 1487185366:0;helm list 
: 1487185391:0;helm list --help
: 1487186001:0;helm repo add deis https://charts.deis.com/workflow
: 1487186271:0;kubectl --namespace=deis get pods -w 
: 1487186284:0;kubectl --namespace=deis get pods 
: 1487186676:0;deis register http://deis.192.168.99.100.nip.io:30069
: 1487186738:0;deis create --no-remote
: 1487192292:0;deis logs -a cranky-lemur
: 1487192372:0;deis logs -a whistful-crab
: 1487192379:0;ls -l /var/log/containers
: 1487192420:0;helm list --all
: 1487193887:0;kubectl --namespace=deis logs deis-logger-fluentd-vlmcb
: 1487194063:0;kubectl --namespace=deis logs deis-database-4237974775-b8jd9
: 1487194090:0;ls -l s3://dbwal/wal_005
: 1487194120:0;aws s3 ls dbwal
: 1487194526:0;helm inspect stable/mysql
: 1487194863:0;helm search postgres
: 1487258674:0;kubectl --namespace=deis describe
: 1487260297:0;deis builds --help
: 1487260459:0;history | grep hello
: 1487260498:0;history > deis-k8s-history.txt
: 1487260506:0;vi deis-k8s-history.txt
: 1487260777:0;kubectl get po --namespace kube-system
: 1487260833:0;deis create --help
: 1487260872:0;deis apps:create hello200-minikube-dev
: 1487260962:0;deis apps:create bamboo-server
: 1487261096:0;more .env.heidi
: 1487261122:0;more .env_default
: 1487261144:0;diff -u .env_heidi .env_default
: 1487261179:0;vi database.yml
: 1487261390:0;history | grep deis | grep config_set
: 1487261424:0;cat .env_default
: 1487261476:0;deis config -a hello200-minikube-dev DATABASE_HOST=localhost
: 1487261496:0;deis config:set -a hello200-minikube-dev DATABASE_HOST=localhost
: 1487261534:0;for i in `cat .env_default` \
do \
deis config:set -a hello200-minikube-dev $i\
done
: 1487261617:0;history | grep stable
: 1487261629:0;helm install stable/postgres
: 1487261659:0;helm search database
: 1487261683:0;helm install stable/postgresql
: 1487262138:0;deis config -a hello200-minikube-dev 
: 1487262218:0;kubectl get pods 
: 1487262245:0;deis pa
: 1487262332:0;deis destroy bamboo-server
: 1487262350:0;deis apps:destroy -a bamboo-server
: 1487262377:0;deis ps -a hello200-minikube-dev
: 1487262430:0;git remotes 
: 1487262500:0;deis git --help
: 1487262562:0;deis git:remote -a hello200-minikube-dev -r dev 
: 1487262685:0;cd .ssh
: 1487262724:0;ssh-add -L
: 1487262751:0;deis keys:add 
: 1487262781:0;git push dev 
: 1487262991:0;kubectl --namespace deis logs deis-builder-574352672-b0q3m
: 1487263032:0;kubectl --namespace deis logs slugbuild-hello200-minikube-dev-3fd2e61a-6a24db4a
: 1487263047:0;kubectl --namespace deis get po
: 1487263486:0;kubectl --namespace hello200-minikube-dev get deployments
: 1487263533:0;kubectl --namespace hello200-minikube-dev get pods -w 
: 1487263617:0;kubectl --namespace hello200-minikube-dev get pods
: 1487263631:0;kubectl --namespace hello200-minikube-dev logs hello200-minikube-dev-web-2498593249-xvqcv
: 1487263717:0;kubectl --namespace hello200-minikube-dev logs hello200-minikube-dev-web-2498593249-xvqcv -f
: 1487263765:0;kubetctl --help
: 1487263787:0;kubectl describe cluster-info
: 1487263840:0;kubectl --namespace deis get services
: 1487263956:0;telnet 192.168.99.100 30069
: 1487264004:0;env | grep DOCKER
: 1487264060:0;kubectl describe ro --namespace deis
: 1487264099:0;kubectl describe service  deis-router
: 1487264113:0;kubectl describe service  deis-router --namespace deis
: 1487264272:0;curl http://192.168.99.100:30069
: 1487264412:0;curl -H "Host: hello200-minikube-dev" http://192.168.99.100:30069
: 1487264428:0;curl -H "Host: hello200-minikube-dev" 192.168.99.100:30069
: 1487264443:0;curl -H "Host: hello200-minikube-dev.myhdev.com" 192.168.99.100:30069
: 1487264504:0;deis domains:add hello200
: 1487264522:0;deis domains
: 1487264557:0;deis logs 
: 1487264612:0;vi config/environment
: 1487264625:0;vi config/environments/staging.rb
: 1487264714:0;deis config:set --help
: 1487264757:0;deis config:set -a hello200-minikube-dev RAILS_ENV=staging RAILS_LOG_TO_STDOUT=true
: 1487264989:0;helm install stable/postgresql --namespace hello200-minikube-dev 
: 1487265056:0;helm delete stable/postgresql --namespace default
: 1487265129:0;helm delete fashionable-elephant
: 1487269821:0;deis ps 
: 1487269972:0;kubectl get secret --namespace hello200-minikube-dev morbid-buffoon-postgresql -o jsonpath="{.data.postgres-user}" | base64 --decode; echo
: 1487269979:0;kubectl get secret --namespace hello200-minikube-dev morbid-buffoon-postgresql -o jsonpath="{.data.postgres-username}" | base64 --decode; echo
: 1487269996:0;kubectl get secret --namespace hello200-minikube-dev morbid-buffoon-postgresql
: 1487270033:0;kubectl --namespace hello200-minikube-dev exec -ti hello200-minikube-dev-web-1377734867-9qhbg bash 
: 1487270165:0;deis config | grep -i url
: 1487270274:0;shipit env show hello200_staging
: 1487270288:0;shipit env show hello200-staging
: 1487270353:0;shipit env config_get wellbeingid-staging DATABASE_URL
: 1487270456:0;deis config:set -a hello200-minikube-dev DATABASE_URL="postgres://10.0.0.2/hello200_development?pool=5"
: 1487270563:0;deis run -- bundle exec rake -T 
: 1487270814:0;shipit env config_get wellbeingid-staging DATABASE_PASSWORD
: 1487270828:0;deis config | grep -i data
: 1487270936:0;helm status morbid-buffoon
: 1487270943:0;kubectl get secret --namespace hello200-minikube-dev morbid-buffoon-postgresql -o jsonpath="{.data.postgres-password}" | base64 --decode; echo
: 1487270959:0;deis config:set -a hello200-minikube-dev DATABASE_URL="postgres://postgres:XbHROpU0sb@10.0.0.2/hello200_development?pool=5"
: 1487271069:0;deis run -- bundle exec rake db:{setup,seed}
: 1487271137:0;ps -ef 
: 1487271159:0;uptime
: 1487271179:0;top
: 1487271265:0;deis ps:restart web 
: 1487271335:0;curl -H "Host: hello200.myhdev.com" 192.168.99.100:5432
: 1487271451:0;deis config:set -a hello200-minikube-dev DISABLE_SSL=true
: 1487271687:0;sudo su - 
: 1487271906:0;curl -H "Host: hello200.myhdev.com" 192.168.99.100:30069
: 1487271941:0;nslookup 192.168.99.100
: 1487271977:0;grep myhdev /etc/hosts
: 1487272076:0;deis config:set -a hello200-minikube-dev RAILS_SERVE_STATIC_ASSETS=true
: 1487272242:0;deis config:set -a hello200-minikube-dev RAILS_SERVE_STATIC_FILES=true
: 1487272349:0;deis config:unset -a hello200-minikube-dev RAILS_SERVE_STATIC_ASSETS
: 1487272682:0;kubectl --namespace deis get logs deis-router-2099956932-38w6q
: 1487272692:0;kubectl --namespace deis logs deis-router-2099956932-38w6q
: 1487272764:0;deis config
: 1487272821:0;deis config:set -a hello200-minikube-dev DOMAIN=hello200.myhdev.com
: 1487272934:0;deis config:set -a hello200-minikube-dev HOST=hello200.myhdev.com
: 1487299072:0;more wbid-src-test-flag-schema.sql
: 1487299100:0;vi wbid-src-test-flag-schema.sql
: 1487299207:0;head -100 wbid-src-test-flag-schema.sql
: 1487299221:0;head -1000 wbid-src-test-flag-schema.sql
: 1487299232:0;head -1000 wbid-src-test-flag-schema.sql | grep json
: 1487299247:0;head -1000 wbid-src-test-flag-schema.sql |more
: 1487299383:0;more wbid-data-dump-src.sql
: 1487299506:0;grep customers wbid-data-dump-src.sql | head -10
: 1487299683:0;grep customers wbid-data-dump-src.sql | wc -l
: 1487299828:0;more src-wbid-aws-dms-create-objects.sql
: 1487299855:0;ls *db*
: 1487299860:0;more src-wbid-db-solo-created-schema-non-vpc-create-statements.sql
: 1487300032:0;more wbid-src-existing-schema.sql
: 1487300176:0;ssh -A -L 9708:wellbeingid-production.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:5432 heidischmidt@54.225.238.195 -N
: 1487300221:0;psql -E -h 127.0.0.1 -p 9708 -U wellbeingid_production_app  --dbname=wellbeingid_production 
: 1487300440:0;grep customers wbid-data-dump-src.sql | more
: 1487300480:0;grep customers wbid-data-dump-src.sql 
: 1487300501:0;grep customers wbid-data-dump-src.sql | grep "INSERT INTO"
: 1487300510:0;grep customers wbid-data-dump-src.sql | grep "INSERT INTO" | wc -l 
: 1487300744:0;grep customers wbid-data-dump-src.sql | grep "INSERT INTO" | awk -F "," '{print $6}'
: 1487300833:0;grep customers wbid-data-dump-src.sql | grep "INSERT INTO" | grep NULL
: 1487300856:0;grep customers wbid-data-dump-src.sql > customers.sql 
: 1487300934:0;grep auto_product_enrollments wbid-data-dump-src.sql > auto_product_enrollments.sql 
: 1487301015:0;grep NULL customers.sql
: 1487301019:0;grep NULL customers.sql | wc -l 
: 1487301072:0;grep -v NULL customers.sql 
: 1487301094:0;grep -v NULL customers.sql | grep "INSERT INTO" 
: 1487301116:0;grep -v NULL customers.sql | grep "INSERT INTO" | grep "Chief Privacy" 
: 1487301124:0;grep -v NULL customers.sql | grep "INSERT INTO" | grep "Chief Privacy" | wc -l 
: 1487301360:0;grep -v NULL customers.sql | grep "INSERT INTO" | grep -v "Chief Privacy" | wc -l 
: 1487301364:0;grep -v NULL customers.sql | grep "INSERT INTO" | grep -v "Chief Privacy" 
: 1487301508:0;more auto_product_enrollments.sql
: 1487341200:0;minikube --help
: 1487341215:0;minikube stop
: 1487341590:0;env | grep -i kop
: 1487341654:0;export NAME=heidi.kubernetes.myhstg.com 
: 1487341659:0;env | grep NAME
: 1487341691:0;aws ec2 describe-availability-zones --region us-east-1
: 1487341743:0;kops create cluster --zones --help
: 1487341795:0;kops create cluster --zones us-east-1a, us-east-1b, us-east-1c --name ${NAME}
: 1487341815:0;kops create cluster --zones us-east-1a,us-east-1b,us-east-1c --name ${NAME}
: 1487341928:0;kops delete --help 
: 1487341954:0;kops delete cluster --name heidi.kubernetes.myhstg.com
: 1487342324:0;man tee
: 1487342353:0;kops create cluster --zones us-east-1a,us-east-1b,us-east-1c --network-cidr 172.28.0.0/16 --name ${NAME} | tee -a kops-test-cluster.log 
: 1487343259:0;kops create cluster --zones us-east-1a,us-east-1b,us-east-1c --network-cidr 172.28.0.0/16 --name ${NAME} --yes
: 1487351000:0;env | grep EDITOR
: 1487351011:0;kops edit cluster ${NAME}
: 1487351139:0;more $HOME/.kops.yaml
: 1487351179:0;kops export --config ${NAME}
: 1487351212:0;kops export --config --name ${NAME}
: 1487351233:0;kops export --name ${NAME}
: 1487351338:0;kops get clusters --help
: 1487351362:0;kops get clusters --target=terraform
: 1487351398:0;kops --target=terraform 
: 1487360559:0;kops update cluster --help
: 1487360628:0;kops update cluster heidi.kubernetes.myhstg.com --target ${NAME}-test-tf-file 
: 1487360670:0;kops create cluster --zones us-east-1a,us-east-1b,us-east-1c --network-cidr 172.28.0.0/16 --name ${NAME} --target 
: 1487360683:0;kops create cluster --zones us-east-1a,us-east-1b,us-east-1c --network-cidr 172.28.0.0/16 --name ${NAME} --target direct 
: 1487360699:0;kops update cluster heidi.kubernetes.myhstg.com --target direct 
: 1487360739:0;more kops-test-cluster.log
: 1487360781:0;kops export kubecfg $NAME 
: 1487360972:0;kops toolbox dump --help
: 1487360992:0;kops toolbox convert-imported --help
: 1487380036:0;pg_dump -h 127.0.0.1 -p 9708 -U wellbeingid_production_app  --dbname=wellbeingid_production  --schema test.sql 
: 1487380071:0;pg_dump -h 127.0.0.1 -p 9708 -U wellbeingid_production_app  --dbname=wellbeingid_production  --schema-only 
: 1487380107:0;pg_dump -h 127.0.0.1 -p 9708 -U meyouhealth  --dbname=wellbeingid_production  --schema-only 
: 1487380140:0;history | grep schema
: 1487380191:0;pg_dump  -h 127.0.0.1 -p 9708 -U wellbeingid_production_app --dbname=wellbeingid_production  --schema-only --superuser=meyouhealth --clean --create --schema=public --file=wbid-src-schema.sql
: 1487380220:0;pg_dump  -h 127.0.0.1 -p 9708 -U meyouhealth --dbname=wellbeingid_production  --schema-only --superuser=meyouhealth --clean --create --schema=public --file=wbid-src-schema.sql
: 1487380252:0;rm Documentsavicat_sudo_debug.log
: 1487380279:0;mv wbid-src-schema.sql Documents/wbid/wbid-src-schema.sql
: 1487380287:0;cd wbid
: 1487380302:0;diff wbid-src-schema.sql wbid-src-existing-schema.sql
: 1487380336:0;diff -u wbid-src-schema.sql wbid-src-existing-schema.sql
: 1487380462:0;scp wbid-src-schema.sql ubuntu@172.16.1.117:
: 1487382589:0;history | grep shipit | tail -10
: 1487382597:0;history | grep shipit | tail -20
: 1487382668:0;shipit env config_list walkadoo-production > shipit-config-get-walkadoo-production-02172017-b4-switch.txt
: 1487383486:0;scp ubuntu@172.16.1.117:/home/ubuntu/db/postgres/wbid-target-schema.sql  wbid-target-schema.sql
: 1487418300:0;history | grep shipit | grep walkadoo-production | grep config_set | tail -3
: 1487418446:0;grep DATA shipit-config-get-walkadoo-production-02172017-b4-switch.txt
: 1487419685:0;shipit env config_set walkadoo-production DATABASE_URL mysql2://walkadoo_prd_app:tje7mnEOkp77k5@walkadoo-production3.cduvhvvpw6oo.us-east-1.rds.amazonaws.com/walkadoo_production?reconnect=true&encoding=utf8mb48&pool=32
: 1487419700:0;shipit env config_set walkadoo-production DATABASE_URL "mysql2://walkadoo_prd_app:tje7mnEOkp77k5@walkadoo-production3.cduvhvvpw6oo.us-east-1.rds.amazonaws.com/walkadoo_production?reconnect=true&encoding=utf8mb48&pool=32"
: 1487419719:0;shipit env config_set walkadoo-production PASSIVE_DATABASE_HOST walkadoo-production3-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com
: 1487419776:0;shipit env config_get walkadoo-production  > shipit-config-get-walkadoo-production-021817-new-vars-b4-deploy-n-switch.txt
: 1487419895:0;shipit env config_set walkadoo-production DATABASE_URL "mysql2://walkadoo_prd_app:tje7mnEOkp77k5@walkadoo-production3.cduvhvvpw6oo.us-east-1.rds.amazonaws.com/walkadoo_production?reconnect=true&encoding=utf8mb4&pool=32"
: 1487419902:0;shipit env config_list walkadoo-production  > shipit-config-get-walkadoo-production-021817-new-vars-b4-deploy-n-switch.txt
: 1487419907:0;grep DATA shipit-config-get-walkadoo-production-021817-new-vars-b4-deploy-n-switch.txt
: 1487419949:0;diff -u shipit-config-get-walkadoo-production-02172017-b4-switch.txt shipit-config-get-walkadoo-production-021817-new-vars-b4-deploy-n-switch.txt
: 1487420084:0;cd ../walkadoo
: 1487420136:0;history | grep walkadoo-production2.cduvhvvpw6oo.us-east-1.rds.amazonaws.com
: 1487420183:0;ssh -A -L 9601:127.0.0.1:9601 ubuntu@54.152.155.24 ssh -L 9601:walkadoo-production2.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1487420821:0;ssh -A -L 9602:127.0.0.1:9602 ubuntu@54.152.155.24 ssh -L 9602:walkadoo-production2-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3307 -N heidischmidt@10.0.14.114
: 1487420853:0;mysql -u meyouhealth -P 9602 -p -h 127.0.0.1 -vvv -D walkadoo_production -A
: 1487425703:0;vi wd-3
: 1487425734:0;vi wd-2
: 1487425860:0;grep tinyint wd-2 wd-3
: 1487426155:0;more mysqldump_schema.sh
: 1487426286:0;mysqldump -u meyouhealth -P 9601 -p -h 127.0.0.1 --no-data --hex-blob --databases walkadoo_production > wd-2-schema.sql
: 1487426307:0;mysqldump -u meyouhealth -P 9601 -h 127.0.0.1 --no-data --hex-blob --databases walkadoo_production > wd-2-schema.sql
: 1487426365:0;more wd-2-schema.sql
: 1487426374:0;mysqldump -u meyouhealth -p -P 9601 -h 127.0.0.1 --no-data --hex-blob --databases walkadoo_production > wd-2-schema.sql
: 1487426433:0;scp wd-2-schema.sql ubuntu@172.16.1.117:
: 1487428099:0;mysql -u meyouhealth -P 9601 -p -h 127.0.0.1 -vvv -D walkadoo_production -A
: 1487428142:0;mysql -u meyouhealth -P 9601 -h 127.0.0.1 -vvv -D walkadoo_production -A
: 1487428163:0;ssh -A -L 9603:127.0.0.1:9603 ubuntu@54.152.155.24 ssh -L 9603:walkadoo-production2.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1487428173:0;mysql -u meyouhealth -P 9603 -h 127.0.0.1 -vvv -D walkadoo_production -A
: 1487428181:0;mysql -u meyouhealth -p -P 9603 -h 127.0.0.1 -vvv -D walkadoo_production -A
: 1487428828:0;grep -i id *.sql 
: 1487428912:0;cp wd-set-autoincrement-from-what-is-in-innodb.sql new-wd-set-autoincrement-from-what-is-in-innodb.sql
: 1487428915:0;vi new-wd-set-autoincrement-from-what-is-in-innodb.sql
: 1487429070:0;cat new-wd-set-autoincrement-from-what-is-in-innodb.sql| sed s/\|//g
: 1487429076:0;cat new-wd-set-autoincrement-from-what-is-in-innodb.sql| sed s/\|//g > new-id.sql 
: 1487429104:0;cat new-id.sql | sed s/UNSIGNED//g
: 1487429117:0;cat new-id.sql | sed s/UNSIGNED//g | sed s/unsigned//g
: 1487429125:0;cat new-id.sql | sed s/UNSIGNED//g | sed s/unsigned//g > wd-new-id.sql 
: 1487429128:0;vi wd-new-id.sql
: 1487429158:0;scp wd-new-id.sql ubuntu@172.16.1.117:
: 1487430128:0;shipit env config_set walkadoo-production PASSIVE_DATABASE_HOST walkadoo-production2-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com
: 1487430166:0;shipit env config_set walkadoo-production DATABASE_URL "mysql2://walkadoo_prd_app:tje7mnEOkp77k5@walkadoo-production2.cduvhvvpw6oo.us-east-1.rds.amazonaws.com/walkadoo_production?reconnect=true&encoding=utf8&pool=32"
: 1487430192:0;shipit env config_list walkadoo-production  > shipit-config-get-walkadoo-production-021817-rollback.txt
: 1487430202:0;grep DATABASE shipit-config-get-walkadoo-production-021817-rollback.txt
: 1487432176:0;scp ubuntu@172.16.1.117:/home/ubuntu/db/postgres/diff* .
: 1487432185:0;scp ubuntu@172.16.1.117:/home/ubuntu/diff* .
: 1487432242:0;scp ubuntu@172.16.1.117:diff-output .
: 1487432247:0;more diff-output
: 1487432275:0;more diff-output | grep -v "utf8"
: 1487432305:0;more diff-output | grep -v "utf8" | grep -v " int(11)"
: 1487432991:0;grepCONCATE *.sql
: 1487432998:0;grep CONCAT *.sql
: 1487433653:0;vi tinyint-chgs-list
: 1487433721:0;cat tinyint-chgs-list | awk -F "|" '{print $3}'
: 1487433733:0;cat tinyint-chgs-list | awk -F "|" '{print $3, " ", $4}'
: 1487433747:0;cat tinyint-chgs-list | awk -F "|" '{print $3, " ", $4, " ", $4}'
: 1487433778:0;cat tinyint-chgs-list | awk -F "|" '{print $3, " ", $4, " ", $4, " ", $16}'
: 1487433782:0;cat tinyint-chgs-list | awk -F "|" '{print $3, " ", $4, " ", $4, " ", $17}'
: 1487433890:0;cat tinyint-chgs-list | awk -F "|" '{print $3, " ", $4, " ", $4, " ", $17, " ", $5}'
: 1487433909:0;cat tinyint-chgs-list | awk -F "|" '{print $3, " ", $4, " ", $4, " ", $17, " ", $6}'
: 1487433915:0;cat tinyint-chgs-list | awk -F "|" '{print $3, " ", $4, " ", $4, " ", $17, " ", $7}'
: 1487434025:0;cat tinyint-chgs-list | awk -F "|" '{print $3, " ", $4, " ", $4, " ", $17, " DEFAULT  ", $7}'
: 1487434055:0;cat tinyint-chgs-list | awk -F "|" '{" ALTER TABLE  ", $4, " ", $4, " ", $17, " DEFAULT  ", $7}'
: 1487434074:0;cat tinyint-chgs-list | awk -F "|" '{ print " ALTER TABLE  ", $4, " ", $4, " ", $17, " DEFAULT  ", $7}'
: 1487434213:0;cat tinyint-chgs-list | awk -F "|" '{ print " ALTER TABLE  ", $4, " ", $4, " ", $17, " DEFAULT  ", $7}' > tinyint.txt
: 1487434471:0;more tinyint.txt
: 1487434552:0;cat tinyint-chgs-list
: 1487434582:0;cat tinyint-chgs-list | awk -F "|" '{ print " ALTER TABLE  ", $4, " ", $5, " ", $17, " DEFAULT  ", $7}' 
: 1487434609:0;cat tinyint-chgs-list | awk -F "|" '{ print " ALTER TABLE  ", $4, " ", $5, " ", $17, " DEFAULT  ", $7}' > tinyint.txt
: 1487435221:0;cat tinyint.txt
: 1487435867:0;vi tinyint.txt
: 1487436188:0;history | grep 9945
: 1487436217:0;telnet walkadoo-production3.us-east-1.rds.amazonaws.com 3306
: 1487436455:0;grep -v "255" wd-2-to-wd3-differences
: 1487438778:0;more wd-2-to-wd3-differences
: 1487438887:0;cd wd-dms-cdc-chgs
: 1487438893:0;more xaa
: 1487439204:0;cd wd-dms-from-scratch
: 1487439218:0;cat wd-alters-all-ucs2-to-utf8mb4.sql
: 1487439272:0;more wd-set-autoincrement-from-what-is-in-innodb.sql
: 1487441093:0;vi aws-dms-cdc-logs-02112017-truncate-reload.txt
: 1487441137:0;grep -i failed aws-dms-cdc-logs-02112017-truncate-reload.txt
: 1487441174:0;vi aws-dms-cdc-logs-02112017-truncate-reload-next.txt
: 1487441276:0;vi aws-dms-cdc-logs-02112017-truncate-reload-next-after-that.txt
: 1487441312:0;vi aws-dms-cdc-logs-02112017-truncate-reload-next-after-that-1.txt
: 1487441337:0;vi aws-dms-cdc-logs-02112017-truncate-reload-next-after-that-2.txt
: 1487441390:0;vi aws-dms-cdc-logs-resume-after-pause-021417.txt
: 1487441449:0;vi aws-dms-cdc-logs-02112017-next-error-after-pause-021417.txt
: 1487442274:0;vi aws-dms-cdc-logs-020617-err.log
: 1487442316:0;vi aws-dms-cdc-logs-020717-err.log
: 1487442373:0;vi aws-dms-cdc-logs-020817-start-of-streaming-insert-errors.log
: 1487442387:0;mv aws-dms-cdc-logs-020617-err.log aws-dms-cdc-logs-020617-err.txt
: 1487442396:0;mv aws-dms-cdc-logs-020717-err.log aws-dms-cdc-logs-020717-err.txt
: 1487442406:0;mv aws-dms-cdc-logs-020817-start-of-streaming-insert-errors.log aws-dms-cdc-logs-020817-start-of-streaming-insert-errors.txt
: 1487442451:0;ls -lrtra
: 1487442498:0;more aws-dms-cdc-logs-020717-err.txt
: 1487442707:0;mv aws-dms-cdc-logs-020717-err.txt aws-dms-cdc-logs-020717-mediumtext-errors.txt
: 1487442714:0;more aws-dms-cdc-logs-020817-start-of-streaming-insert-errors.txt
: 1487442929:0;more aws-dms-cdc-logs-02112017-truncate-reload.txt
: 1487442944:0;more aws-dms-cdc-logs-02112017-truncate-reload-next.txt
: 1487603024:0;deis logs
: 1487603131:0;history | grep hello200-min
: 1487603162:0;history | grep hello200-minikube-dev
: 1487603232:0;kubectl --namespace deis get pods
: 1487605884:0;cd Documents/minikube
: 1487605949:0;kubectl context current-config
: 1487606038:0;history | more
: 1487606078:0;ls -l *hi*
: 1487606286:0;history | grep kops | grep kuber
: 1487606299:0;kops update cluster heidi.kubernetes.myhstg.com --yes
: 1487606354:0;kops validate
: 1487606399:0;kops validate --name heidi.kubernetes.myhstg.com 
: 1487606681:0;shipit deployment logs 4892
: 1487606892:0;nslookup 203.0.113.123
: 1487606945:0;kubectl cluster --help
: 1487607081:0;kubectl clusterinfo
: 1487607547:0;kubectl dashboard --help
: 1487607675:0;kubectl get pods --all-namespaces  | grep dash
: 1487607783:0;mkdir kubernetes
: 1487607802:0;wget https://rawgit.com/kubernetes/dashboard/master/src/deploy/kubernetes-dashboard.yaml
: 1487607813:0;vi kubernetes-dashboard.yaml
: 1487608113:0;kubectl create -f https://rawgit.com/kubernetes/dashboard/master/src/deploy/kubernetes-dashboard.yaml
: 1487608285:0;nslookup api.heidi.kubernetes.myhstg.com
: 1487608506:0;helm --help 
: 1487608632:0;helm init --help
: 1487608659:0;helm init 
: 1487608748:0;helm repo list 
: 1487608800:0;helm repo update 
: 1487608825:0;helm install deis/workflow --namespace deis
: 1487609848:0;deis register --help
: 1487609875:0;deis register deis.heidi.myhstg.com 
: 1487609918:0;deis login --help
: 1487609937:0;deis login deis.heidi.myhstg.com --username=heidischmidt
: 1487616366:0;heml --help
: 1487616409:0;nslookup deis.heidi.myhstg.com
: 1487616426:0;kubectl get pods --namespace deis -w
: 1487616443:0;kubectl get pods --namespace deis 
: 1487616469:0;kubectl get namespaces --all-namespaces
: 1487616474:0;kubectl get namespaces 
: 1487616533:0;helm delete --help
: 1487616720:0;helm delete --dry-run geared-tarsier --kube-context heidi.kubernetes.myhstg.com --debug 
: 1487616766:0;helm delete  geared-tarsier --kube-context heidi.kubernetes.myhstg.com --debug 
: 1487617053:0;kops validate cluster heidi.kubernetes.myhstg.com
: 1487617131:0;kops delete cluster --name heidi.kubernetes.myhstg.com -v 
: 1487617143:0;kops delete cluster --name heidi.kubernetes.myhstg.com -v debug 
: 1487617153:0;kops delete cluster --name heidi.kubernetes.myhstg.com 
: 1487617207:0;kops delete cluster --name heidi.kubernetes.myhstg.com --yes
: 1487617459:0;more ~/.kube/config
: 1487617503:0;kubectl config delete-context --help
: 1487617513:0;kubectl config delete-context heidi.kubernetes.myhstg.com
: 1487617549:0;grep heidi /Users/heidischmidt/.kube/config
: 1487617581:0;kubectl get-contexts --help
: 1487617629:0;kubectl config set-context minikube
: 1487617684:0;kubectl config unset --help
: 1487617705:0;kubectl config use-context minikube
: 1487617853:0;kubectl -version
: 1487618126:0;kubectl get-contexts
: 1487618501:0;kill -9 13449
: 1487618503:0;ps -ef | grep kubectl
: 1487618962:0;kubectl --namespace prometheus get pods -l app=prometheus
: 1487619645:0;kubectl config list
: 1487620161:0;kubectl get nodes --help
: 1487620189:0;kubectl get services
: 1487620194:0;kubectl get services --help
: 1487620527:0;kubectl logs hello200-staging-jobs-13731140-cvq7r
: 1487620557:0;kubectl logs -c hello200-staging-jobs-13731140-cvq7r
: 1487620634:0;kubectl logs hello200-staging-twilio-2131200236-kpkey
: 1487620645:0;kubectl logs hello200-staging-twilio-2131200236-kpkey --namespace hello200-staging
: 1487620698:0;kubectl logs hello200-staging-web-3667914858-jcig0 --namespace hello200-staging
: 1487620753:0;kubectl logs deis-registry-proxy-kgrxx --namespace deis
: 1487621170:0;kubectl delete pod deis-registry-proxy-nthq5 --namespace deis 
: 1487621357:0;kubectl delete pod deis-registry-proxy-n41ug --namespace deis 
: 1487621424:0;kubectl get pods --all-namespaces | grep Out
: 1487621434:0;kubectl get pods --all-namespaces | grep Out | tr
: 1487621445:0;kubectl get pods --all-namespaces | grep Out | tr -s " " 
: 1487621464:0;kubectl get pods --all-namespaces | grep Out | tr -s " "  | cut -f2 -d " "
: 1487621495:0;kubectl get pods --all-namespaces | grep Out | tr -s " "  | cut -f2 -d " " | xargs kubectl delete pod --namespace deis 
: 1487621511:0;kubectl get pods --all-namespaces | grep deis-registry-proxy
: 1487621547:0;kubectl describe pod deis-registry-proxy-6sdt5
: 1487621558:0;kubectl describe pod deis-registry-proxy-6sdt5 --namespace deis
: 1487621736:0;kubectl describe pod deis-registry-proxy-7hgjx --namespace deis
: 1487621785:0;kubectl describe pod deis-registry-proxy-d4gqh --namespace deis
: 1487622342:0;kubectl get pods --all-namespaces  | grep controller
: 1487622638:0;kubectl describe pod deis-controller-2828047466-lmtfw
: 1487622646:0;kubectl describe pod deis-controller-2828047466-lmtfw --namespace deis
: 1487690684:0;brew list | grep py
: 1487690698:0;brew install pyenv
: 1487690832:0;pyenv install 3.5.2
: 1487691356:0;kill -9 493 
: 1487693062:0;echo "[FIRING:2] DeploymentReplicasUnavailable (kube_deployment_status_replicas_unavailable kube-state-metrics 100.96.9.24:8080 kubernetes-service-endpoints kube-state-metrics prometheus deis page)" | wc -c 
: 1487693086:0;echo "[RESOLVED] DeploymentReplicasUnavailable (kube_deployment_status_replicas_unavailable kube-state-metrics 100.96.9.24:8080 kubernetes-service-endpoints kube-state-metrics prometheus deis page)" | wc -c 
: 1487693103:0;echo "[FIRING:12] DeploymentReplicasUnavailable (kube_deployment_status_replicas_unavailable kube-state-metrics 100.96.9.24:8080 kubernetes-service-endpoints kube-state-metrics prometheus page)" | wc -c
: 1487693149:0;echo "[FIRING:1] DeploymentReplicasUnavailable (kube_deployment_status_replicas_unavailable kube-state-metrics concierge-production-jobs 100.96.9.24:8080 kubernetes-service-endpoints kube-state-metrics prometheus concierge-production page)" | wc -c
: 1487695471:0;history | grep git
: 1487695492:0;;gdit
: 1487695503:0;git remotes
: 1487695665:0;deis keys
: 1487696721:0;history | grep pod 
: 1487696745:0;kubectl describe pod  alertmanager-1169564396-5oke4  --namespace deis
: 1487696753:0;kubectl get pods --all-namespaces | grep alert
: 1487697381:0;kubectl --namespace prometheus exec -ti alertmanager-1169564396-5oke4 bash 
: 1487697419:0;kubectl --namespace prometheus exec -ti alertmanager-1169564396-5oke4 bash
: 1487697434:0;kubectl --namespace prometheus exec -ti alertmanager-1169564396-5oke4 sh
: 1487697836:0;kubectl --namespace prometheus exec -ti alertmanager-1169564396-5oke4 sh cat /etc/alertmanager/config.ym
: 1487701505:0;kubectl describe pod alertmanager-1169564396-5oke4Âƒ€--namespace prometheus
: 1487701525:0;kubectl describe pod  alertmanager-1169564396-5oke4  --namespace prometheus
: 1487701578:0;kubectl describe nodes ip-172-20-53-13.ec2.internal/172.20.53.13
: 1487701629:0;kubectl describe nodes  | grep 100.96.9.24
: 1487701641:0;kubectl describe nodes  | grep 100.96
: 1487701775:0;kubectl describe all pods 
: 1487701784:0;kubectl describe all pods --all-namespaces
: 1487701794:0;kubectl describe all pods --namespace prometheus
: 1487701831:0;kubectl describe pods --namespace prometheus
: 1487701855:0;kubectl describe pods --namespace prometheus | grep 100
: 1487701870:0;kubectl describe pods --all-namespaces | grep 100
: 1487701926:0;kubectl describe pods --all-namespaces > kubectl-describe-pods-all-namespaces.log
: 1487702156:0;head -100 kubectl-describe-pods-all-namespaces.log
: 1487702175:0;head -100 kubectl-describe-pods-all-namespaces.log | grep IP:
: 1487702184:0;vi kubectl-describe-pods-all-namespaces.log
: 1487702241:0;grep "IP:" kubectl-describe-pods-all-namespaces.log | grep 100.96.9
: 1487702246:0;grep "IP:" kubectl-describe-pods-all-namespaces.log | more
: 1487702467:0;kube-controller-manager --help
: 1487702515:0;kubectl --namespace prometheus exec -ti alertmanager-1169564396-5oke4 sh 
: 1487702626:0;ssh admin@ip-172-20-117-217.ec2.internal
: 1487705974:0;vi pagerduty-alert-k8s.txt
: 1487706129:0;cat pagerduty-alert-k8s.txt
: 1487706225:0;cp pagerduty-alert-k8s.txt pagerduty-alert-k8s-modded-cpy.txt
: 1487706231:0;vi pagerduty-alert-k8s-modded-cpy.txt
: 1487706389:0;Description": "[RESOLVED] DeploymentReplicasUnavailable (kube_deployment_status_replicas_unavailable kube-state-metrics 100.96.9.24:8080 kubernetes-service-endpoints kube-state-metrics prometheus page)",\
  "details": {\
    "firing": "",\
    "num_firing": "0",\
    "num_resolved": "12",\
    "resolved": "Labels:\n
: 1487706400:0;cat pagerduty-alert-k8s-modded-cpy.txt
: 1487728152:0;history | grep pods
: 1487728396:0;kill -9 
: 1487728495:0;kubectl get pods --all-namespaces | grep 
: 1487728497:0;iris-staging-clockwork-462506624
: 1487728507:0;kubectl get pods --all-namespaces | grep iris-staging-clockwork-462506624
: 1487728526:0;kubectl get pods --namespaces iris-staging
: 1487728532:0;kubectl get pods --namespace iris-staging
: 1487778080:0;more pagerduty-alert-k8s-modded-cpy.txt
: 1487778121:0;grep deis-monitor-grafana kubectl-describe-pods-all-namespaces.log
: 1487778178:0;kubectl get pods --all-namespaces | grep grafana
: 1487779269:0;kubectl describe nodes > kubectl-describe-nodes.log
: 1487862831:0;kubectl use-context k8s.meyouhealth.com
: 1487866223:0;shipit deployment logs 4906 > wbid-prd-deploy-fail-4906.log
: 1487866240:0;shipit deployment logs 4905 > wbid-stg-deploy-4905-failed.log
: 1487866247:0;more wbid-prd-deploy-fail-4906.log
: 1487866291:0;shipit deployment logs 4906
: 1487866985:0;history | grep wellbeingid
: 1487867122:0;psql -E -h 127.0.0.1 -p 9710 -U wellbeingid_production_app  --dbname=wellbeingid_production
: 1487867182:0;cd Documents/wbid
: 1487867185:0;ls -l *dms*
: 1487867190:0;more src-wbid-aws-dms-objects-drop.sql
: 1487867343:0;shipit deployment logs 4905
: 1487867426:0;shipit deployment logs 4906 | more
: 1487867446:0;shipit deployment logs 4906 
: 1487867539:0;curl http://repos.sensuapp.org/apt/pubkey.gpg
: 1487867578:0;shipit create --help
: 1487867616:0;shipit deployment create wellbeingid-production df3291454d517b2f795554db962025eed992855a 
: 1487868119:0;shipit deployment logs 4908
: 1487868570:0;shipit deployment create wellbeingid-production df3291454d517b2f795554db962025eed992855a
: 1487868794:0;shipit deployment create wellbeingid-staging df3291454d517b2f795554db962025eed992855a
: 1487868810:0;shipit deployment logs 4910
: 1487868960:0;shipit deployment logs 4911
: 1487868984:0;shipit env show wellbeingid-staging 
: 1487869458:0;shipit deployment logs 4909
: 1487869672:0;ssh heidischmidt@bdb9a226bd984f81b45f6494b35c42dcbc8492e1
: 1487870597:0;grep version metadata.rb
: 1487870633:0;git add Berksfile.lock 
: 1487870639:0;git add metadata.rb
: 1487872671:0;history | grep -i berksfile
: 1487872920:0;berks version
: 1487873055:0;git add Berksfile.lock
: 1487873127:0;berks install 
: 1487873430:0;vi metadata.rb
: 1487873670:0;more Berksfile
: 1487873790:0;history > history 
: 1487873793:0;vi history
: 1487873811:0;rm history
: 1487873838:0;berks show chef-dailychallenge
: 1487874265:0;vi Berksfile.lock
: 1487874674:0;berks update
: 1487874742:0;berks upload
: 1487874789:0;berks list 
: 1487874805:0;berks --help
: 1487874877:0;berks list
: 1487875031:0;cd ../dc
: 1487875045:0;vi chef-json-b4-fix.log
: 1487875059:0;vi chef-json-after-fix.log
: 1487875108:0;diff -u chef-json-b4-fix.log chef-json-after-fix.log
: 1487880777:0;aws dms --help
: 1487880798:0;aws dms describe-replication-tasks help
: 1487880834:0;aws dms describe-replication-tasks  > ../wbid/ams-dms-replication-tasks.txt
: 1487880850:0;more ams-dms-replication-tasks.txt
: 1487880863:0;mv ams-dms-replication-tasks.txt aws-dms-replication-tasks.txt
: 1487880868:0;cat aws-dms-replication-tasks.txt
: 1487880913:0;vi aws-dms-replication-tasks.txt
: 1487880929:0;more aws-dms-replication-tasks.txt
: 1487880951:0;vi wbid-migration-dms-task.txt
: 1487881272:0;more wbid-migration-dms-task.txt
: 1487952955:0;shipit env show wellbringtracker-staging
: 1487953018:0;shipit deployment logs 4730
: 1487953069:0;shipit env show wellbeingtracker-staging
: 1487953643:0;shipit env show wellbeingtracker-production
: 1487954987:0;kubernetes config current-context
: 1487955021:0;deis releases --help
: 1487955035:0;deis releases list wbt-production
: 1487955041:0;deis releases list --help
: 1487955046:0;deis releases list -a wbt-production
: 1487955066:0;deis releases:list --app=wbt-production
: 1487955301:0;deis | grep git
: 1487956363:0;kubectl get pods -a wbt-production
: 1487956532:0;history | grep deis | grep hello200
: 1487957566:0;deis logs --help
: 1487957575:0;deis logs -a wbt-production
: 1487957585:0;deis logs -a wbt-production | grep -i hook
: 1487960698:0;kubectl config use-context --help
: 1487960712:0;deis | grep login 
: 1487960720:0;history | grep deis | grep login
: 1487965067:0;history | tail -2000
: 1487965317:0;kubectl describe pod deis-builder-2759337600-3ysik --namespace deis
: 1487965356:0;kubectl describe pods deis-builder-2759337600-3ysik --namespace deis
: 1487965431:0;kubectl describe pods --help
: 1487965444:0;kubectl describe pods deis
: 1487965481:0;history | grep kubectl | grep pod
: 1487965491:0;kubectl describe pods --namespace deis
: 1487965545:0;kubectl describe pods --namespace deis > Documents/kubernetes/kubectl-describe-pods-namespace-deis.022417.log
: 1487965624:0;ls -l kops
: 1487965628:0;mkdir kops 
: 1487965643:0;history | grep kops > kops-history.log
: 1487965651:0;mv kops-history.log kops/
: 1487965672:0;more get_ip_mysql_hostlist.sh
: 1487965689:0;cd deis-k8s
: 1487965825:0;more kubectl-describe-pods-namespace-deis.022417.log
: 1487965840:0;mv kubectl-describe-pods-namespace-deis.022417.log prod-kubectl-describe-pods-namespace-deis.022417.log
: 1487965932:0;history | grep 100
: 1487965952:0;vi prod-kubectl-describe-pods-namespace-deis.022417.log
: 1487965975:0;head -100 prod-kubectl-describe-pods-namespace-deis.022417.log
: 1487966150:0;ssh admin@:
: 1487966157:0;ssh admin@ip-172-24-115-71.ec2.internal 
: 1487966510:0;history | grep "100" 
: 1487966528:0;history | grep "ssh admin"
: 1487966665:0;vi kubectl-describe-nodes.log
: 1487966736:0;more kubectl-describe-nodes.log
: 1487966786:0;ssh admin@ip-172-20-119-5.ec2.internal
: 1487967595:0;history | grep wbt-staging
: 1487967998:0;kubectl get pods --namespace wbt-production
: 1487968185:0;history | grep deis | grep run
: 1487968208:0;deis logs deis-builder-2759337600-3ysik
: 1487968270:0;deis apps:logs --help 
: 1487968327:0;kubectl get pods --namespace deis
: 1487968338:0;kubectl --namespace=deis logs deis-builder-2759337600-3ysik
: 1487968433:0;kubectl --namespace=deis logs deis-builder-2759337600-3ysik > kubectl-namespace-deis-logs-deis-builder-2759337600-3ysik-troubleshooting-wbt-production.log
: 1487968446:0;vi kubectl-namespace-deis-logs-deis-builder-2759337600-3ysik-troubleshooting-wbt-production.log
: 1487974658:0;history | grep kubectl | grep wbt-staging
: 1487974744:0;kubectl --namespace wbt-staging logs wbt-staging-jobs-3272182220-cyrx2
: 1487974969:0;kill -9 5178
: 1487975168:0;kubectl --namespace wbt-staging attach  wbt-staging-jobs-3272182220-cyrx2
: 1487975189:0;kubectl --namespace wbt-staging attach  --help
: 1487975219:0;kubectl attach wbt-staging-jobs-3272182220-cyrx2 -i -t
: 1487975230:0;kubectl attach wbt-staging-jobs -i -t
: 1487975242:0;history | grep kubectl| grep attach
: 1487975432:0;kubectl get pods --namespace wbt-staging
: 1487975508:0;kubectl --namespace wbt-staging logs wbt-staging-jobs-3272182220-ivft0
: 1487975614:0;~/Documents/kubernetes â€¹2.2.0â€º  $ kubectl get pods --namespace wbt-staging
: 1487975614:0;NAME                                                     READY     STATUS    RESTARTS   AGE
: 1487975614:0;wbt-staging-clockwork-3261633104-yaozf                   1/1       Running   0          1h
: 1487975614:0;wbt-staging-conciergelistener-1395236712-uhorc           1/1       Running   0          1h
: 1487975614:0;wbt-staging-jobs-3272182220-ivft0                        1/1       Running   0          1m
: 1487975614:0;wbt-staging-wbidaccountchangelistener-4153322640-5g66i   1/1       Running   0          1h
: 1487975614:0;wbt-staging-wbidaccountdisablelistener-541371976-z13ca   1/1       Running   0          1h
: 1487975614:0;wbt-staging-web-1458566386-bptv7                         1/1       Running   1          1h
: 1487975614:0;wbt-staging-web-1458566386-zhpin                         1/1       Running   0          1h
: 1487975614:0;~/Documents/kubernetes â€¹2.2.0â€º  $ wbt-staging-jobs-3272182220-cyrx2
: 1487975614:0;~/Documents/kubernetes â€¹2.2.0â€º  $ kubectl --namespace wbt-staging logs wbt-staging-jobs-3272182220-ivft0                                    ~/Documents/                                                                          1 âƒ¦µ
: 1487975614:0;2017-02-24T22:29:20.757Z 1 TID-grb9tfml0 DEBUG: ReliablePush activated
: 1487977002:0;aws s3 website help
: 1487977021:0;aws s3 ls myh-deis-staging-builder/home
: 1487977034:0;aws s3 ls myh-deis-staging-builder/home/*
: 1487977037:0;aws s3 ls myh-deis-staging-builder/home/
: 1488224027:0;shipit deployment logs 4907
: 1488224525:0;kubetctl config current-context
: 1488224642:0;kill -9 40831
: 1488296535:0;more aws-dms-cdc-logs-02112017-next-error-after-pause-021417.txt
: 1488296654:0;more aws-dms-cdc-logs-020617-err.txt
: 1488296782:0;more aws-dms-cdc-logs-020717-mediumtext-errors.txt
: 1488304072:0;history | grep namespaces | grep get
: 1488304134:0;more prod-kubectl-describe-pods-namespace-deis.022417.log | grep deis-builder
: 1488308930:0;ping 10.0.14.225 80
: 1488310070:0;docker pull https://quay.io/repository/myhadmin/quitnet-server/image/27555d135c60b7b13cfa9423633a7176b49a85eeb57cd321632ebcc8ed799a9a
: 1488310536:0;ssh heidischmidt@10.0.14.225
: 1488310646:0;ssh ubuntu@10.0.5.195
: 1488310706:0;ssh heidischmidt@10.0.5.195
: 1488310962:0;history | grep ssh | grep jump
: 1488310966:0;history | grep ssh | grep 58
: 1488311076:0;docker pull quay.io/repository/myhadmin/quitnet-server/image/27555d135c60b7b13cfa9423633a7176b49a85eeb57cd321632ebcc8ed799a9a
: 1488311236:0;cat .ssh/config
: 1488311295:0;ssh heidischmidt@54.152.155.24
: 1488311454:0;ssh ubuntu@54.152.155.24
: 1488317737:0;ssh heidischmidt@10.0.14.224
: 1488317764:0;ssh -t -A heidischmidt@54.152.155.24 ssh -t heidischmidt@10.0.5.195
: 1488317774:0;ssh -t -A heidischmidt@54.152.155.24 ssh -t heidischmidt@10.0.14.224
: 1488317826:0;ssh -t -A heidischmidt@54.152.155.24 ssh -t heidischmidt@10.0.14.225
: 1488317876:0;shipit deployment logs 4918
: 1488333520:0;shipit ssh walkado-production
: 1488383197:0;which mysqldbcompare 
: 1488383212:0;locate mysqldbcompare
: 1488383319:0;ssh admin@172.16.1.117
: 1488383408:0;brew install mysqqldbcompare
: 1488383424:0;brew install mysqldbcompare
: 1488383539:0;brew install mysql-utilities
: 1488383608:0;brew cask install mysql-utilities
: 1488383624:0;brew cask install Caskroom/cask/mysql-utilities
: 1488383648:0;brew tap caskroom/cask
: 1488383649:0;brew install brew-cask
: 1488383763:0;brew install brew-cask-completion
: 1488383785:0;brew cask install mysqlworkbench
: 1488383957:0;which mysqldbcompare
: 1488384021:0;brew search mysql
: 1488384038:0;brew search python
: 1488384283:0;brew install Caskroom/cask/mysql-utilities
: 1488385959:0;history | grep deis | grep register
: 1488393144:0;deis auth:register https://deis.meyouhealth.com --username=antares.meketa password=HEvIAThETEloseCT
: 1488393198:0;deis auth:register https://deis.meyouhealth.com --username=antares.meketa --password=HEvIAThETEloseCT
: 1488393223:0;more /Users/heidischmidt/.deis/client.json
: 1488393301:0;history | grep deis | grep login | grep meyouhealth
: 1488393319:0;deis auth:register https://deis.meyouhealth.com --username=collin.waid --password=raHaTerNaTeRbIAL
: 1488393344:0;cat /Users/heidischmidt/.deis/client.json
: 1488393360:0;deis auth:register https://deis.meyouhealth.com --username=nate.cobb --password=SuaCyRIEctaINfeL
: 1488393394:0;deis auth:register https://deis.meyouhealth.com --username=simran.fitzgerald --password=ATeNIndAyuGHTMar
: 1488393425:0;deis auth:register https://deis.meyouhealth.com --username=stephen.sopp --password=DRATicepleaLSelI
: 1488393460:0;history | grep deis | grep login | grep myhstg
: 1488393478:0;deis auth:register https://deis.myhstg.com --username=antares.meketa --password=AWAinfoRitcHevel
: 1488393523:0;deis auth:register https://deis.myhstg.com --username=collin.waid --password=iagERVaDErGANces
: 1488393552:0;deis auth:register https://deis.myhstg.com --username=nate.cobb --password=NcISEYBRiAntAtiC
: 1488393582:0;deis auth:register https://deis.myhstg.com --username=simran.fitzgerald --password=PawLaGaLEcerLYMc
: 1488393639:0;deis auth:register https://deis.myhstg.com --username=stephen.sopp --password=LivErmuSenSeDIan
: 1488393709:0;for i in `deis apps | grep staging | awk '{print $1}' `\
do \
deis perms:create stephen.sopp --app $i\
done
: 1488393740:0;for i in `deis apps | grep staging | awk '{print $1}' `\
do \
deis perms:create simran.fitzgerald --app $i\
done
: 1488393758:0;for i in `deis apps | grep staging | awk '{print $1}' `\
do \
deis perms:create nate.cobb --app $i\
done
: 1488393779:0;for i in `deis apps | grep staging | awk '{print $1}' `\
do \
deis perms:create collin.waid --app $i\
done
: 1488393791:0;for i in `deis apps | grep staging | awk '{print $1}' `\
do \
deis perms:create antares.meketa --app $i\
done
: 1488393877:0;for i in `deis apps | grep production | awk '{print $1}' `\
do \
deis perms:create antares.meketa --app $i\
done
: 1488393893:0;for i in `deis apps | grep production | awk '{print $1}' `\
do \
deis perms:create collin.waid --app $i\
done
: 1488393911:0;for i in `deis apps | grep production | awk '{print $1}' `\
do \
deis perms:create nate.cobb --app $i\
done
: 1488393929:0;for i in `deis apps | grep production | awk '{print $1}' `\
do \
deis perms:create simran.fitzgerald --app $i\
done
: 1488393943:0;for i in `deis apps | grep production | awk '{print $1}' `\
do \
deis perms:create stephen.sopp --app $i\
done
: 1488394293:0;aws s3 ls myh-kops
: 1488394361:0;deis --version
: 1488394412:0;brew search deis
: 1488396262:0;history | grep waid
: 1488396305:0;deis auth:register https://deis.meyouhealth.com --username=collin.waid --password=r3daCtrNaTeRbIAL
: 1488396325:0;deis user --help
: 1488396332:0;deis delete --help
: 1488396468:0;for i in `deis apps | grep production | awk '{print $1}' `\
do \
deis perms:delete collin.waid --app $i\
done
: 1488396808:0;deis users | sort
: 1488396825:0;deis users | sed s/*//
: 1488396834:0;deis users > list
: 1488396849:0;cat list | sort
: 1488398548:0;telnet walkadoo-production3.cduvhvvpw6oo.us-east-1.rds.amazonaws.com 3306
: 1488398713:0;history | grep ssh | grep walkadoo-production2
: 1488466397:0;ssh -t -A heidischmidt@52.86.252.208 ssh -t heidischmidt@10.0.17.71
: 1488476817:0;history | grep ssh | grep walkadoo-production
: 1488476856:0;ssh -A -L 9613:127.0.0.1:9603 ubuntu@54.152.155.24 ssh -L 9613:wd-prd-to-wd-prd3-utf8mb4.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1488476874:0;more mysqldump_schema.sh 
: 1488476945:0;ssh -A -L 9613:127.0.0.1:9613 ubuntu@54.152.155.24 ssh -L 9613:wd-prd-to-wd-prd3-utf8mb4.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1488476963:0;ssh -A -L 9614:127.0.0.1:9614 ubuntu@54.152.155.24 ssh -L 9614:wd-prd-to-wd-prd3-utf8mb4.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1488476974:0;sh -x mysqldump_schema.sh 9614 
: 1488477082:0;/usr/bin/time mysqldump -u meyouhealth -p -P 9614 -h 127.0.0.1 -v --no-data --hex-blob --databases walkadoo_production > wd-prd-to-wd-prd3-utf8mb4-schema-dump.sql 
: 1488477342:0;history | grep kubectl | grep names
: 1488477351:0;kubectl describe pods --namespace wilder-production
: 1488477620:0;kubectl --namespace wilder-production logs wilder-production-web-1660262328-b6dm1 
: 1488477652:0;kubectl --namespace wilder-production logs wilder-production-web-1660262328-b6dm1
: 1488477708:0;ping wilder.meyouhealth.com
: 1488477719:0;ping walkadoo.meyouhealth.com
: 1488477981:0;history | grep wilder
: 1488477992:0;ssh -A -L 3309:localhost:3309 ubuntu@54.152.155.24 ssh -L 3309:wilder-production2.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N ubuntu@10.0.14.26
: 1488478009:0;history | grep mysql | grep 3309
: 1488478032:0;mysql -u meyouhealth -P 3309 -h 127.0.0.1 
: 1488478098:0;history | grep kubectl | grep attach
: 1488478119:0;history | grep wilder | grep kubectl 
: 1488478158:0;kubectl --namespace wilder-production attach wilder-production-jobs-881798290-tqrp4
: 1488478199:0;kubectl attach --namespace wilder-production  wilder-production-jobs-881798290-tqrp4  -i -t 
: 1488478254:0;history | grep kubectl | grep attach 
: 1488478276:0;kubectl attach wilder-production-jobs -i -t
: 1488478288:0;kubectl --help 
: 1488478362:0;kubectl attach POD -c wilder-production-jobs-881798290-drply -i -t 
: 1488478373:0;kubectl attach -c wilder-production-jobs-881798290-drply -i -t 
: 1488478390:0;kubectl attach POD wilder-production-jobs-881798290-drply -c wilder-production-jobs-881798290-drply -i -t 
: 1488478431:0;kubectl attach POD 881798290-drply -c wilder-production-jobs -i -t 
: 1488478446:0;kubectl attach POD  -c wilder-production-jobs -i -t 
: 1488478456:0;kubectl attach 881798290-drply -c wilder-production-jobs -i -t 
: 1488478468:0;kubectl attach wilder-production-jobs-881798290-drply -c wilder-production-jobs -i -t 
: 1488478577:0;kubectl help
: 1488478596:0;kubectl attach --help
: 1488478625:0;kubectl attach wilder-production-jobs-881798290-drply
: 1488479151:0;kubectl attach 291581129-3ug1r
: 1488479167:0;kubectl attach wilder-production-jobs-291581129-3ug1r
: 1488479175:0;kubectl attach wilder-production-jobs-291581129-3ug1r --namespace wilder-production
: 1488479246:0;kubectl --namespace wilder-production logs wilder-production-jobs-881798290-drply  
: 1488479279:0;kubectl logs wilder-production-jobs-881798290-drply --namespace wilder-production
: 1488479357:0;history | grep kubectl | grep replica 
: 1488479437:0;kubectl get rc --help
: 1488479451:0;kubectl get rc,services
: 1488479467:0;kubectl get rc --namespace wilder-production
: 1488480616:0;kubectl attach wilder-production-jobs-291581129-3ug1r -i 
: 1488480630:0;kubectl attach wilder-production-jobs-291581129-3ug1r --namespace wilder-production -i
: 1488480649:0;kubectl attach -h 
: 1488480665:0;kubectl attach wilder-production-jobs-291581129-3ug1r --namespace wilder-production -i -t
: 1488486551:0;history | grep wilder | grep ssh 
: 1488486756:0;ssh -A -L 3319:localhost:3319 ubuntu@54.152.155.24 ssh -L 3319:wilder-production2.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N ubuntu@10.159.160.235
: 1488486768:0;ssh ubuntu@10.159.160.235
: 1488486797:0;history | grep 54.242.121.10
: 1488486868:0;history | grep mysql
: 1488488131:0;shipit deployment logs 4925
: 1488555632:0;shipit deployment logs 4926
: 1488556062:0;history | grep walkadoo-production2
: 1488556118:0;ssh -A -L 9611:127.0.0.1:9611 ubuntu@54.152.155.24 ssh -L 9611:walkadoo-production2.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1488556164:0;mysql -u meyouhealth -p -P 9611 -h 127.0.0.1 -vvv -D walkadoo_production -A
: 1488557807:0;shipit env config_list walkadoo-staging | grep DATA
: 1488567607:0;shipit deployment logs 4931
: 1488816199:0;cle
: 1488819525:0;kubectl get logs --namespace wbt-production 
: 1488819605:0;kubectl --namespace wbt-production logs wbt-production-conciergelistener-4129177412-3dhzw
: 1488819783:0;kubectl --namespace wbt-production logs wbt-production-conciergelistener-4129177412-jnlty
: 1488819792:0;kubectl get pods --all-namespaces | grep wbt-production-conciergelistener
: 1488823768:0;aws elasticache --help
: 1488823800:0;describe-cache-clusters help
: 1488823804:0;aws describe-cache-clusters help
: 1488823818:0;aws elasticache describe-cache-clusters help
: 1488823841:0;aws elasticache describe-cache-clusters --cache-cluster-id wd-prod-redis-001
: 1488824581:0;aws elasticache describe-replication-groups help
: 1488824589:0;aws elasticache describe-replication-groups 
: 1488824618:0;ls -l redis
: 1488824637:0;ls -l aws*
: 1488824646:0;cd aws_cli_output
: 1488824670:0;aws elasticache describe-replication-groups > aws-elasticache-desc-replication-groups-03062017.txt
: 1488824715:0;head -750 aws-elasticache-desc-replication-groups-03062017.txt
: 1488825191:0;aws elasticache help | grep desc
: 1488825216:0;aws elasticache describe-cache-clusters > aws-elasticache-describe-cache-clusters-03062017.txt
: 1488825251:0;cat aws-elasticache-describe-cache-clusters-03062017.txt | grep -B 10 Engine
: 1488825272:0;cat aws-elasticache-describe-cache-clusters-03062017.txt | grep -B 15 EngineVersion 
: 1488825287:0;cat aws-elasticache-describe-cache-clusters-03062017.txt | grep -B 3 EngineVersion 
: 1488825499:0;cat aws-elasticache-describe-cache-clusters-03062017.txt | grep  EngineVersion | \
more
: 1488825516:0;cat aws-elasticache-describe-cache-clusters-03062017.txt | grep  EngineVersion | sort | more
: 1488825524:0;cat aws-elasticache-describe-cache-clusters-03062017.txt | grep  EngineVersion | sort | uniq -c 
: 1488826038:0;more aws-elasticache-describe-cache-clusters-03062017.txt
: 1488826139:0;aws elasticache help | grep read
: 1488826143:0;aws elasticache help | grep replica
: 1488828579:0;shipit deployment logs 4949
: 1488831040:0;vi aws-elasticache-describe-cache-clusters-03062017.txt
: 1488831059:0;grep -i CurrentRole aws-elasticache-describe-cache-clusters-03062017.txt
: 1488831095:0;grep CurrentRole aws-elasticache-desc-replication-groups-03062017.txt
: 1488831104:0;vi aws-elasticache-desc-replication-groups-03062017.txt
: 1488831200:0;head -810 aws-elasticache-desc-replication-groups-03062017.txt
: 1488831344:0;aws elasticache help | grep cluster
: 1488835769:0;kubectl --namespace wilder-production logs wilder-production-web-1986304368-thqtl
: 1488835799:0;kubectl get pods --all-namespaces | head -1
: 1488835803:0;kubectl get pods --all-namespaces | grep wilder
: 1488897134:0;kubectl get pods --all-namespaces | grep walka
: 1488897410:0;history | grep deis | grep rel
: 1488897419:0;deis releases -a iris-production
: 1488897469:0;history | grep nodes
: 1488897502:0;kubectl describe nodes > kubectl-describe-nodes-030717.log
: 1488897689:0;vi kubectl-describe-nodes-030717.log
: 1488897778:0;history | grep deis |grep login
: 1488901818:0;kubectl describe nodes > kubectl-describe-nodes-staging-030717.log
: 1488901830:0;vi kubectl-describe-nodes-staging-030717.log
: 1488903999:0;which helm
: 1488904053:0;which tiller
: 1488904978:0;more kubectl-describe-nodes-staging-030717.log
: 1488904987:0;more kubectl-describe-nodes-staging-030717.log | grep controller
: 1488905853:0;helm get values
: 1488905890:0;helm get values 2.8.1
: 1488906126:0;helm upgrade --help
: 1488906226:0;helm get values workflow-2.8.1
: 1488906849:0;kubectl describe nodes > kubectl-describe-nodes-staging-after-upgrade-helm-deis-030717-1214pm.log
: 1488907542:0;deis releases hello200-staging
: 1488907549:0;deis releases -a hello200-staging
: 1488908044:0;kubectl annotate --help
: 1488908156:0;history | grep kubectl | grep get
: 1488908452:0;history | grep deis | grep login | grep heidi
: 1488908517:0;helm get jobs
: 1488908523:0;kubectl get jobs
: 1488908556:0;kubectl get pods --all-namespaces
: 1488908682:0;kubectl describe nodes > kubectl-describe-nodes-prod-during-upgrade.log
: 1488908700:0;history | grep inspect
: 1488914105:0;helm inspect deis/workflow
: 1488914159:0;helm status deis-workflow
: 1488917223:0;kops --version
: 1488917370:0;kops edit --help
: 1488917398:0;kops validate cluser
: 1488918915:0;kubectl config all-contexts
: 1488918977:0;env | grep KOP
: 1488919604:0;history | grep kops | grep export
: 1488919672:0;history | grep deis | grep login 
: 1488936441:0;shipi ssh deployment-api
: 1489074974:0;df -Ph 
: 1489075174:0;kops export iwrty.k8s.myhstg.com
: 1489075238:0;kops export --name iwrty.k8s.myhstg.com
: 1489075324:0;kops export kubecfg 
: 1489075353:0;vi .kube
: 1489075360:0;cd .kube
: 1489075453:0;grep ery .kube/config
: 1489075461:0;kops delete --help
: 1489075467:0;kops delete cluster erytu.k8s.myhstg.com
: 1489075487:0;kops delete cluster --name erytu.k8s.myhstg.com
: 1489075500:0;vi config
: 1489075542:0;kops export --help
: 1489075554:0;kops export kubecfg --help
: 1489075570:0;kops export kubecfg iwrty.k8s.myhstg.com
: 1489076619:0;kubectl get --all-namespaces
: 1489077161:0;kubectl create cluster --help
: 1489077182:0;kops create cluster --help
: 1489078443:0;kops delete cluster --name iwrty.k8s.myhstg.com
: 1489078464:0;kubectl config --help
: 1489078504:0;kubectl delete-context erytu.k8s.myhstg.com
: 1489078515:0;kubectl config delete-context erytu.k8s.myhstg.com
: 1489078586:0;kubectl config delete-context iwrty.k8s.myhstg.com
: 1489078612:0;kubectl config set-context lrywh.k8s.myhstg.com
: 1489078629:0;history | grep kubectl | grep export
: 1489078640:0;history | grep kubectl | grep iwtry
: 1489078648:0;history | grep iwrty
: 1489089632:0;shipit deployment logs 4971
: 1489089740:0;shipit create deployment --help
: 1489089755:0;shipit deployment --help
: 1489089776:0;shipit deployment create --help
: 1489089827:0;shipit create walkadoo-production 54500e28ec9e7159c4d1c5416d1c9b3940b2f38e
: 1489089840:0;history | grep shipit | grep walkadoo
: 1489089917:0;shipit deployment create walkadoo-production 54500e28ec9e7159c4d1c5416d1c9b3940b2f38e
: 1489090308:0;shipit env config-get walkadoo-production | grep -i new
: 1489090321:0;shipit env config-get walkadoo-production 
: 1489090331:0;shipit env config-get --help
: 1489090351:0;shipit config-list walkadoo-production
: 1489090360:0;shipit --help
: 1489090376:0;shipit help config-list
: 1489090380:0;shipit help config-get
: 1489090410:0;shipit env config_list walkadoo-production
: 1489090449:0;shipit env config_list walkadoo-production | grep KEY
: 1489090501:0;shipit env config_list wellbeingid-production | grep KEY
: 1489090516:0;shipit env config_list wellbeingid-production | grep -i new
: 1489090533:0;shipit env config_list wellbeingtracker-production | grep KEY
: 1489090576:0;shipit deployment logs 4978
: 1489160132:0;jobs
: 1489163606:0;kubectl describe nodes --help
: 1489163655:0;kubectl describe nodes ip-172-20-221-156.ec2.internal
: 1489163831:0;kubectl get pods --selector=app=cassandra rc -o \\
: 1490281823:0;  jsonpath='{.items[*].metadata.labels.version}'
: 1489163858:0;kubectl get pods --selector=app=kube-system rc -o  jsonpath='{.items[*].metadata.labels.version}'
: 1489163882:0;kubectl get pods --selector=app=iris-staging rc -o  jsonpath='{.items[*].metadata.labels.version}'
: 1489164708:0;history | grep json
: 1489164882:0;kubectl get nodes -o jsonpath="{.items[*].status.addresses[?(@.type=="externalID")].address}"
: 1489170728:0;history | grep shared-p
: 1489170760:0;history | grep shared-p | grep post
: 1489170926:0;more pg_grants_hello200.sql
: 1489170990:0;cp pg_grants_hello200.sql pg_grants_deis-staging-new.sql
: 1489171243:0;vi pg_grants_deis-staging-new.sql
: 1489171409:0;cat pg_grants_deis-staging-new.sql
: 1489172801:0;helm status
: 1489172821:0;helm status newbie-elephant
: 1489174061:0;helm status deis-workflow-ywhet
: 1489174137:0;kubectl get -all-namespaces
: 1489174142:0;kubectl get pod
: 1489174163:0;kubectl --all-namespaces get pods
: 1489174169:0;kubectl --all-namespaces 
: 1489174176:0;kubectl get pods --all-namespaces 
: 1489175555:0;kubectl attach deis-registry-proxy-423q7 -i  
: 1489175570:0;kubectl attach deis-registry-proxy-423q7 -i  --namespace deis
: 1489175608:0;kubectl attach alertmanager-1953001165-nb1j1 -i  --namespace prometheus
: 1489175651:0;kubectl top pod POD_NAME --containers
: 1489175673:0;kubectl top pod newrelic-agent-9g7q7 --containers
: 1489175692:0;kubectl top pod alertmanager-1953001165-nb1j1 --containers
: 1489175721:0;kubectl top pod calico-node-1w3jm --containers
: 1489175817:0;kubectl get ds
: 1489175847:0;kubectl get rs
: 1489175945:0;kubectl cluster-info dump --output-directory=~/Documents/kubernetes/current-cluster-state
: 1489175953:0;ls ~/Documents/kubernetes/current-cluster-state
: 1489175964:0;kubectl cluster-info dump --output-directory=current-cluster-state
: 1489175986:0;kubectl cluster-info --help
: 1489176122:0;kubectl cluster-info dump --help
: 1489176166:0;kubectl cluster-info dump --all-namespaces --output-directory=~/Documents/
: 1489176180:0;ls -l ~/Documents
: 1489176199:0;kubectl cluster-info dump --namespaces default,kube-system
: 1489176295:0;kubectl cluster-info dump --all-namespaces > cluster-state-dump-all.txt
: 1489176317:0;vi errors-kubectl-cluster-dump
: 1489176334:0;more cluster-state-dump-all.txt
: 1489177763:0;deis ps:list dashiell
: 1489177777:0;deis ps:list -a dashiell
: 1489262483:0;more /etc/resolve.conf
: 1489262675:0;cat .kube/config
: 1489262839:0;history | grep cluster
: 1489263544:0;ssh admin@172.24.115.71
: 1489263810:0;ssh admin@172.24.60.122
: 1489264537:0;vi issues
: 1489264709:0;grep out list
: 1489264746:0;grep out list | sed s/out//
: 1489265124:0;ssh admin@172.24.96.253
: 1489265222:0;ssh admin@172.24.66.124
: 1489265265:0;ssh admin@172.24.34.107
: 1489265310:0;ssh admin@172.24.82.78
: 1489269163:0;kubectl get pods -o wide  --nmespace deis
: 1489269222:0;kubectl get pods --namespace wilder-production
: 1489269445:0;history | grep kubectl | grep logs 
: 1489269517:0;history | grep deis | grep logs
: 1489269538:0;deis logs -a deis deis-router-2099956932-aep4z
: 1489269706:0;kubectl edit deployment deis-router --namespace deis
: 1489270114:0;kubectl get pods -o wide  --all-namespaces > list
: 1489270133:0;grep -i termin list
: 1489270323:0;kubectl get pods -o wide  --all-namespaces > kube-pods-list1.log
: 1489270334:0;date >> kube-pods-list1.log
: 1489270337:0;more kube-pods-list1.log
: 1489270472:0;watch kubectl get pods -o wide  --all-namespaces
: 1489270658:0;kubectl get pods -o wide  --all-namespaces | grep error
: 1489270670:0;kubectl get pods -o wide  --all-namespaces | grep -i error
: 1489271259:0;history | grep kubectl | grep logs
: 1489271279:0;kubectl --namespace hello200-production logs hello200-production-web-3352543205-tw89e
: 1489271356:0;kubectl --namespace kube-system logs kubernetes-dashboard-3203700628-zpacv
: 1489271486:0;kubectl --namespace hello200-production logs hello200-production-jobs-759649471-obt3x
: 1489272214:0;kubectl --namespace hello200-production logs hello200-production-wbidaccountchangelistener-440956803-sdvcj
: 1489272280:0;more kubectl-describe-pods-all-namespaces.log
: 1489272322:0;kubectl describe pods --all-namespaces > prod-kubectl-desc-all-pods-all-namespaces.log
: 1489272351:0;cd kubectl 
: 1489272375:0;kubectl get pods -o wide  --all-namespaces > prod-kube-get-all-pods-all-namespaces.log
: 1489272396:0;kubectl --namespace hello200-production logs hello200-production-wbidaccountchangelistener-440956803-sdvcj > prod-h200-logs-wbid-tar-issue.log
: 1489272403:0;more prod-kubectl-describe-pods-namespace-deis.022417.log
: 1489272412:0;more prod-kubectl-desc-all-pods-all-namespaces.log
: 1489272462:0;grep -B 10 kubelet prod-kubectl-desc-all-pods-all-namespaces.log
: 1489272471:0;grep -B 20 kubelet prod-kubectl-desc-all-pods-all-namespaces.log
: 1489272686:0;kubectl get pods -o wide  --all-namespaces > prod-kube-get-all-pods-all-namespaces-2.log
: 1489272692:0;more prod-kube-get-all-pods-all-namespaces-2.log
: 1489272729:0;head prod-kube-get-all-pods-all-namespaces-2.log
: 1489272748:0;head -1 prod-kube-get-all-pods-all-namespaces-2.log
: 1489272752:0;more prod-kube-get-all-pods-all-namespaces-2.log | grep m 
: 1489272980:0;kubectl delete pod --help
: 1489272990:0;kubectl delete pod kubernetes-dashboard-3203700628-zpacv
: 1489272998:0;kubectl delete pod kubernetes-dashboard-3203700628-zpacv 
: 1489273002:0;more prod-kube-get-all-pods-all-namespaces-2.log | grep dash
: 1489273014:0;kubectl delete pod kubernetes-dashboard-3203700628-zpacv --namespace kube-system
: 1489273030:0;kubectl get pods -o wide  --all-namespaces > prod-kube-get-all-pods-all-namespaces-3.log
: 1489273036:0;grep dash prod-kube-get-all-pods-all-namespaces-3.log
: 1489273067:0;kubectl --namespace kube-system logs kubernetes-dashboard-3203700628-w0q1u
: 1489273475:0;kubectl --namespace deis logs deis-monitor-telegraf-48173
: 1489273703:0;kubectl --namespace deis logs deis-monitor-telegraf-fbcjf
: 1489274482:0;kubectl --namespace deis logs deis-monitor-telegraf-f5gwr
: 1489274531:0;kubectl get pods -o wide  --all-namespaces | egrep -wi --color 'CrashLoopBackOff|error|ContainerCreating' 
: 1489274615:0;kubectl get pods -o wide  --all-namespaces > kubectl-get-pods-o-wide-all-namespaces-after-kerfluffle.log
: 1489274625:0;date >> kubectl-get-pods-o-wide-all-namespaces-after-kerfluffle.log
: 1489275225:0;kubectl describe pods --all-namespaces > prod-kubectl-desc-all-pods-all-namespaces-after.log
: 1489275287:0;more ls -l *pods*
: 1489275293:0;ls -l *pods*
: 1489275307:0;grep deis prod-kube-get-all-pods-all-namespaces-2.log
: 1489275316:0;grep deis prod-kube-get-all-pods-all-namespaces-2.log | grep registry
: 1489275463:0;grep deis prod-kube-get-all-pods-all-namespaces-2.log | grep router
: 1489275891:0;kubectl get pods -o wide  --all-namespaces > kubectl-get-pods-o-wide-all-namespaces-after-kerfluffle-1.log
: 1489276041:0;kubectl get pods -o wide  --all-namespaces | head 
: 1489276147:0;kubectl get pods -o wide  --all-namespaces | grep deis | grep registry
: 1489276163:0;kubectl --namespace deis logs deis-registry-4027102140-1iz9h
: 1489276251:0;kubectl --namespace deis logs deis-registry-4027102140-gihkl
: 1489276381:0;kubectl --namespace iris-production logs iris-production-web-559501739-0gvy9
: 1489276477:0;kubectl --namespace iris-production logs iris-production-web-559501739-3mjol
: 1489276523:0;kubectl --namespace iris-production logs iris-production-wbidaccountchangeslistener-1286540053-c4uio
: 1489276538:0;kubectl --namespace iris-production logs iris-production-jobs-2719868534-bd37d
: 1489279576:0;kubectl --namespace deis logs deis-logger-176328999-vydk2
: 1489279630:0;kubectl delete pod deis-logger-176328999-vydk2 --namespace deis
: 1489279658:0;kubectl --namespace deis logs deis-logger-176328999-oh3fg
: 1489279710:0;kubectl --namespace deis logs deis-router-2099956932-aep4z
: 1489279751:0;kubectl --namespace deis logs deis-router-2099956932-aep4z | ;logcheck
: 1489279796:0;kubectl --namespace deis logs deis-router-2099956932-aep4z | egrep -wi --color 'error|503|499|warning'
: 1489279849:0;kubectl --namespace deis logs deis-router-2099956932-aep4z | egrep -wi --color 'error|503|499|warning|Connection reset by peer'
: 1489279898:0;kubectl --namespace deis logs deis-router-2099956932-aep4z | egrep -wi --color ' 503 | 499 |Connection timed out) while connecting to upstream|Connection reset by peer'
: 1489279932:0;kubectl --namespace deis logs deis-router-2099956932-aep4z | egrep -wi --color ' 503 | 499 |Connection timed out|Connection reset by peer'
: 1489282273:0;kubectl get pods -o wide  --all-namespaces --show-labels | wc -l 
: 1489282380:0;kubectl get pods -o wide  --all-namespaces --show-labels | grep "ip-172-24-121-242.ec2.internal"
: 1489282419:0;kubectl get pods -o wide  --all-namespaces --show-labels | grep -v "ip-172-24-121-242.ec2.internal"
: 1489282460:0;kubectl get pods -o wide  --all-namespaces --show-labels | grep -v "ip-172-24-121-242.ec2.internal" | grep -v kube-system 
: 1489282472:0;kubectl get pods -o wide  --all-namespaces --show-labels | grep -v "ip-172-24-121-242.ec2.internal" | grep -v kube-system | grep -v default
: 1489282491:0;kubectl get pods -o wide  --all-namespaces --show-labels | grep -v "ip-172-24-121-242.ec2.internal" | grep -v kube-system | grep -v default | grep -v deis
: 1489282497:0;kubectl get pods -o wide  --all-namespaces --show-labels | grep -v "ip-172-24-121-242.ec2.internal" | grep -v kube-system | grep -v default 
: 1489284031:0;kubectl get pods -o wide  --all-namespaces --show-labels | grep -v "ip-172-24-121-242.ec2.internal" 
: 1489284055:0;kubectl get pods -o wide  --all-namespaces --show-labels | grep  "ip-172-24-121-242.ec2.internal" 
: 1489284061:0;kubectl get pods -o wide  --all-namespaces --show-labels | grep  "ip-172-24-121-242.ec2.internal"  | wc -l 
: 1489284193:0;kubectl get pods -o wide  --all-namespaces --show-labels > kube-get-pods-all-namespaces-show-labels-during-reboot.log
: 1489284250:0;kubectl get pods -o wide  --all-namespaces --show-labels > kube-get-pods-all-namespaces-show-labels-during-reboot-1.log
: 1489284259:0;more kube-get-pods-all-namespaces-show-labels-during-reboot-1.log
: 1489284373:0;kubectl get pods -o wide  --all-namespaces --show-labels > kube-get-pods-all-namespaces-show-labels-during-reboot-2.log
: 1489284380:0;more kube-get-pods-all-namespaces-show-labels-during-reboot-2.log
: 1489284404:0;grep -i completed kube-get-pods-all-namespaces-show-labels-during-reboot.log
: 1489284409:0;grep -i completed kube-get-pods-all-namespaces-show-labels-during-reboot.log | wc -l 
: 1489284415:0;grep -i completed kube-get-pods-all-namespaces-show-labels-during-reboot-1.log | wc -l 
: 1489284420:0;grep -i completed kube-get-pods-all-namespaces-show-labels-during-reboot-2.log | wc -l 
: 1489284428:0;kubectl get pods -o wide  --all-namespaces --show-labels > kube-get-pods-all-namespaces-show-labels-during-reboot-3.log
: 1489284435:0;grep -i completed kube-get-pods-all-namespaces-show-labels-during-reboot-3.log | wc -l 
: 1489284465:0;kubectl get pods -o wide  --all-namespaces --show-labels > kube-get-pods-all-namespaces-show-labels-during-reboot-4.log
: 1489284472:0;grep -i completed kube-get-pods-all-namespaces-show-labels-during-reboot-4.log | wc -l 
: 1489284485:0;kubectl get pods -o wide  --all-namespaces --show-labels > kube-get-pods-all-namespaces-show-labels-during-reboot-5.log
: 1489284498:0;grep -i completed kube-get-pods-all-namespaces-show-labels-during-reboot-5.log | wc -l 
: 1489284518:0;grep ip-172-24-121-242.ec2.internal kube-get-pods-all-namespaces-show-labels-during-reboot-5.log
: 1489284537:0;default                   newrelic-agent-63qdq                                            0/1       Pending             0          6s        <none>          ip-172-24-121-242.ec2.internal   name=newrelic
: 1489284537:0;deis                      deis-logger-fluentd-3y64o                                       0/1       Pending             0          6s        <none>          ip-172-24-121-242.ec2.internal   app=deis-logger-fluentd,heritage=deis
: 1489284537:0;deis                      deis-monitor-telegraf-7xfm2                                     0/1       Pending             0          4s        <none>          ip-172-24-121-242.ec2.internal   app=deis-monitor-telegraf
: 1489284564:0;kubectl get pods -o wide  --all-namespaces --show-labels > kube-get-pods-all-namespaces-show-labels-after-reboot.log
: 1489284569:0;more kube-get-pods-all-namespaces-show-labels-after-reboot.log
: 1489284604:0;ssh admin@172.24.121.242
: 1489284679:0;ssh admin@172.24.110.158
: 1489285594:0;vi list
: 1489285602:0;cat list
: 1489286279:0;kubectl get pods -o wide  --namespace iris-production
: 1489286298:0;kubectl describe nodes -o 
: 1489286354:0;kubectl describe nodes > kubectl-desc-nodes-prod-after-redistribute.lgo
: 1489286365:0;mv kubectl-desc-nodes-prod-after-redistribute.lgo kubectl-desc-nodes-prod-after-redistribute.log
: 1489286388:0;kubectl get nodes > kubectl-get-nodes.log
: 1489286416:0;vi kubectl-desc-nodes-prod-after-redistribute.log
: 1489287779:0;more kubectl-desc-nodes-prod-after-redistribute.log
: 1489287835:0;more kubectl-desc-nodes-prod.log
: 1489287917:0;kubectl get events > kubectl-get-events
: 1489296633:0;ssh heidischmidt@10.159.160.235
: 1489296688:0;history | grep daily
: 1489296725:0;hsitory | grep ssh | grep dc
: 1489296820:0;ssh heidischmidt@ec2-54-242-121-10.compute-1.amazonaws.com 
: 1489296845:0;ssh heidischmidt@ec2-54-242-121-10.compute-1.amazonaws.com
: 1489296901:0;ssh -t -A heidischmidt@52.86.252.208 ssh -t heidischmidt@10.159.160.235
: 1489299586:0;history | grep -i berks
: 1489299602:0;berks verify
: 1489417489:0;ssh -A -L 9612:127.0.0.1:9612 ubuntu@54.152.155.24 ssh -L 9612:walkadoo-production2.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@10.0.14.114
: 1489417557:0;history | grep mysql | grep 9611
: 1489417581:0;mysql -u meyouhealth -p -P 9612 -h 127.0.0.1 -vvv -D walkadoo_production -A 
: 1489423567:0;ssh -t -A heidischmidt@54.152.155.24 ssh -t heidischmidt@10.0.14.174
: 1489424118:0;curl -H "Host: https://dashiell.myhstg.com/widgets/release:daily_challenge"
: 1489424123:0;curl https://dashiell.myhstg.com/widgets/release:daily_challenge
: 1489424225:0;curl -k https://dashiell.myhstg.com/widgets/release:daily_challenge
: 1489424965:0;history | grep logs | grep kube
: 1489425036:0;history | grep kubectl | grep inspect
: 1489425081:0;history | grep ssh | grep 172
: 1489425138:0;ssh -t -A heidischmidt@54.152.155.24 ssh -t heidischmidt@10.0.14.114
: 1489425502:0;ssh admin@100.97.55.225
: 1489425715:0;kubectl exec --help
: 1489425736:0;kubectl exec -p dashiell-web-1768261142-vd3t7 -i -t 
: 1489425749:0;kubectl exec bash -p dashiell-web-1768261142-vd3t7 -i -t 
: 1489425782:0;kubectl exec bash -c dashiell-web -i -t 
: 1489425796:0;kubectl exec -c dashiell-web -i -t bash
: 1489425835:0;kubectl exec kubernetes-dashboard-3203831700-rp7w0 -i -t bash
: 1489425856:0;kubectl exec dashiell-web-1768261142-vd3t7 -i -t bash
: 1489425866:0;kubectl exec dashiell-web-1768261142-vd3t7 -i -t bash --namespace dashiell
: 1489426788:0;kubectl --namespace dashiell logs dashiell-web-1768261142-vd3t7
: 1489426811:0;curl -k https://dashiell.myhstg.com/
: 1489427510:0;ssh -t -A heidischmidt@54.152.155.24 ssh -t heidischmidt@10.0.14.73
: 1489431492:0;kubectl describe dashiell-web-1768261142-vd3t7
: 1489431501:0;kubectl describe --help
: 1489431537:0;kubectl describe pods --namespace dashiell
: 1489432013:0;kubectl get node3s
: 1489432045:0;kubectl get nodes -o
: 1489432154:0;kubectl get nodes -o json > ~/Documents/kubernetes/kubectl-get-nodes-o-json-output-capacity-lywrh-new-staging.log
: 1489432197:0;vi ~/Documents/kubernetes/kubectl-get-nodes-o-json-output-capacity-lywrh-new-staging.log
: 1489432262:0;grep -B 10 capacity ~/Documents/kubernetes/kubectl-get-nodes-o-json-output-capacity-lywrh-new-staging.log
: 1489432407:0;kubectl get rc
: 1489432604:0;kubectl top node ip-172-20-146-44.ec2.internal
: 1489432629:0;kubectl top node
: 1489432687:0;kubectl get pods
: 1489432706:0;kubectl top nodes
: 1489432717:0;kubectl top node --help
: 1489432722:0;kubectl top node ip-172-20-117-217.ec2.internal
: 1489433151:0;kubectl explain pods
: 1489478417:0;kubectl get pods -o wide  --all-namespacesd
: 1489478452:0;kubectl get pods -o wide  --all-namespaces| grep tiller
: 1489478458:0;kubectl get pods -o wide  --all-namespaces| grep dns
: 1489478486:0;kubectl get pods -o wide  --all-namespaces| grep deis-builder
: 1489478513:0;kubectl get event
: 1489478541:0;kubectl get pods -o wide  --all-namespaces| grep deis-logger
: 1489478650:0;history | grep kubectl | grep log
: 1489478711:0;kubectl --namespace kube-system logs tiller-deploy-3066893457-difv6 
: 1489478775:0;kubectl --namespace kube-system logs kube-dns-v20-3531996453-wcefi -c dnsmasq
: 1489478784:0;kubectl --namespace kube-system logs kube-dns-v20-3531996453-wcefi 
: 1489478837:0;kubectl logs kubedns
: 1489478848:0;kubectl logs kubedns --namespace kube-system
: 1489478859:0;kubectl logs -c kubedns --namespace kube-system
: 1489478876:0;kubectl logs kube-dns-v20-3531996453-wcefi -c kubedns --namespace kube-system
: 1489478909:0;kubectl logs deis-logger-redis-2637728276-dy2cp --namespace deis
: 1489498542:0;ps -ef | grep kube
: 1489498981:0;kubectl get pods --help
: 1489499138:0;kubectl get pods -a --all-namespaces | grp prom
: 1489499143:0;kubectl get pods -a --all-namespaces | grep prom
: 1489499217:0;kubectl get pods -a --namespace prometheus
: 1489588561:0;ssh -t -A heidischmidt@54.152.155.24 ssh -t heidischmidt@10.0.5.72
: 1489588627:0;history | grep console 
: 1489588652:0;shipit env update --console-host 10.0.5.72 deployment-api
: 1489588719:0;shipit env show deployment-api
: 1489591891:0;shipit deployment logs 4997
: 1489591960:0;shipit deployment create walkadoo-production e599a0c0759ff35532a37e8c15f4382d69177d46 
: 1489591977:0;shipit deployment logs 4998
: 1489592492:0;history | grep knife | grep node
: 1489592532:0;knife search node "chef_environment:walkadoo-production\
"
: 1489592547:0;knife search node "chef_environment:walkadoo-production"
: 1489592569:0;knife search node "chef_environment:walkadoo-production" > wd-chef-node-search-031417.log
: 1489592657:0;~ â€¹2.2.0â€º  $ knife search node "chef_environment:walkadoo-production" > wd-chef-node-search-031417.log
: 1489592657:0;6 items found
: 1489592657:0;~ â€¹2.2.0â€º  $ knife node delete -y i-0bd63534214745791
: 1489592657:0;Deleted node[i-0bd63534214745791]
: 1489592722:0;vi wd-chef-client-deploy-failure.log
: 1489592764:0;cat wd-chef-client-deploy-failure.log
: 1489592872:0;shipi deployment logs 4999
: 1489592892:0;knife node delete -y i-0bd63534214745791
: 1489592909:0;shipit deployment logs 4999
: 1489624046:0;history | grep shipit | grep console 
: 1489624076:0;shipit env update --console-host 10.0.14.156 deployment-api
: 1489624903:0;ssh -t -A heidischmidt@54.152.155.24 ssh -t heidischmidt@10.0.14.43
: 1489625274:0;shipit env update --console-host 10.0.14.43 deployment-api
: 1489625513:0;history | grep shipit | grep deployment | grep walkadoo
: 1489625798:0;shipit deployment logs 5006
: 1489686961:0;kubectl config use-context
: 1489687028:0;shipit deployment logs 5016
: 1489687191:0;kubectl proxy &
: 1489687284:0;etcctl
: 1489687306:0;which etcdctl 
: 1489687313:0;brew search etcdctl
: 1489687341:0;deis tls --help
: 1489687379:0;deis tls -a concierge-production
: 1489687696:0;which journalctl
: 1489687744:0;cd ../kubectl 
: 1489687771:0;kubectl get nodes > kubectt-get-nodes-new-031617.log 
: 1489687806:0;kubectl describe nodes > kubectl-desc-nodes-new-031617.log 
: 1489687820:0;vi kubectl-desc-nodes-new-031617.log
: 1489688409:0;history | grep kubectl | grep exec 
: 1489688474:0;kubectl describe nodes
: 1489688520:0;history | grep event
: 1489688529:0;more kubectl-get-events
: 1489688936:0;kubectl get nodes --show-labels
: 1489689602:0;curl -L https://github.com/coreos/etcd/releases/download/v3.0.6/etcd-v3.0.6-darwin-amd64.zip -o etcd-v3.0.6-darwin-amd64.zip
: 1489689609:0;unzip etcd-v3.0.6-darwin-amd64.zip && cd etcd-v3.0.6-darwin-amd64
: 1489689617:0;ls-l | grep etc
: 1489689623:0;ls -l | grep etc
: 1489689634:0;./etcdctl --help
: 1489689660:0;./etcdctl cluster-health
: 1489717100:0;kubectl get pods -o wide  --all-namespaces | grep -i metrics
: 1489717122:0;kubectl get pods -o wide  --all-namespaces | head -2
: 1489718825:0;ssh admin@ip-172-24-113-60.ec2.internal
: 1489721078:0;kill -9 7820
: 1489763743:0;ps -ef | grep kubectl 
: 1489764233:0;ssh admin@172.20.95.12
: 1489764814:0;kubectl config use-contexts k8s.myhstg.com
: 1489764897:0;kubectl describe pod wbt-staging-jobs-105678929-j0cm6 --namespace wbt-staging
: 1489765100:0;kubectl describe node ip-172-20-95-12.ec2.internal -o wide
: 1489765118:0;kubectl describe nodes ip-172-20-95-12.ec2.internal
: 1489770751:0;kubectl --namespace deis logs deis-builder-3607535561-gorht 
: 1489772055:0;bundle show gemfury
: 1489772099:0;grep fury Gemfile.lock
: 1489772831:0;heml ls
: 1489773003:0;deis users --help
: 1489773010:0;deis --helps
: 1489773057:0;deis perms:list airflow-staging
: 1489773064:0;deis perms:list -a airflow-staging
: 1489773697:0;kubectl --namespace deis logs deis-builder-3607535561-gorht kubectl --namespace deis logs deis-builder-3607535561-gorht
: 1489773817:0;kubetcl config get-context
: 1489775420:0;kubectl get service
: 1489775445:0;kubectl get --namespace=deis
: 1489775455:0;kubectl get service --namespace=deis
: 1489775616:0;kubectl --namespace deis logs deis-builder-3607535561-gorht
: 1489775908:0;kubectl describe pods/deis-builder-3607535561-gorht --namespace deis  
: 1489776469:0;kubectl delete pod --help 
: 1489776490:0;kubectl delete pod deis-builder-3607535561-gorht --namespace deis
: 1489776581:0;~ â€¹2.1.5â€º  $ kubectl get pods -o wide  --all-namespaces | grep deis-builder
: 1489776581:0;deis                   deis-builder-3607535561-pw15g                                0/1       Running            0          23s       100.96.3.174     ip-172-20-53-13.ec2.internal
: 1489776581:0;~ â€¹2.1.5â€º  $  kubectl --namespace deis logs deis-builder-3607535561-pw15g
: 1489776581:0;2017/03/17 18:48:16 Starting health check server on port 8092
: 1489776581:0;2017/03/17 18:48:16 Starting deleted app cleaner
: 1489776581:0;2017/03/17 18:48:16 Starting SSH server on 0.0.0.0:2223
: 1489776581:0;Listening on 0.0.0.0:2223
: 1489776581:0;Accepting new connections.
: 1489776626:0;kubect describe pod kubectl --namespace deis logs deis-builder-3607535561-pw15g --namespace deis
: 1489776632:0;kubectl describe pod kubectl --namespace deis logs deis-builder-3607535561-pw15g --namespace deis
: 1489776651:0;kubectl describe pod deis-builder-3607535561-pw15g --namespace deis
: 1489781461:0;kubectl --namespace deis logs deis-builder-3607535561-pw15g --namespace deis
: 1489782536:0;helm fetch deis/workflow --untar
: 1489782546:0;vim workflow/values.yaml
: 1489782599:0;helm lint workflow/
: 1489782744:0;sudo docker pull quay.io/deis/dockerbuilder:v2.6.0
: 1489782752:0;docker pull quay.io/deis/dockerbuilder:v2.6.0
: 1489783796:0;find deis-builder-rc.yaml -print
: 1489783849:0;more builder-service
: 1489783852:0;more builder-service.yaml
: 1489783864:0;grep 2.7 *
: 1489783947:0;cd ../../../
: 1489783953:0;helm fetch deis/deis
: 1489784216:0;cd charts
: 1489784220:0;cd builder
: 1489784435:0;kubectl --namespace prometheus get pods -l app=alertmanager -o name | sed 's/^.*\///' | xargs -I{} kubectl --namespace prometheus port-forward {} 9093:9093
: 1489784589:0;kubectl cluster-info
: 1489784623:0;kubectl proxy
: 1489785162:0;date >> b4-brew-upgrade.log
: 1489785185:0;helm version >> b4-brew-upgrade.log
: 1489785197:0;deis version >> b4-brew-upgrade.log
: 1489785260:0;brew info kubernetes
: 1489785278:0;brew list | grep kubernetes
: 1489785293:0;brew info kubernetes-helm >> b4-brew-upgrade.log
: 1489785324:0;brew info kubernetes-cli >> b4-brew-upgrade.log
: 1489785367:0;brew info deis >> b4-brew-upgrade.log
: 1490022795:0;kubectl get current-contexts
: 1490022816:0;kubectl config current-contexts
: 1490022822:0;kubectl config -h 
: 1490023024:0;ssh admin@ip-172-20-95-12.ec2.internal
: 1490023108:0;kubectl describe node ip-172-20-95-12.ec2.internal
: 1490023616:0;date >> ~/Documents/kubernetes/deis-logs-deis-builder-3607535561-pw15g.log
: 1490024264:0;vi kubectl-deis-desc-service-deis-router.log
: 1490024310:0;kubectl --namespace deis logs deis-builder-3607535561-pw15g
: 1490024699:0;kubectl get deployments
: 1490024709:0;kubectl get deployments --namespace deis
: 1490024714:0;kubectl get deployments --namespace deis -o wide 
: 1490024721:0;kubectl get deployments --namespace deis -o wide  --show-labels
: 1490024749:0;kubectl get deployments --help
: 1490024767:0;kubectl get deployments --namespace deis -o json  --show-labels
: 1490024801:0;kubectl get deployments --namespace deis -o json  --show-labels > kubectl-get-deployments-namespace-deis--o-json-show-labels.log
: 1490024815:0;grep -B 5 image kubectl-get-deployments-namespace-deis--o-json-show-labels.log
: 1490024828:0;grep -B 10 image kubectl-get-deployments-namespace-deis--o-json-show-labels.log
: 1490024856:0;grep  image kubectl-get-deployments-namespace-deis--o-json-show-labels.log
: 1490024880:0;mv kubectl-get-deployments-namespace-deis--o-json-show-labels.log kubectl-get-deployments-namespace-deis--o-json-show-labels-old-staging.log
: 1490024928:0;kubect config use-context lrywh.k8s.myhstg.com
: 1490024969:0;kubectl get deployments --namespace deis -o json  --show-labels > kubectl-get-deployments-namespace-deis--o-json-show-labels-new-staging-lrywh.log 
: 1490024980:0;grep image kubectl-get-deployments-namespace-deis--o-json-show-labels-old-staging.log
: 1490024988:0;grep image kubectl-get-deployments-namespace-deis--o-json-show-labels-old-staging.log | grep quay
: 1490024998:0;grep image kubectl-get-deployments-namespace-deis--o-json-show-labels-old-staging.log | grep quay > current-staging-deis-quay
: 1490025020:0;grep image kubectl-get-deployments-namespace-deis--o-json-show-labels-new-staging-lrywh.log | grep quay > new-staging-lyrwh-deis-quay
: 1490025030:0;paste current-staging-deis-quay new-staging-lyrwh-deis-quay
: 1490025059:0;helm ls > lyrwh-helm-ls
: 1490025094:0;helm ls 
: 1490025117:0;helm ls > k8s-myhstg-helm-ls
: 1490025174:0;kubectl get deployments --namespace deis -o json  --show-labels > kubectl-get-deployments-namespace-deis--o-json-show-labels-old-staging-2.log 
: 1490025182:0;diff kubectl-get-deployments-namespace-deis--o-json-show-labels-old-staging.log kubectl-get-deployments-namespace-deis--o-json-show-labels-old-staging-2.log
: 1490025370:0;kubectl get deployments --namespace deis -o json  --show-labels > kubectl-get-deployments-namespace-deis--o-json-show-labels-new-staging-lyrwh-2.log 
: 1490025393:0;diff kubectl-get-deployments-namespace-deis--o-json-show-labels-new-staging-lrywh.log kubectl-get-deployments-namespace-deis--o-json-show-labels-new-staging-lyrwh-2.log | grep image 
: 1490025438:0;diff kubectl-get-deployments-namespace-deis--o-json-show-labels-new-staging-lrywh.log kubectl-get-deployments-namespace-deis--o-json-show-labels-old-staging-2.log
: 1490025448:0;diff kubectl-get-deployments-namespace-deis--o-json-show-labels-new-staging-lrywh.log kubectl-get-deployments-namespace-deis--o-json-show-labels-old-staging-2.log | grep image | grep quy
: 1490025450:0;diff kubectl-get-deployments-namespace-deis--o-json-show-labels-new-staging-lrywh.log kubectl-get-deployments-namespace-deis--o-json-show-labels-old-staging-2.log | grep image | grep quay
: 1490025571:0;diff kubectl-get-deployments-namespace-deis--o-json-show-labels-new-staging-lrywh.log kubectl-get-deployments-namespace-deis--o-json-show-labels-new-staging-lyrwh-2.log 
: 1490025596:0;diff kubectl-get-deployments-namespace-deis--o-json-show-labels-new-staging-lrywh.log kubectl-get-deployments-namespace-deis--o-json-show-labels-new-staging-lyrwh-2.log | grep image | grep quay
: 1490025608:0;diff kubectl-get-deployments-namespace-deis--o-json-show-labels-old-staging.log kubectl-get-deployments-namespace-deis--o-json-show-labels-old-staging-2.log | grep image | grep quay
: 1490031267:0;kubectl get pods -o wide  --all-namespaces | grep tel
: 1490031285:0;kubectl delete --help
: 1490031331:0;kubectl delete pod deis-monitor-telegraf-e6nw5 --namespace deis 
: 1490031345:0;kubectl delete pod deis-monitor-telegraf-mvq3u --namespace deis 
: 1490031348:0;kubectl get pods -o wide  --all-namespaces | grep tel 
: 1490031587:0;kubectl --namespace deis describe service deis-router
: 1490031729:0;kubectl --namespace=deis annotate service/deis-router service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout=1200
: 1490031752:0;kubectl --namespace=deis annotate deployment/deis-router router.deis.io/nginx.bodySize=10M
: 1490031918:0;kubectl describe pod deis-builder-3977879498-7ywfg --namespace deis
: 1490032259:0;kubectl --namespace kube-system logs kube-dns-autoscaler-2813114833-q6oie
: 1490032271:0;kubectl --namespace kube-system logs kube-dns-v20-3531996453-wcefi
: 1490032287:0;kubectl --namespace kube-system logs kube-dns-v20-3531996453-y91gu
: 1490032555:0;kubectl --namespace deis describe pod deis-builder-3977879498-7ywfg
: 1490033084:0;kubectl --namespace deislogs deis-builder-3977879498-7ywfg
: 1490033089:0;kubectl --namespace deis logs deis-builder-3977879498-7ywfg
: 1490033609:0;ssh admin@ip-172-20-53-13.ec2.internal
: 1490033697:0;kubectl get pods -o wide  --all-namespaces | grep build
: 1490034067:0;for i in `deis apps | grep staging | awk '{print $1}' `\
do \
deis perms:create bolek.kurowski --app $i\
done
: 1490034083:0;kubectl --namespace deis describe pods deis-builder-3977879498-7ywfg
: 1490034574:0;kubectl get events --namespace deis --since=10m
: 1490034771:0;kubectl log kube-dns-pod -c kubedns
: 1490034880:0;kubectl --namespace kube-system logs kube-dns-v20-3531996453-y91gu  
: 1490035028:0;vi i-o-timeout-noted.txt
: 1490035350:0;kubectl --namespace kube-system logs kube-dns-v20-3531996453-y91gu  -c kubedns
: 1490035359:0;kubectl --namespace kube-system logs kube-dns-v20-3531996453-wcefi  -c kubedns
: 1490035429:0;kubectl get pods -o wide  --all-namespaces > kubectl-get-pods-per-ip-k8s-myhstg-com-032017.log
: 1490035437:0;grep 100.96.2.19 kubectl-get-pods-per-ip-k8s-myhstg-com-032017.log
: 1490035474:0;grep 100.64.0.1 kubectl-get-pods-per-ip-k8s-myhstg-com-032017.log
: 1490035496:0;grep 100.96.3.185 kubectl-get-pods-per-ip-k8s-myhstg-com-032017.log
: 1490035550:0;vi tcpdump-some-ip-list-deis-builder
: 1490035580:0;more tcpdump-some-ip-list-deis-builder
: 1490035603:0;more tcpdump-some-ip-list-deis-builder | awk -F ">" '{print $1}'
: 1490035614:0;more tcpdump-some-ip-list-deis-builder | awk -F ">" '{print $1}' | grep 100
: 1490035621:0;more tcpdump-some-ip-list-deis-builder | awk -F ">" '{print $1}' | grep 100 | sort
: 1490035644:0;more tcpdump-some-ip-list-deis-builder | awk -F ">" '{print $1}' | grep 100 | sort | grep -v "18:18"
: 1490035653:0;more tcpdump-some-ip-list-deis-builder | awk -F ">" '{print $1}' | grep 100 | sort | grep -v "18:18" | uniq -c
: 1490035685:0;more tcpdump-some-ip-list-deis-builder | awk -F ">" '{print $1}' | grep 100 | sort | grep -v "18:18" | uniq -c > tcpdump-some-host-list
: 1490035765:0;grep 100.64.0.10 kubectl-get-pods-per-ip-k8s-myhstg-com-032017.log
: 1490035795:0;grep 100.70.92.248 kubectl-get-pods-per-ip-k8s-myhstg-com-032017.log
: 1490035842:0;grep 100.96.3.1 kubectl-get-pods-per-ip-k8s-myhstg-com-032017.log
: 1490035882:0;grep 100.96.4.237 kubectl-get-pods-per-ip-k8s-myhstg-com-032017.log
: 1490036003:0;history | grep ssh | grep dc
: 1490036065:0;ssh ubuntu@54.196.151.95
: 1490036104:0;history | grep 54.196.151.95
: 1490036112:0;history | grep ec2
: 1490036155:0;history | grep ec2-54-196-151-95
: 1490048777:0;ping -c 100 ec2-54-160-100-223.compute-1.amazonaws.com
: 1490109224:0;kubectl config current-context
: 1490110096:0;history | grep deis | grep config
: 1490110141:0;shipi ssh wellbeingid-production
: 1490110298:0;deis config:set -a airflow-staging TEST-VAR=test-deis-builder-for-pushing-release 
: 1490110339:0;deis config:set -a airflow-staging TEST_VAR="Test of deis builder for airflow -- heidi schmidt"  
: 1490110444:0;kubectl --namespace deis describe pod deis-builder-2051187435-3q1nw
: 1490110529:0;kubectl --namespace deis describe pod deis-builder-140878763-e3ykw
: 1490110553:0;deis config --help
: 1490110579:0;deis config:unset -a airflow-staging TEST_VAR
: 1490110836:0;deis apps 
: 1490111769:0;kubectl get deployment --namespace deis
: 1490111832:0;kubectl get certificate --namespace deis
: 1490111875:0;kubectl edit deployment --namespace deis 
: 1490111920:0;more deis-namespace-kubectl.yaml
: 1490112421:0;history | grep airflow
: 1490114444:0;kubectl --namespace deis logs deis-builder-2051187435-3q1nw
: 1490114470:0;kubectl --namespace deis logs deis-builder-3177030498-o0cun
: 1490114516:0;kubectl --namespace deis logs deis-builder-3177030498-o0cun > kubectl-namespace-deis-logs-deis-builder-3177030498-o0cun.log 
: 1490114681:0;kubectl get events --namespace deis 
: 1490114877:0;cat kubectl-namespace-deis-logs-deis-builder-3177030498-o0cun.log
: 1490114952:0;2017/03/21 16:15:25 Listing Buckets took 0.043369119000000005 seconds
: 1490114953:0;receiving git repo name: airflow-staging.git, operation: git-upload-pack, fingerprint: f6:b9:c0:7c:5c:0f:88:59:ef:34:85:a0:a9:05:59:c6, user: bolek.kurowski
: 1490114953:0;creating repo directory /home/git/airflow-staging.git
: 1490114953:0;writing pre-receive hook under /home/git/airflow-staging.git
: 1490114953:0;git-shell -c git-upload-pack 'airflow-staging.git'
: 1490114953:0;Waiting for git-receive to run.
: 1490114953:0;Waiting for deploy.
: 1490114953:0;Deploy complete.
: 1490114953:0;---> ---> ---> ---> ---> ---> ---> ---> 2017/03/21 16:15:35 Listing Buckets took 0.042520442000000006 seconds
: 1490114954:0;2017/03/21 16:15:45 Listing Buckets took 0.11306300600000001 seconds
: 1490114954:0;2017/03/21 16:15:55 Listing Buckets took 0.433965317 seconds
: 1490114954:0;2017/03/21 16:15:55 Slow listing took 0.433965317 seconds
: 1490114954:0;2017/03/21 16:16:05 Listing Buckets took 0.046857148 seconds
: 1490114954:0;2017/03/21 16:16:15 Listing Buckets took 0.047715776 seconds
: 1490114954:0;2017/03/21 16:16:25 Listing Buckets took 0.043729523 seconds
: 1490114954:0;2017/03/21 16:16:35 Listing Buckets took 0.295951748 seconds
: 1490114954:0;2017/03/21 16:16:35 Slow listing took 0.295951748 seconds
: 1490114954:0;Accepted connection.
: 1490114954:0;Starting ssh authentication
: 1490114954:0;Channel type: session
: 1490120866:0;kubectl edit deployment Deis-router --namespace deis
: 1490121025:0;kubectl --namespace=deis patch deployment deis-builder --type='json' -p=[{"op": "replace", "path": "/spec/template/spec/containers/0/livenessProbe/timeoutSeconds", "value":"10"}]
: 1490121076:0;kubectl --namespace=deis patch deployment deis-builder --type=json --help
: 1490121163:0;rm "\--*"
: 1490121170:0;rm ---
: 1490121180:0;rm \-\--
: 1490121188:0;vi deis-namespace-kubectl.yaml
: 1490121240:0;kubectl --namespace=deis patch deployment deis-builder --type=json -p='[{"op": "replace", "path": "/spec/template/spec/containers/0/livenessProbe/timeoutSeconds", "value":"10"}]'
: 1490121379:0;kubectl patch --help
: 1490121558:0;kubectl --namespace=deis patch  deis-builder --type="json" -p='[{"op": "replace", "path": "/spec/template/spec/containers/0/livenessProbe/timeoutSeconds", "value":"10"}]'
: 1490121571:0;kubectl --namespace=deis patch deployment deis-builder --type="json" -p='[{"op": "replace", "path": "/spec/template/spec/containers/0/livenessProbe/timeoutSeconds", "value":"10"}]'
: 1490121730:0;kubectl --namespace=deis patch deployment deis-builder --type='json' -p='[{"op": "replace", "path": "/spec/template/spec/containers/0/livenessProbe/timeoutSeconds", "value":"10"}]'
: 1490121802:0;kubectl --namespace=deis patch deployment deis-builder --type='json' -p='[{"op": "replace", "path": "/spec/template/spec/containers/0/livenessProbe/timeoutSeconds", "value":10}]'
: 1490121826:0;kubectl --namespace=deis patch deployment deis-builder --type='json' -p='[{"op": "replace", "path": "/spec/template/spec/containers/0/readinessProbe/timeoutSeconds", "value":"10"}]'
: 1490121838:0;kubectl --namespace=deis patch deployment deis-builder --type='json' -p='[{"op": "replace", "path": "/spec/template/spec/containers/0/readinessProbe/timeoutSeconds", "value":10}]'
: 1490121922:0;mv deis-edit-2.yaml deis-namespace-kubectl-2.yaml
: 1490121935:0;diff -u deis-namespace-kubectl.yaml deis-namespace-kubectl-2.yaml
: 1490122638:0;kubectl --namespace=deis describe pods  deis-monitor-telegraf-ldkzl
: 1490122974:0;kubectl get nodes -o wide | grep ip-172-20-110-193.ec2.internal
: 1490195977:0;kubectl get pods -o wide  --all-namespaces | grep tele
: 1490196048:0;history | grep logs
: 1490196058:0;kubectl --namespace deis logs deis-builder-3177030498-o0cun --since=20m
: 1490196088:0;kubectl describe pod deis-builder-1588437946-7ppmt --namespace deis
: 1490196250:0;kubectl describe pod airflow-staging-scheduler-3361996664-3asyn --namespace airflow-staging 
: 1490197798:0;grep builder *
: 1490197806:0;grep builder *.log
: 1490197902:0;kubectl describe pod deis-builder-1588437946-7ppmt > kubectl-desc-pod-deis-builder-1588437946-7ppmt.log
: 1490198071:0;kubectl describe pod deis-builder-2494734377-r64k9
: 1490198428:0;brew list kubernetes-cli
: 1490198659:0;more kubectl-desc-pod-deis-builder-1588437946-7ppmt.log
: 1490198667:0;rm kubectl-desc-pod-deis-builder-1588437946-7ppmt.log
: 1490198735:0;more kubectl-namespace-deis-logs-deis-builder-3177030498-o0cun.log
: 1490202518:0;brew info kubernetes-cli
: 1490202528:0;kubectl get pods -o wide  --all-namespaceso | grep builder
: 1490204154:0;deis releases -a airflow-staging
: 1490204349:0;history | grep deis | grep heidi
: 1490204356:0;deis auth:login https://deis.meyouhealth.com --ssl-verify=true --username=heidi.schmidt --password= JxgcoU6HrRzuxtpymMrVshya
: 1490204368:0;deis login https://deis.meyouhealth.com --ssl-verify=true --username=heidi.schmidt --password= JxgcoU6HrRzuxtpymMrVshya
: 1490204412:0;deis releases -a airflow-production
: 1490204718:0;deis users
: 1490204831:0;vi perms.sh 
: 1490204860:0;chmod +x perms.sh
: 1490204991:0;deis users | grep -v "===" | grep -v "*"
: 1490205028:0;export APP_NAME=wilder-staging
: 1490205062:0;export APP_NAME=wbt-staging
: 1490205078:0;export APP_NAME=walkadoo-staging
: 1490205079:0;./perms.sh wilder-staging
: 1490205115:0;export APP_NAME=iris-staging
: 1490205137:0;export APP_NAME=hurby-server-staging
: 1490205152:0;export APP_NAME=hello200-staging
: 1490205163:0;export APP_NAME=dcmobile-staging
: 1490205176:0;export APP_NAME=airflow-staging
: 1490205178:0;./perms.sh
: 1490205339:0;deis releases -a walkadoo-staging 
: 1490205390:0;more perms.sh
: 1490205403:0;deis perms --help
: 1490205423:0;deis perms:list --help
: 1490205439:0;deis perms:list -a walkadoo-staging
: 1490205738:0;kubectl --namespace deis logs deis-builder-2494734377-r64k9 > kubectl-namespace-deis-logs-deis-builder-2494734377-r64k9-b4-after-deis-create-perms-reapplied.log
: 1490205748:0;date >> kubectl-namespace-deis-logs-deis-builder-2494734377-r64k9-b4-after-deis-create-perms-reapplied.log
: 1490205751:0;more kubectl-namespace-deis-logs-deis-builder-2494734377-r64k9-b4-after-deis-create-perms-reapplied.log
: 1490205920:0;kubectl describe pod deis-builder-2494734377-r64k9 --namespace deis
: 1490207231:0;kubectl describe pod deis-controller-3595397862-zw5ux
: 1490207236:0;kubectl describe pod deis-controller-3595397862-zw5ux --namespace deis
: 1490208334:0;ls -ltra | tail -5
: 1490208339:0;vi logs-for-slack.log
: 1490208427:0;ssh-add -l 
: 1490208467:0;vi deis-keys-ssh-add-keys-pub-key.log
: 1490209501:0;cd ../kubernetes
: 1490209511:0;vi testing-all-apps-deis-builder.log
: 1490209652:0;more testing-all-apps-deis-builder.log
: 1490209657:0;more deis-keys-ssh-add-keys-pub-key.log
: 1490209836:0;kubectl get events --namespace deis > kubectl-get-events-deis-staging.log
: 1490209867:0;kubectl --namespace deis logs deis-builder-3607535561-2ew1q
: 1490209955:0;deis config:set DEIS_DEBUG=1 -a airflow-staging
: 1490210037:0;vi setting-deis-debug-for-builder-staging-logs.log
: 1490210131:0;deis ps
: 1490210149:0;deis ps kubectl --namespace wilder-staging wilder-staging-web-1651477183-d42te
: 1490210167:0;deis ps `kubectl --namespace wilder-staging wilder-staging-web-1651477183-d42te`
: 1490210180:0;kubectl describe pod wilder-staging-web-1651477183-d42te
: 1490210236:0;deis ps:list --help
: 1490210243:0;deis ps:list -a wilder-staging --help
: 1490210247:0;deis ps:list -a wilder-staging 
: 1490210275:0;kubectl --namespace wilder-staging  wilder-staging-web-3159460638-a28co
: 1490210287:0;kubectl --namespace wilder-staging  pod wilder-staging-web-3159460638-a28co
: 1490210296:0;kubectl --namespace wilder-staging top wilder-staging-web-3159460638-a28co
: 1490210325:0;kubectl --namespace wilder-staging logs wilder-staging-web-3159460638-a28co
: 1490242749:0;kubectl get events --all-namespaces >> ../kubernetes/k8s.meyouhealth.com-kube-get-events-all-prom-alert-pgrduty.log
: 1490242770:0;date >> ../kubernetes/k8s.meyouhealth.com-kube-get-events-all-prom-alert-pgrduty.log
: 1490242779:0;kubectl get pods -o wide  --all-namespaces >> ../kubernetes/k8s.meyouhealth.com-kube-get-events-all-prom-alert-pgrduty.log
: 1490242791:0;more ../kubernetes/k8s.meyouhealth.com-kube-get-events-all-prom-alert-pgrduty.log
: 1490278098:0;desi whoami
: 1490278106:0;kubectl get pods -o wide  --all-namespaces >> ../kubernetes/k8s.meyouhealth.com-kube-get-events-all-prom-alert-pgrduty-032317.log
: 1490278132:0;kubectl get events  --all-namespaces >> ../kubernetes/k8s.meyouhealth.com-kube-get-events-all-prom-alert-pgrduty-032317.log
: 1490278136:0;more ../kubernetes/k8s.meyouhealth.com-kube-get-events-all-prom-alert-pgrduty-032317.log
: 1490278371:0;kubectl get nodes -o wide >> k8s.meyouhealth.com-kube-get-events-all-prom-alert-pgrduty-032317.log
: 1490279128:0;kubectl get pods -o wide  --all-namespaces | grep dataviz-production-clockwork-2801009164-l04dj
: 1490279157:0;kubectl --namespace dataviz-production logs dataviz-production-clockwork-2801009164-rwn7c
: 1490279181:0;kubectl describe pod dataviz-production-clockwork-2801009164-rwn7c
: 1490279192:0;kubectl get pods -o wide  --all-namespaces | grep dataviz-production-clockwork
: 1490279208:0;kubectl describe pod dataviz-production-clockwork-2801009164-rwn7c --namespace dataviz-production
: 1490279229:0;ssh admin@ip-172-24-34-107.ec2.internal
: 1490279285:0;vi node-with-disk-issues-disk-layout-032317.txt
: 1490279351:0;history | grep s3
: 1490279356:0;s3 ls myh-deis-production-builder
: 1490279363:0;aws s3 ls myh-deis-production-builder
: 1490279472:0;kubectl get events -namespace deis
: 1490279515:0;kubectl get pods -o wide  --all-namespaces | grep deis-con
: 1490279543:0;kubectl restart --help
: 1490279571:0;kubectl get pods -o wide  --all-namespaces | grep deis-build
: 1490279584:0;kubectl describe pod deis-builder-2494603305-cf5hx --namespace deis
: 1490279840:0;kubectl get pods -o wide  --all-namespaces | grep kubelet
: 1490279911:0;kubectl --namespace deis logs deis-builder-2494603305-cf5hx | grep "Readinesscheck endpoint timed out after 10s" | wc -l 
: 1490279939:0;kubectl --namespace deis logs deis-builder-2494603305-cf5hx --since=10m
: 1490279944:0;kubectl --namespace deis logs deis-builder-2494603305-cf5hx --since=15m
: 1490279949:0;kubectl --namespace deis logs deis-builder-2494603305-cf5hx --since=20m
: 1490280269:0;kubectl get pods -o wide  --all-namespaces | grep desi
: 1490280538:0;time aws s3 ls myh-deis-production-builder
: 1490281676:0;kubectl get events --namespace deis | more
: 1490281721:0;kubectl get events --namespace deis > kubectl-get-events-deis-prod-032317.log 
: 1490281735:0;grep -i disk kubectl-get-events-deis-prod-032317.log
: 1490281740:0;grep -i unhealthy kubectl-get-events-deis-prod-032317.log
: 1490281772:0;kubectl --namespace deis logs deis-controller-337311818-5h9wv
: 1490281791:0;kubectl get pods -o wide  --namespace deis
: 1490281823:0;kubectl get pods -o wide  --namespace deis >> kubectl-get-events-deis-prod-032317.log
: 1490281908:0;kubectl get nodes -o wide >> kubectl-get-events-deis-prod-032317.log
: 1490282085:0;deis healthchecks:list -a deis-builder
: 1490282095:0;deis healthchecks:list -a deis
: 1490282113:0;deis healthchecks:list -a airflow-production
: 1490282128:0;deis healthchecks:list -a wilder-production
: 1490282265:0;vi readiness-probe-needs-port-set.log
: 1490282499:0;grep "100.96.11.140" kubectl-get-events-deis-prod-032317.log
: 1490282512:0;grep "100.96.11.140" k8s.meyouhealth.com-kube-get-events-all-prom-alert-pgrduty-032317.log
: 1490282534:0;kubectl describe node ip-172-24-82-78.ec2.internal
: 1490282618:0;vi k8s.meyouhealth.com-kube-get-events-all-prom-alert-pgrduty.log
: 1490282669:0;kubectl describe node ip-172-24-82-78.ec2.internal >> k8s.meyouhealth.com-kube-get-events-all-prom-alert-pgrduty-032317.log
: 1490282683:0;grep ip-172-24-82-78.ec2.internal k8s.meyouhealth.com-kube-get-events-all-prom-alert-pgrduty-032317.log
: 1490317495:0;kubectl get serviceAccounts -n deis
: 1490317525:0;kubectl --namespace deis logs deis-builder-2494603305-cf5hx
: 1490369011:0;;kcli
: 1490369352:0;kubectl config use-context k8s.myhstg.com
: 1490369478:0;history | grep -i debug
: 1490369494:0;deis config:set DEIS_DEBUG=1 -a dcmobile-staging
: 1490369644:0;deis config:set DEIS_DEBUG=1 -a hello200-staging
: 1490369745:0;deis config:set DEIS_DEBUG=1 -a hurby-server-staging
: 1490369835:0;deis config:set DEIS_DEBUG=1 -a iris-staging
: 1490369959:0;deis config:set DEIS_DEBUG=1 -a walkadoo-staging
: 1490370123:0;deis config:set DEIS_DEBUG=1 -a wbt-staging
: 1490370234:0;deis config:set DEIS_DEBUG=1 -a wilder-staging
: 1490375632:0;kubectl --namespace deis logs deis-builder-2494734377-r64k9
: 1490375862:0;kubectl --namespace deis logs deis-controller-3595397862-zw5ux
: 1490376513:0;kubectl --namespace deis logs deis-controller-3595397862-zw5ux > ~/Documents/kubernetes-controller-staging-after-setting-deis-debug.log
: 1490376851:0;more deis-namespace-kubectl-2.yaml
: 1490379561:0;kubectl --namespace deis logs deis-controller-3595397862-zw5ux > ~/Documents/kubernetes-controller-staging-after-setting-deis-debug2.log
: 1490379574:0;more ~/Documents/kubernetes-controller-staging-after-setting-deis-debug2.log
: 1490380418:0;more k8s.meyouhealth.com-kube-get-events-all-prom-alert-pgrduty-032317.log
: 1490380432:0;more kubectl-deis-logs-deis-builder-1588437946-7ppmt-key-not-found.log
: 1490380454:0;grep ERROR *builder*
: 1490628663:0;vi elasticache-read-replica.txt
: 1490628719:0;more ../aws_cli_output/aws-elasticache-desc-replication-groups-03062017.txt
: 1490628731:0;vi ../aws_cli_output/aws-elasticache-desc-replication-groups-03062017.txt
: 1490628761:0;cat elasticache-read-replica.txt
: 1490628797:0;aws elasticache create-cache-cluster --help
: 1490628997:0;aws elasticache create-cache-cluster --cache-cluster-id wd-prod-redis-002 --replicationgroup-id wd-prod-redis-001
: 1490629025:0;aws elasticache create-cache-cluster --cache-cluster-id "wd-prod-redis-002" --replicationgroup-id "wd-prod-redis-001"
: 1490629098:0;aws elasticache create-cache-cluster help
: 1490630121:0;kubectl --namespace deis logs deis-builder-1253567498-zc6wv
: 1490630158:0;kubectl edit deployment --namespace deis
: 1490630836:0;kubectl --namespace deis logs deis-builder-545647551-x9snc
: 1490636046:0;shipit deployment logs 5103
: 1490636228:0;kops export kubecfg
: 1490636245:0;kops config use-context lrywh.k8s.myhstg.com
: 1490636338:0;deis keys:list
: 1490636391:0;cd ../chef-dailychallenge
: 1490636405:0;rm Berksfile.lock
: 1490636427:0;cd ../zsh-code
: 1490636442:0;cd EasyMySQL
: 1490636448:0;cd ../kops
: 1490636518:0;rm Screen*png
: 1490636818:0;kubectl --namespace airflow-staging logs airflow-staging-web-486013422-cjznj
: 1490636992:0;kubectl --namespace airflow-staging logs airflow-staging-scheduler-10393356-62lc4
: 1490637466:0;deis releases -a hurby-server-staging
: 1490637478:0;deis releases -a iris-staging
: 1490637516:0;deis releases -a wbt-staging
: 1490637532:0;deis releases -a wilder-staging
: 1490638439:0;kubectl --namespace=deis describe service deis-builder
: 1490638477:0;kubectl --namespace=deis get service --help
: 1490638486:0;kubectl --namespace=deis get service 
: 1490638542:0;kubectl --namespace=airflow-staging get service 
: 1490638579:0;kubectl --namespace=deis describe service deis-logger
: 1490638664:0;kubectl --namespace=deis describe service deis-router
: 1490638748:0;cd ../wellbeingid
: 1490638759:0;cd ../reporting
: 1490638789:0;cd walkadoo-ios
: 1490638819:0;cd ../ls -l
: 1490638838:0;cd ../research-code
: 1490638855:0;cd myh-monitoring
: 1490638862:0;cd ../myh-aws
: 1490638874:0;cd ../iozone-results-comparator
: 1490638894:0;cd ../guides
: 1490638904:0;cd ../dotfiles
: 1490638912:0;cd ../dockerbook-code
: 1490638926:0;cd ../deployment-api
: 1490638948:0;got pull
: 1490638962:0;cd sourcecode
: 1490639003:0;cd ../boot2docker
: 1490639064:0;helm inspect values deis/workflow | sed -n '1!p' > helm-chart-deis-workflow-values.yaml
: 1490639121:0;kubectl edit deployment deis-builder --namespace deis
: 1490639157:0;kubectl edit deployment deis-controller --namespace deis
: 1490639194:0;diff -u deis-builder-check.1 deis-controller-check.1
: 1490639205:0;diff -u deis-builder-check.1 deis-controller-check.1 | grep -v "^ "
: 1490639292:0;grep -i limit deis-builder-check.1
: 1490639336:0;grep SELECTOR deis-builder-check.1
: 1490639339:0;more deis-builder-check.1
: 1490639363:0;grep DEBUG deis-builder-check.1
: 1490639380:0;grep MODE deis-controller-check.1
: 1490639429:0;grep DEIS_DEPLOY_BATCHES *check*
: 1490639975:0;aws ls myh*
: 1490639986:0;aws ls *builder*
: 1490639995:0;aws s3 ls *builder*
: 1490640001:0;aws s3 ls | grep builder
: 1490640024:0;aws s3 ls | grep registry
: 1490640289:0;kubectl --namespace deis edit daemonset  deis-logger-fluentd
: 1490640376:0;kubectl --namespace deis edit daemonset  deis-monitor-telegraf
: 1490640416:0;kubectl --namespace deis get daemonset
: 1490640435:0;kubectl --all-namespaces get daemonset
: 1490640443:0;kubectl get daemonset
: 1490640448:0;kubectl get daemonset --all-namespaces
: 1490640485:0;kubectl --all-namespaces get service
: 1490640498:0;kubectl get service --all-namespaces
: 1490640553:0;kubectl --namespace=airflow-staging describe service airflow-staging
: 1490640663:0;brew list | grep deis
: 1490640672:0;brew info | grep deis
: 1490640679:0;brew info deis
: 1490711929:0;kops export kubecfg lrywh.k8s.myhstg.com
: 1490711945:0;kubectl config delete-context k8s.myhstg.com
: 1490711946:0;kubectl config delete-cluster k8s.myhstg.com
: 1490712335:0;deis version
: 1490712512:0;kubectl get events --all-namespaces | more
: 1490712550:0;kubectl --namespace airflow-staging logs airflow-staging-scheduler-10393356-6k3d3
: 1490712559:0;kubectl get pods -o wide  --all-namespaces | grep airflow
: 1490712572:0;kubectl --namespace airflow-staging logs airflow-staging-scheduler-10393356-7jjzc
: 1490712586:0;kubectl --namespace airflow-staging logs airflow-staging-web-486013422-cb6ww
: 1490713040:0;kubectl get nodes -o wide â€ƒ´show-labels 
: 1490713046:0;kubectl get nodes -o wide -â€ƒ´show-labels 
: 1490713088:0;kubectl get events --all-namespaces -o wide
: 1490713277:0;kubectl --namespace deis logs deis-registry-782034515-df1gh
: 1490713666:0;kubectl get events â€ƒ´-all-namespaces
: 1490713673:0;kubectl get events --namespace deis
: 1490713710:0;kubectl --namespace deis logs deis-builder-4086353868-20kcm
: 1490713800:0;kubectl describe pod deis-builder-4086353868-20kcm --namespace deis
: 1490714066:0;kops edit instancegroup master-us-east-1c
: 1490714126:0;kubectl describe node ip-172-20-143-248.ec2.internal | less
: 1490714764:0;kubectl get events â€ƒ´all-namespaces
: 1490714770:0;kubectl get events -â€ƒ´all-namespaces
: 1490714914:0;kubectl get events | head
: 1490714958:0;vi kubectl-get-events-differences-between-all-namespaces-vs-not
: 1490715100:0;rm -i *
: 1490715237:0;kubectl --namespace prometheus logs prometheus-2473814154-qwbcd
: 1490715278:0;kubectl --namespace prometheus logs prometheus-2473814154-qwbcd > kubectl-get-logs-prometheus-pod.log
: 1490715331:0;ssh admin@100.89.164.25
: 1490715455:0;history | grep exec | grep kube
: 1490715473:0;kubectl exec prometheus-2473814154-qwbcd -i -t bash
: 1490715491:0;kubectl exec --namespace prometheusprometheus-2473814154-qwbcd -i -t bash 
: 1490715497:0;kubectl exec --namespace prometheus prometheus-2473814154-qwbcd -i -t bash 
: 1490715524:0;kubectl  --namespace prometheus exec prometheus-2473814154-qwbcd -i -t bash 
: 1490715544:0;kubectl --namespace prometheus exec -ti prometheus-2473814154-qwbcd bash
: 1490715756:0;vi curent-etc-prometheus-prometheus-yaml.yml
: 1490715785:0;mv curent-etc-prometheus-prometheus-yaml.yml current-etc-prometheus-prometheus-yaml.yml
: 1490715791:0;vi current-etc-prometheus-prometheus-yaml.yml
: 1490799171:0;[10:45 AM] Deployment API: #5115 complete for Walkadoo Staging, elapsed time: 17 minutes
: 1490806435:0;kubectl get pods -Lapp -Ltier -Lrole
: 1490806448:0;kubectl get pods -Lapp -Ltier -Lrole --all-namespaces
: 1490806513:0;kubectl get pods -lapp=iris-staging,role=slave
: 1490806516:0;kubectl get pods -lapp=iris-staging
: 1490806526:0;kubectl get pods -lapp=iris-staging --namespace iris-staging
: 1490814246:0;kubectl --namespace deis logs deis-builder-4086353868-20kcm --timestamps 
: 1490814265:0;kubectl --namespace deis logs deis-builder-4086353868-20kcm --timestamps  --follow
: 1490814728:0;tar xvfz prometheus-*.tar.gz
: 1490814757:0;locate prometheus.yml
: 1490814868:0;kubectl describe pod prometheus-3412682896-g00xt --namespace prometheus
: 1490814918:0;history | grep prometheus
: 1490815103:0;kubectl exec prometheus-3412682896-g00xt -i -t bash --namespace prometheus
: 1490815206:0;kubectl exec -i -t bash --namespace prometheus
: 1490815226:0;kubectl exec prometheus-3412682896-hvb2h -i -t bash 
: 1490815267:0;kubectl describe prometheus-3412682896-l6nt1
: 1490815274:0;kubectl describe pod prometheus-3412682896-l6nt1
: 1490815301:0;kubectl --namespace prometheus logs prometheus-3412682896-l6nt1
: 1490815320:0;kubectl --namespace prometheus logs prometheus-3412682896-l6nt1 
: 1490815336:0;kubectl exec prometheus-3412682896-l6nt1 -i -t bash 
: 1490815359:0;kubectl exec prometheus-3412682896-hvb2h -i -t bash --namespace prometheus
: 1490815373:0;kubectl exec prometheus-3412682896-l6nt1 -i -t bash --namespace prometheus
: 1490815385:0;kubectl exec prometheus-3412682896-l6nt1 -i -t ls
: 1490815404:0;kubectl exec prometheus-3412682896-l6nt1 -i -t ls --namespace prometheus
: 1490815415:0;kubectl exec prometheus-3412682896-l6nt1 -i -t whoami --namespace prometheus
: 1490815424:0;kubectl exec prometheus-3412682896-l6nt1 -i -t df -Ph . --namespace prometheus
: 1490815459:0;kubectl exec prometheus-3412682896-l6nt1 -i -t -- bash -il --namespace prometheus
: 1490815496:0;kubectl exec prometheus-3412682896-l6nt1 -i -t ls /etc/prometheus --namespace prometheus
: 1490884016:0;ssh admin@ip-172-24-60-122
: 1490884063:0;ssh admin@ip-172-24-60-122.ec2.internal
: 1490884580:0;kubectl config use-context k8s.meyouhealth.com
: 1490884625:0;kubectl --namespace prometheus logs prometheus-1733696681-odzdg
: 1490884644:0;kubectl describe pod prometheus-1733696681-odzdg
: 1490884656:0;kubectl describe pod prometheus-1733696681-odzdg --namespace prometheus
: 1490885494:0;mv kubectl-edit-deployment-prometheus-all-yaml.yml Documents/kubernetes/
: 1490885510:0;grep quay kubectl-edit-deployment-prometheus-all-yaml.yml
: 1490885643:0;more prometheus
: 1490885856:0;kubectl describe pod prometheus-2672565423-xmqg0 --namespace prometheus
: 1490885885:0;kubectl --namespace prometheus logs prometheus-2672565423-xmqg0
: 1490885959:0;kubectl --namespace prometheus logs prometheus-2672565423-enued
: 1490886053:0;kubectl --namespace prometheus logs prometheus-2672565423-enued 
: 1490886140:0;cat kubectl-edit-deployment-prometheus-all-yaml.yml | wc -l 
: 1490886145:0;vi kubectl-edit-deployment-prometheus-all-yaml.yml
: 1490886627:0;kubectl describe pod prometheus-2672565423-enued
: 1490886634:0;kubectl describe pod prometheus-2672565423-enued --namespace prometheus
: 1490886756:0;cd ../../Documents/kubernetes
: 1490886961:0;git clone git@github.com:meyouhealth/k8s-ops.git 
: 1490887058:0;kubectl --help
: 1490887647:0;kubectl edit deployment --namespace prometheus
: 1490887704:0;kubectl describe pod prometheus-1733696681-s5fgw
: 1490887712:0;kubectl describe pod prometheus-1733696681-s5fgw --namespace prometheus
: 1490887979:0;diff -u prometheus.yml ~/Documents/kubernetes/latest-prometheus-yaml-deployment-broken-parsing-v152.yml
: 1490888054:0;vimdiff prometheus.yml ~/Documents/kubernetes/latest-prometheus-yaml-deployment-broken-parsing-v152.yml
: 1490888295:0;vimdiff prometheus.yml ~/Documents/kubernetes/kubectl-edit-deployment-prometheus-all-yaml.yml
: 1490892988:0;more ~/Documents/kubernetes/kubectl-edit-deployment-prometheus-all-yaml.yml
: 1490894320:0;kubectl --namespace prometheus get configmap
: 1490894337:0;kubectl --namespace prometheus get configmap -o wide
: 1490894386:0;more helm-chart-deis-workflow-values.yaml
: 1490894444:0;helm inspect deis-workflow
: 1490894454:0;helm inspect workflow-v2.12.0
: 1490894470:0;helm inspect stable/workflow-v2.12.0
: 1490894851:0;history | grep edit
: 1490894955:0;kubectl --namespace prometheus get configmap --show-labels
: 1490895099:0;cd workflow
: 1490895108:0;more Chart
: 1490895115:0;more values.yaml
: 1490895390:0;kubectl --namespace prometheus get configmap --show-labels prometheus-config
: 1490895479:0;helm repo --help
: 1490895485:0;helm repo list
: 1490895656:0;more /Users/heidischmidt/.helm
: 1490895664:0;ls -ltra /Users/heidischmidt/.helm
: 1490895668:0;ls -ltra /Users/heidischmidt/.helm/repository
: 1490895678:0;more /Users/heidischmidt/.helm/repository/repositories.yaml
: 1490895789:0;helm get values deis-workflow
: 1490896057:0;kubectl get pods -o wide  --all-namespacess | grep prom
: 1490896296:0;helm get deis-workflow | grep image
: 1490896320:0;helm get deis-workflow --help
: 1490896435:0;kubectl --namespace prometheus logs prometheus-2672565423-fr7pw
: 1490897150:0;kubectl describe pod prometheus-2672565423-fr7pw --namespace prometheus
: 1490897765:0;kubectl describe node ip-172-24-110-158.ec2.internal
: 1490897841:0;ssh admin@kubectl describe node ip-172-24-110-158.ec2.internal
: 1490897852:0;ssh admin@ip-172-24-110-158.ec2.internal
: 1490900524:0;kubectl --namespace deis logs deis-builder-2494734377-mlyts
: 1490900544:0;kubectl --namespace deis logs deis-builder-2494734377-mlyts --timestamps
: 1490900588:0;kubectl --namespace deis logs deis-controller-3595397862-vb91c
: 1490900593:0;kubectl --namespace deis logs deis-controller-3595397862-vb91c --timestamps
: 1490900619:0;kubectl --namespace deis logs deis-builder-2494734377-mlyts --timestamps --follow
: 1490900731:0;kubectl --namespace deis logs deis-controller-3595397862-vb91c --timestamps --follow
: 1490900851:0;kubectl get pods -o wide  --all-namespaces | grep deis | grep control
: 1490900866:0;kubectl get pods -o wide  --all-namespaces | grep deis | grep router
: 1490900881:0;kubectl --namespace deis logs deis-router-2126433040-fsl7w --timestamps --follow
: 1490901029:0;kubectl --namespace deis logs deis-router-2126433040-fsl7w --timestamps | grep -B 10 "broken header"
: 1490901043:0;kubectl --namespace deis logs deis-router-2126433040-fsl7w --timestamps 
: 1490901080:0;kubectl --namespace deis logs deis-router-2126433040-fsl7w --timestamps | grep "broken header"
: 1490901097:0;kubectl logs --help
: 1490901138:0;kubectl --namespace deis logs deis-router-2126433040-fsl7w --timestamps > ~/Documents/kubernetes/deis-router-broken-header.log
: 1490901160:0;grep -B 3 "broken header" ~/Documents/kubernetes/deis-router-broken-header.log
: 1490901170:0;grep broken ~/Documents/kubernetes/deis-router-broken-header.log
: 1490901221:0;kubectl --namespace deis logs deis-router-2126433040-fsl7w  > ~/Documents/kubernetes/deis-router-broken-header-no-timestamps.log
: 1490901230:0;grep broken ~/Documents/kubernetes/deis-router-broken-header-no-timestamps.log
: 1490901246:0;strings ~/Documents/kubernetes/deis-router-broken-header-no-timestamps.log | grep broken
: 1490901490:0;kubectl --namespace deis logs deis-controller-3595397862-vb91c --timestamps --follow 
: 1490901504:0;deis releases -a wilder-production-jobs
: 1490901507:0;deis releases -a wilder-production
: 1490902089:0;for i in `deis apps`\
do \
deis releases -a $i | grep -v travis > deis-rel-${i}.log \
done 
: 1490902112:0;rm deis-rel-=*.log
: 1490902117:0;rm deis-rel-Apps.log
: 1490902122:0;more deis-rel-airflow-production.log
: 1490902133:0;more deis-rel-dcmobile-production.log
: 1490902145:0;more deis-rel-dataviz-production.log
: 1490902167:0;grep deployed deis-rel-*.log
: 1491235014:0;bundle install 
: 1491235037:0;rbenv install 2.3.3
: 1491235157:0;deis logout 
: 1491235205:0;nslookup api.lrywh.k8s.myhstg.com
: 1491238438:0;which postgres
: 1491238569:0;brew services start postgres
: 1491238581:0;psql -E
: 1491238602:0;psql -h 127.0.0.1 -P 5432
: 1491238616:0;ps -ef | grep postgres
: 1491238621:0;brew list services
: 1491238682:0;brew list phantomjs
: 1491238723:0;more .env_heidi
: 1491238754:0;cp .env_heidi .env_heidi_prom_dev_test
: 1491238761:0;vi .env_heidi_prom_dev_test
: 1491238897:0;psql -h 127.0.0.1 -P 9998
: 1491238904:0;psql -h 127.0.0.1 -p 9998
: 1491238908:0;psql -h 127.0.0.1 -p 5432
: 1491238953:0;more .env_heidi_prom_dev_test
: 1491239003:0;more database.yml
: 1491239013:0;diff database.yml database.yml.heidi
: 1491239032:0;paste database.yml database.yml.heidi
: 1491239061:0;grep DATABASE_PASSWORD ../.env
: 1491239077:0;cat database.yml
: 1491239080:0;database.yml.heidi
: 1491239084:0;cat database.yml.heidi
: 1491239139:0;psql -h 127.0.0.1 
: 1491239216:0;ls -ltra /usr/local/var/postgres
: 1491239227:0;more /usr/local/var/postgres/server.log
: 1491239291:0;brew --help
: 1491239298:0;brew uninstall postgres
: 1491239313:0;brew uninstall postgres --force postgresql
: 1491239323:0;brew install postgres
: 1491239339:0;ls -l /usr/local/var/postgres
: 1491239353:0;initdb /usr/local/var/postgres
: 1491239405:0;psql -v
: 1491239409:0;psql --version
: 1491239441:0;psql -h 
: 1491239444:0;psql 
: 1491239450:0;tail /usr/local/var/postgres/server.log
: 1491239466:0;cd /usr/local/var/postgres/
: 1491239493:0;mv postgres postgres9.4
: 1491239522:0;brew services postgres stop
: 1491239540:0;brew services postgresql stop\\
: 1491239544:0;brew services postgresql \

: 1491239553:0;brew services stoppostgresql \

: 1491239557:0;brew services stop postgresql \

: 1491239564:0;brew servies list
: 1491239592:0;psql
: 1491239606:0;psql --help
: 1491239632:0;locate plist
: 1491239655:0;ls -l /usr/local/Cellar/postgresql/9.6.2/homebrew.mxcl.postgresql.plist
: 1491239671:0;cp /usr/local/Cellar/postgresql/9.6.2/homebrew.mxcl.postgresql.plist ~/Library/LaunchAgents/
: 1491239674:0;ls -l ~/Library/LaunchAgents
: 1491239683:0;more ~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist
: 1491239808:0;rebenv local 2.3.3
: 1491239815:0;rbenv local 2.3.3
: 1491239897:0;ruby install 2.3.3
: 1491239911:0;history | grep ruby | grep install
: 1491239931:0;more Gemfile | grep ruby
: 1491239938:0;rbenv local 2.3.0
: 1491240376:0;gem install bundler
: 1491240392:0;grep 3dLPMBp77q4gwJHpPx2u .env
: 1491240396:0;grep 3dLPMBp77q4gwJHpPx2u *
: 1491240405:0;vi .env
: 1491240522:0;gem env
: 1491240604:0;env | grep GEM
: 1491240810:0;bundle config gem.fury.io heschmidt04:HF0Sk13s
: 1491240922:0;bundle config https://gem.fury.io heschmidt04:d_KzY8hFUfNWdYLGeQsb
: 1491240998:0;vi .bundle
: 1491241006:0;vi .bundle/config
: 1491241028:0;bundle config gem.fury.io heschmidt04:d_KzY8hFUfNWdYLGeQsb
: 1491242439:0;more ~/.bundle/config
: 1491242516:0;vi ~/.bundle/config
: 1491242846:0;brew list postgresql
: 1491242868:0;brew info postgresql
: 1491243123:0;sudo apt-get
: 1491243135:0;sudo apt-get install libreadline-dev
: 1491243148:0;which apt-get
: 1491243244:0;brew update readline
: 1491243251:0;brew upgrade readline
: 1491243311:0;brew reinstall postgresql 
: 1491243338:0;psql -d postges
: 1491243381:0;more /Users/heidischmidt/Documents/git/bamboo-server/config/application.rb
: 1491243397:0;more .env
: 1491243472:0;rbenv local
: 1491243504:0;brew list ruby
: 1491243529:0;brew list readline-dev
: 1491243532:0;brew list readline
: 1491243538:0;brew search readline-dev
: 1491243600:0;brew reinstall readline 
: 1491243672:0;brew services stop postgresql
: 1491243679:0;brew remove readline
: 1491243692:0;brew install readline
: 1491243710:0;brew link readline --force
: 1491243739:0;brew services start postgresql
: 1491243772:0;ls -l brew link readline --force
: 1491243834:0;ln -s /usr/local/opt/readline/lib/libreadline.7.0.dylib /usr/local/opt/readline/lib/libreadline.6.dylib
: 1491243836:0;ls -l /usr/local/opt/readline/lib/
: 1491243924:0;bundle exec rake db:create && bundle exec rake db:reset
: 1491244010:0;psql -d postgres
: 1491244111:0;psql -d hello200_development
: 1491244175:0;bundle exec rake db:fixtures
: 1491244302:0;bundle exec rake -T
: 1491244362:0;RAILS_ENV=test
: 1491244363:0;bundle exec rake db:fixtures:load
: 1491244372:0;RAILS_ENV=development
: 1491244376:0;echo $RAILS_ENV
: 1491244480:0;brew upgrade redis
: 1491244488:0;brew service start redis
: 1491244493:0;brew services start redis
: 1491244542:0;bundle exec rails server -p 5000
: 1491244636:0;nslookup hello200.myhdev.com
: 1491244761:0;more .env 
: 1491244907:0;gem list
: 1491244934:0;gem search prometheus/client
: 1491244946:0;gem list rack
: 1491244955:0;gem list prometheus
: 1491245033:0;gem search prometheus-client
: 1491245313:0;gem install prometheus-client
: 1491245725:0;rake -T 
: 1491245758:0;rake spec:metrics
: 1491245855:0;vi config.ru
: 1491245956:0;ls -l Procfile
: 1491245960:0;cat Procfile
: 1491245991:0;gem list puma
: 1491246005:0;bundle show prometheus-client
: 1491246222:0;history | grep gem install
: 1491246227:0;history | grep gem | grep install
: 1491246269:0;gem install prometheus-client -v 0.7.0-rc.1
: 1491246283:0;gem list prometheus-client
: 1491246305:0;vi Gemfile
: 1491246419:0;cat config.ru
: 1491246430:0;bundle exec puma
: 1491247340:0;history | grep ln
: 1491325777:0;history | grep yaml
: 1491325785:0;kubectl --namespace prometheus get configmap --show-labels prometheus-config -o yaml
: 1491325845:0;kubectl --namespace prometheus get configmap --show-labels prometheus-config -o yaml > kubectl-namespace-prom-get-configmap.040417.yaml
: 1491325999:0;kubectl get pods -o wide  --all-namespaces | grep 172
: 1491326014:0;kubectl get pods -o wide  --all-namespaces | grep 100.94.92
: 1491326033:0;kubectl get pods -o wide  --all-namespaces | grep 100.94.92.157
: 1491326043:0;kubectl get pods -o wide  --all-namespaces | grep 100.94.92.172
: 1491326385:0;ls -l con*
: 1491327793:0;cd app/models
: 1491327802:0;more background_latency_checker.rb
: 1491327838:0;find . -name "clockwork*" -print
: 1491327850:0;find . -name "*sidekiq*" -print
: 1491327856:0;cat ./config/initializers/sidekiq.rb
: 1491327973:0;grep job kubectl-namespace-prom-get-configmap.040417.yaml
: 1491329021:0;kubectl get pods -o wide  --all-namespaces | grep wellbeing
: 1491329037:0;kubectl get pods -o wide  --all-namespaces | grep well
: 1491329271:0;more config/clock.rb
: 1491329290:0;kubectl get pods -o wide  --all-namespaces | grep conc
: 1491329423:0;kubectl get services 
: 1491329429:0;kubectl get services -o wide  --all-namespaces
: 1491329451:0;kubectl get services -o wide  --namespace wbt-staging
: 1491329466:0;kubectl get services -o yaml  --namespace wbt-staging
: 1491329531:0;vi ~/Documents/kubernetes/kubectl-wbt-service-info.yml.log
: 1491332969:0;cd ../..
: 1491333011:0;cp kubectl-namespace-prom-get-configmap.040417.yaml kubectl-namespace-prom-configmap-wbid-update.yml
: 1491333492:0;shipit config_get wellbeingid-staging
: 1491333506:0;shipit --help 
: 1491333512:0;shipit env --help
: 1491333522:0;shipit environment --help
: 1491333534:0;shipit environment config-get 
: 1491333543:0;shipit environment config-get wellbeingid-staging
: 1491333549:0;history | grep config-get
: 1491333561:0;shipit env config-get wellbeingid-staging
: 1491355293:0;vi kubectl-namespace-prom-get-configmap.040417.yaml
: 1491355676:0;more kubectl-namespace-prom-configmap-wbid-update.yml
: 1491355724:0;vi kubectl-namespace-prom-configmap-wbid-update.yml
: 1491355757:0;mv kubectl-namespace-prom-configmap-wbid-update.yml kubectl-namespace-prom-configmap-wbid-update.mockup
: 1491355774:0;cp kubectl-namespace-prom-get-configmap.040417.yaml prom.yaml
: 1491355778:0;vi prom.yaml
: 1491403775:0;kubectl get config-map --namespace prometheus
: 1491403787:0;kubectl get config-maps --namespace prometheus -o yaml
: 1491403794:0;history | grep config 
: 1491403814:0;kubectl --namespace prometheus get configmap prometheus-config -o yaml
: 1491403819:0;kubectl --namespace prometheus get configmap prometheus-config 
: 1491403823:0;kubectl --namespace prometheus get configmap prometheus-config -o json
: 1491403892:0;deis auth:login https://deis.myhstg.com --ssl-verify=true --username=heidi.schmidt --password=m3fkbpCirTEmikGvmQkbxKoL
: 1491403917:0;cp /Users/heidischmidt/.deis/client.json /Users/heidischmidt/.deis/production.json
: 1491403975:0;DEIS_PROFILE=production deis login
: 1491403984:0;DEIS_PROFILE=production deis auth:login
: 1491404000:0;DEIS_PROFILE=production deis keys
: 1491404005:0;DEIS_PROFILE=production deis keys:list
: 1491404074:0;deis logout
: 1491404086:0;more /Users/heidischmidt/.deis/production.json
: 1491404188:0;DEIS_PROFILE=production deis auth:login https://deis.meyouhealth.com --ssl-verify=true --username=heidi.schmidt 
: 1491404453:0;deis auth:login https://deis.myhstg.com --ssl-verify=true --username=heidi.schmidt --password=4JxuTyVxfYkh
: 1491404519:0;DEIS_PROFILE=staging deis auth:login https://deis.myhstg.com --ssl-verify=true --username=heidi.schmidt --password=4JxuTyVxfYkh
: 1491404558:0;DEIS_PROFILE=staging deis ps:list -a 
: 1491404562:0;DEIS_PROFILE=staging deis ps:list 
: 1491405829:0;shipi ssh walkadoo-staging
: 1491407099:0;kubectl get configmaps --namespace prometheus 
: 1491412159:0;ssh -t -A heidischmidt@54.152.155.24 ssh -t heidischmidt@10.0.5.165
: 1491413544:0;ssh -t -A heidischmidt@54.152.155.24 ssh -t heidischmidt@10.0.5.215
: 1491415298:0;rm -rf docs
: 1491415336:0;mkdir prometheus
: 1491415342:0;git clone git@github.com:prometheus/docs.git
: 1491415375:0;bundle install
: 1491415396:0;cd docs
: 1491415400:0;bundle
: 1491415569:0;bundle exec nanoc
: 1491415593:0;bundle exec guard
: 1491415618:0;cd prometheus/docs
: 1491415620:0;bundle exec nanoc view
: 1491424906:0;shipit deployment logs 5153
: 1491424934:0;shipit deployment logs 5153 > Documents/walkadoo/shipit-deployment-logs-5153.log
: 1491424941:0;more Documents/walkadoo/shipit-deployment-logs-5153.log
: 1491425021:0;shipit deployment logs 5153 | more
: 1491425032:0;shipit deployment logs 5153 | less
: 1491425504:0;grep -B 10 clockwork  Documents/walkadoo/shipit-deployment-logs-5153.log
: 1491426248:0;shipit env walkadoo-staging
: 1491426285:0;shipit env show walkadoo-staging
: 1491426333:0;shipit deployment logs 5152 | less
: 1491426837:0;shipit create deployment walkadoo-production --help
: 1491428230:0;shipit deployment logs 5154
: 1491428328:0;history | grep knife | grep delete
: 1491428591:0;shipit deployment logs 5155
: 1491428711:0;history | grep knife | grep list
: 1491428730:0;knife search chef_environment:walkadoo-production
: 1491429007:0;knife search chef_environment:walkadoo-production |grep -B 4 Node
: 1491429018:0;knife search chef_environment:walkadoo-production |grep Node
: 1491429037:0;knife search chef_environment:walkadoo-production |grep Node > Documents/walkadoo/knife-list
: 1491429054:0;paste Documents/walkadoo/knife-list Documents/walkadoo/ec2-list
: 1491429164:0;paste Documents/walkadoo/knife-list Documents/walkadoo/ec2-list Documents/aws-gui-list
: 1491429432:0;grep i-e5706b7c Documents/aws-gui-list
: 1491429449:0;grep i-09fe5bd73394f6296 Documents/aws-gui-list
: 1491429463:0;grep i-0c4f5cd5a0fbd9ea0 Documents/aws-gui-list
: 1491429479:0;grep i-0fefc7d3b32079855 Documents/aws-gui-list
: 1491429493:0;grep i-94877907 Documents/aws-gui-list
: 1491429669:0;vi Documents/aws-gui-list
: 1491429926:0;knife client delete -y i-0eedb3787fb1b7fba
: 1491429996:0;shipit deployment logs 5155 | less
: 1491432274:0;history | grep client | grep delete
: 1491432290:0;knife node delete -y i-0eedb3787fb1b7fba
: 1491432324:0;history | grep shipit | grep deployment | grep create
: 1491432332:0;shipit deployment create walkadoo-production 34bfd15503433a64a409a073086ed74d5bdea7ee
: 1491432343:0;rbenv local 2.1.5 
: 1491432438:0;shipit deployment logs 5156
: 1491438163:0;shipi ssh walkadoo-production
: 1491503007:0;grep job *
: 1491503020:0;more ec2.tf
: 1491503027:0;more provider.tf
: 1491503031:0;more variables.tf
: 1491503037:0;more elasticache.tf
: 1491503491:0;shipit ssh walkadoo-staging
: 1491504507:0;ssh -t -A heidischmidt@52.86.252.208 ssh -t heidischmidt@10.0.5.178
: 1491504720:0;ssh -t -A heidischmidt@54.152.155.24 ssh -t heidischmidt@10.0.5.178
: 1491531342:0;kubectl describe pod prometheus-3412682896-649hm 
: 1491531350:0;kubectl describe pod prometheus-3412682896-649hm --namespace prometheus
: 1491531474:0;kubectl get configmaps --namespace prometheus prometheus-rules -o yaml
: 1491531505:0;kubectl get configmaps --namespace prometheus prometheus-rules -o yaml > Documents/kubernetes/kubectl-get-configmaps-prom-rules.yaml
: 1491531524:0;cp Documents/kubernetes/kubectl-get-configmaps-prom-rules.yaml Documents/kubernetes/kubectl-get-configmaps-prom-rules-bkgrd-high-thresh.yaml
: 1491532260:0;more /etc/hosts
: 1491532268:0;vi /etc/hosts
: 1491532296:0;sudo vi /etc/hosts
: 1491532604:0;history | grep time
: 1491532637:0;history | grep kubectl 
: 1491532721:0;vi Documents/kubernetes/kubectl-get-configmaps-prom-rules-bkgrd-high-thresh.yaml
: 1491533335:0;cat Documents/kubernetes/kubectl-get-configmaps-prom-rules-bkgrd-high-thresh.yaml
: 1491534212:0;grep config *
: 1491534240:0;kubectl get configmaps prometheus --namespace prometheus 
: 1491534247:0;kubectl get configmap prometheus --namespace prometheus 
: 1491534257:0;history | grep config | grep get
: 1491534273:0;kubectl get configmaps --namespace prometheus prometheus-config -o yaml
: 1491535419:0;aws ec2 describe-instance-status --filters Name=instance-status.status,Values=impaired
: 1491535451:0;aws ec2 describe-instance-status --instance-ids i-94877907
: 1491589561:0;grep bearer kubectl-namespace-prom-get-configmap.040417.yaml
: 1491590646:0;cd Downloads/prometheus-1.5.2.linux-amd64
: 1491590653:0;which nano
: 1491590676:0;nano prometheus.yml
: 1491590784:0;nohup ./prometheus-1.3.1.linux-amd64/prometheus -storage.local.memory-chunks=10000 &
: 1491590827:0;nohup ./prometheus -storage.local.memory-chunks=10000 &
: 1491590853:0;rm no
: 1491590856:0;rm nohup.out
: 1491590867:0;brew search prometheus
: 1491590878:0;brew install prometheus
: 1491590909:0;./prometheus --version
: 1491590926:0;brew list prometheus
: 1491590946:0;ls -l /usr/local/Cellar/prometheus/1.5.2
: 1491590955:0;ls -l /usr/local/Cellar/prometheus/1.5.2/*
: 1491590971:0;more /usr/local/Cellar/prometheus/1.5.2/README.md
: 1491591014:0;ls -ltra /usr/local/Cellar/prometheus
: 1491591019:0;ls -ltra /usr/local/Cellar/prometheus/1.5.2
: 1491591032:0;cp prometheus.yml /usr/local/Cellar/prometheus/1.5.2
: 1491591064:0;brew services start prometheus
: 1491591075:0;history | grep plist
: 1491591085:0;ls -ltra /usr/local/Cellar/prometheus/1.5.2/*
: 1491591101:0;ls -ltra /usr/local/Cellar/prometheus/1.5.2/li*
: 1491591106:0;ls -ltra /usr/local/Cellar/prometheus/1.5.2/li*/con*
: 1491591180:0;prometheus --help
: 1491591212:0;nohup prometheus -storage.local.memory-chunks=10000 &
: 1491591243:0;ls -l data
: 1491591263:0;ipconfig 
: 1491591266:0;ifconfig -a
: 1491591405:0;wget https://github.com/juliusv/prometheus_demo_service/releases/download/0.0.4/prometheus_demo_service-0.0.4.linux-amd64.tar.gz
: 1491591415:0;tar xvfz prometheus_demo_service-0.0.4.linux-amd64.tar.gz
: 1491591423:0;tar -xvfz prometheus_demo_service-0.0.4.linux-amd64.tar.gz
: 1491591432:0;tar -xvf prometheus_demo_service-0.0.4.linux-amd64.tar.gz
: 1491591441:0;gunzip dprometheus_demo_service-0.0.4.linux-amd64.tar.gz
: 1491591472:0;ls -l dp*
: 1491591481:0;wget https://github.com/juliusv/prometheus_demo_service/releases/download/0.0.4/prometheus_demo_service-0.0.4.linux-amd64.tar.gz 
: 1491591532:0;wget https://github.com/juliusv/prometheus_demo_service/releases/download/0.0.4/prometheus_demo_service-0.0.4.linux.amd64.tar.gz
: 1491591573:0;tar -xvfz prometheus_demo_service-0.0.4.linux.amd64.tar.gz
: 1491591579:0;tar xvfz prometheus_demo_service-0.0.4.linux.amd64.tar.gz
: 1491591583:0;prometheus_demo_service-0.0.4.linux.amd64.tar.gzls -ltra
: 1491591608:0;cd prometheus_demo_service
: 1491592550:0;prometheus_demo_service -listen-address=:8080 &
: 1491592573:0;prometheus_demo_service --help
: 1491592595:0;ls -l prometheus-1.5.2.linux-amd64
: 1491592765:0;ls -le prometheus_demo_service
: 1491592780:0;ls -le prometheus-1.5.2.linux-amd64/prometheus
: 1491592825:0;sudo nohup prometheus_demo_service -listen-address=:8080 &
: 1491592834:0;fg
: 1491592853:0;sudo su -
: 1491592983:0;file prometheus
: 1491593071:0;rm prometheus_demo_service*
: 1491593085:0;tar xvfz prometheus_demo_service-0.0.4.tar.gz
: 1491593095:0;tar -xvfz prometheus_demo_service-0.0.4.tar.gz
: 1491593103:0;gunzip prometheus_demo_service-0.0.4.tar.gz
: 1491593111:0;ls -l prom*
: 1491593137:0;wget https://github.com/juliusv/prometheus_demo_service/archive/0.0.2.tar.gz
: 1491593149:0;ls -l 0.0.2.tar.gz
: 1491593167:0;mv 0.0.2.tar.gz prometheus-demo-service-0.0.2.tar.gz
: 1491593177:0;file prometheus-demo-service-0.0.2.tar.gz
: 1491593188:0;tar -tvfz prometheus-demo-service-0.0.2.tar.gz
: 1491593196:0;tar -tvf prometheus-demo-service-0.0.2.tar.gz
: 1491593205:0;ls -l prometheus_demo_service-0.0.2
: 1491593247:0;uname -a
: 1491593276:0;rm prometheus-demo-service-0.0.2.tar.gz
: 1491593290:0;tar xvfz prometheus_demo_service-0.0.4.darwin.amd64.tar.gz
: 1491593297:0;ls -l prometheus_demo_service
: 1491593301:0;file prometheus_demo_service
: 1491593317:0;./prometheus_demo_service -listen-address=:8080 &
: 1491593326:0;./prometheus_demo_service -listen-address=:8081 &
: 1491593334:0;./prometheus_demo_service -listen-address=:8082 &
: 1492018616:0;kubectl exec -ti prometheus-2406627989-bqfhq --namespace prometheus shâ€‚â€‚
: 1492018640:0;kubectl exec -ti prometheus-2406627989-qfj01 --namespace prometheus shâ€‚â€‚
: 1492018657:0;kubectl exec -ti prometheus-2406627989-qfj01 --namespace prometheus ls /var/â€‚â€‚
: 1492018662:0;kubectl exec -ti prometheus-2406627989-qfj01 --namespace prometheus ls /var
: 1492018668:0;kubectl exec -ti prometheus-2406627989-qfj01 --namespace prometheus ls /var/secrets
: 1492018673:0;kubectl exec -ti prometheus-2406627989-qfj01 --namespace prometheus ls /var/secrets/metric-token
: 1492018679:0;kubectl exec -ti prometheus-2406627989-qfj01 --namespace prometheus ls /var/secrets/metric-token/
: 1492018683:0;kubectl exec -ti prometheus-2406627989-qfj01 --namespace prometheus ls /var/secrets/
: 1492018697:0;kubectl exec -ti prometheus-2406627989-qfj01 --namespace prometheus cat /var/secrets/metrics-token
: 1492018705:0;kubectl exec -ti prometheus-2406627989-qfj01 --namespace prometheus cat /var/secrets/metrics-token/metrics-token
: 1492018752:0;more more kubectl-wbt-service-info.yml.log
: 1492018879:0;more kubectl-get-configmaps-prom-rules.yaml
: 1492019098:0;ps -ef | grep prom > local-prometheus-tutorial-running.log
: 1492019117:0;kill -9 82396 89011 89043 89075
: 1492019120:0;ps -ef | grep prom
: 1492019229:0;kubectl get configmap prometheus-rules --namespace prometheus -o yaml > kubectl-configmap-prom-alert-rules.041217.yaml
: 1492019243:0;diff -u kubectl-get-configmaps-prom-rules.yaml kubectl-get-configmaps-prom-rules-bkgrd-high-thresh.yaml
: 1492019267:0;kubectl get events --all-namespaces --sort-by \".lastTimestamp\"
: 1492019274:0;kubectl get events --all-namespaces --sort-by ".lastTimestamp"
: 1492019364:0;kubectl get configmap prometheus --namespace prometheus -o yaml > kubectl-configmap-prometheus.041217.yaml
: 1492019404:0;kubectl get configmap prometheus-config --namespace prometheus -o yaml > kubectl-configmap-prometheus.041217.yaml
: 1492019420:0;cp kubectl-configmap-prometheus.041217.yaml ~/Documents/git/k8s-ops
: 1492019430:0;cd ~/Documents/git/k8s-ops
: 1492019445:0;more add-ons/prometheus/prometheus-config/prometheus.yml
: 1492019463:0;diff -u add-ons/prometheus/prometheus-config/prometheus.yml kubectl-configmap-prometheus.041217.yaml
: 1492019497:0;cp kubectl-configmap-prometheus.041217.yaml add-ons/prometheus/prometheus-config
: 1492019688:0;ls -l add-ons
: 1492019705:0;more alertmanager.yml
: 1492019759:0;more kubectl-configmap-prom-alert-rules.041217.yaml
: 1492019771:0;more kubectl-get-configmaps-prom-rules-bkgrd-high-thresh.yaml
: 1492019825:0;vi sidekiq-metric-alert.rules
: 1492020054:0;cat sidekiq-metric-alert.rules
: 1492020155:0;more deployment-replicats-unavailable.rules
: 1492020189:0;vimdiff prometheus.yml kubectl-configmap-prometheus.041217.yaml
: 1492021017:0;man curl 
: 1492021054:0;curl https://account.myhstg.com/metrics
: 1492021060:0;curl -l  https://account.myhstg.com/metrics
: 1492021364:0;kubectl get deployment --namespace prometheus prometheus -o yaml
: 1492021400:0;kubectl get deployment --namespace prometheus prometheus -o yaml > kubectl-get-deployment-prometheus.yml
: 1492021791:0;cd ../prometheus-rules
: 1492021898:0;kubectl --namespace prometheus logs alertmanager-2060873422-ps1cp
: 1492021918:0;kubectl --namespace prometheus logs prometheus-2406627989-22lk9
: 1492024396:0;kubectl get secret metrics-token -o yaml
: 1492024404:0;kubectl get secret metrics-token -o yaml --namespace prometheus
: 1492024993:0;which base64
: 1492024999:0;man base64
: 1492025193:0;echo "cat /var/secrets/metrics-token/metrics-token\
ad778be3abdaabd0b856c8b7563a08cd5023b400321aca6b213f6fcaa50423a0"
: 1492025209:0;echo "kubectl get secret metrics-token -o yaml --namespace prometheus                                                                                                                         1 âƒ¦µ\
apiVersion: v1\
data:\
  metrics-token: YWQ3NzhiZTNhYmRhYWJkMGI4NTZjOGI3NTYzYTA4Y2Q1MDIzYjQwMDMyMWFjYTZiMjEzZjZmY2FhNTA0MjNhMA=="
: 1492025235:0;echo "ad778be3abdaabd0b856c8b7563a08cd5023b400321aca6b213f6fcaa50423a0" | base64
: 1492025411:0;echo "ad778be3abdaabd0b856c8b7563a08cd5023b400321aca6b213f6fcaa50423a0" | base64 > shipit-var-base64
: 1492025459:0;vi kubectl-get-secret-metrics-token
: 1492025470:0;paste kubectl-get-secret-metrics-token
: 1492025477:0;paste shipit-var-base64
: 1492087308:0;;hgrep 
: 1492087316:0;history | grep wellbeing
: 1492087339:0;ssh -A -L 9710:wellbeingid-production.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:5432 heidischmidt@54.225.238.195 -N
: 1492087451:0;psql -E -h 127.0.0.1 -p 9710 -U meyouhealth  --dbname=wellbeingid_production
: 1492090983:0;history | grep shipit | grep wellbeing
: 1492090989:0;curl https://account.myhstg.com/metrics -H "Authorization: <value from shipit config>" 
: 1492091156:0;kubectl exec -ti prometheus-2406627989-22lk9 --namespace prometheus /bin/sh
: 1492091370:0;curl https://account.myhstg.com/metrics -H "Authorization: ad778be3abdaabd0b856c8b7563a08cd5023b400321aca6b213f6fcaa50423a0"
: 1492093850:0;history | grep "ssh -t"
: 1492099215:0;kubectl --namespace prometheus get pods -l app=prometheus -o name | sed 's/^.*\///' 
: 1492099817:0;find . -name background_latency_checker* -print
: 1492099825:0;find . -name *latency_checker* -print
: 1492099831:0;find . -name *latency* -print
: 1492099879:0;more clock.rb
: 1492099886:0;grep -i latency clock.rb
: 1492099922:0;cd ../app/jobs
: 1492099944:0;cd controllers
: 1492100106:0;cd spec
: 1492100111:0;cd requests
: 1492100118:0;more metrics_spec.rb
: 1492100434:0;cp kubectl-configmap-prometheus.041217.yaml kubectl-configmap-prometheus.wbid-prd-stg.041317.yml
: 1492100438:0;vi kubectl-configmap-prometheus.041217.yaml
: 1492100565:0;cat kubectl-configmap-prometheus.wbid-prd-stg.041317.yml
: 1492100602:0;cp kubectl-configmap-prometheus.wbid-prd-stg.041317.yml wbid-metrics.yml
: 1492100605:0;vi wbid-metrics.yml
: 1492100652:0;history | grep kubectl | grep configmap
: 1492102005:0;more wbid-postgres.041317-758am.log
: 1492102835:0;kubectl describe pod prometheus-2406627989-pg2d2 --namespace prometheus
: 1492107982:0;aws rds describe-pending-maintenance-actions 
: 1492110672:0;kubectl get pods -o wide  --all-namespaces | grep walk
: 1492110729:0;shipit deployment logs 5224
: 1492110948:0;cd Documents/walkadoo
: 1492110962:0;cat ec2-list
: 1492110973:0;mv ec2-list ec2-list-old
: 1492110976:0;vi ec2-list
: 1492111005:0;vi chef-list
: 1492111024:0;paste ec2-list chef-list
: 1492111031:0;cat ec2-list | sort
: 1492111037:0;cat ec2-list | sort > ec2-list-sorted
: 1492111048:0;cat chef-list| sort > chef-list-sorted
: 1492111058:0;paste ec2-list-sorted chef-list-sorted
: 1492112054:0;shipit deployment logs 5225
: 1492112066:0;rbenv local 
: 1492112102:0;rbenv local 2.2.0
: 1492112133:0;;knifedel
: 1492112136:0;knife client delete -y i-0fefc7d3b32079855
: 1492112141:0;kinfe node delete -y i-0fefc7d3b32079855
: 1492112149:0;knife node delete -y i-0fefc7d3b32079855
: 1492112170:0;history | grep knife | grep searcg
: 1492112171:0;history | grep knife | grep searc
: 1492112184:0;knife search chef_environment:walkadoo-production |grep Node > Documents/walkadoo/knife-list-041317
: 1492112190:0;more Documents/walkadoo/knife-list-041317
: 1492112275:0;shipit deployment log 5225
: 1492112378:0;shipit deployment create walkadoo-production 1e27d94a3064c553bb7d59bc39694c336547fc92
: 1492112420:0;shipit deployment log 1e27d94a3064c553bb7d59bc39694c336547fc92
: 1492113264:0;shipit deployment log 5226
: 1492114056:0;shipit deployment log 5226 | grep Updating
: 1492114167:0;shipit deployment log 5226 | grep quay
: 1492114212:0;docker pull quay.io/myhadmin/walkadoo:1e27d94a3064c553bb7d59bc39694c336547fc92
: 1492175072:0;which kubelet-wrapper
: 1492175082:0;brew search kubelet-wrapper
: 1492176010:0;script kubectl-port-fwd-9090-session-debug.log 
: 1492176019:0;kubectl --namespace prometheus get pods -l app=prometheus -o name | sed 's/^.*\///' | xargs -I{} kubectl --namespace prometheus port-forward {} 9090:9090  --v=9
: 1492177486:0;kops export neywk.k8s.meyouhealth.com
: 1492177528:0;kops export kubecfg neywk.k8s.meyouhealth.com
: 1492178561:0;cd kubernetes
: 1492178598:0;mv kubectl-port-fwd-9090-session-debug.log  ~/Documents/kubernetes/
: 1492178614:0;more ~/Documents/kubernetes/kubectl-port-fwd-9090-session-debug.log
: 1492439015:0;kubectl --namespace hurby-server-staging logs hurby-server-staging-web-1455758217-x4spl
: 1492439035:0;kubectl get events --all-namespaces 
: 1492439277:0;ssh admin@ip-172-20-197-199.ec2.internal
: 1492441494:0;ssh -t heidischmidt@54.221.12.183
: 1492441510:0;history | grep dc-staging
: 1492441710:0;ssh heidischmidt@54.221.12.183 
: 1492442873:0;kubectl delete hurby-server-staging-houston-1327347804-gqtd5 --namespace hurby-server-staging
: 1492443491:0;history | grep aws 
: 1492443594:0;aws ec2 describe-instance --instance-ids i-07c39bab8aeafd833
: 1492443621:0;aws ec2 describe-hosts --instance-ids i-07c39bab8aeafd833
: 1492443629:0;aws ec2 describe-instance-status --instance-ids i-07c39bab8aeafd833
: 1492443680:0;ssh heidischmidt@54.221.12.183
: 1492443760:0;ssh heidischmidt@54.158.30.198
: 1492443770:0;ssh ubuntu@54.158.30.198
: 1492443919:0;kubectl describe node ip-172-20-197-199.ec2.internal 
: 1492443957:0;kubectl get pods -o wide  --namespace hurby-server-staging
: 1492444236:0;kubectl describe pod hurby-server-staging-houston-1327347804-gqtd5 --namespace hurby-server-staging
: 1492444253:0;kubectl describe pod hurby-server-staging-houston-1327347804-mh9kw --namespace hurby-server-staging
: 1492444307:0;history | grep "get node"
: 1492444314:0;kubectl get nodes --all-namespaces
: 1492444380:0;kubectl get events --all-namespaces | grep hurby
: 1492444750:0;kubectl get pods -o wide  --all-namespaces | grep 119
: 1492444767:0;kubectl get pods -o wide  --all-namespaces | grep 199
: 1492448772:0;kubectl get pods -o wide  --all-namespaces | grep 172.20.199.117
: 1492448826:0;kubectl get pods -o wide  --all-namespaces | grep hurby
: 1492450025:0;;kgn
: 1492450048:0;echo "https://kubernetes.io/docs/tasks/debug-application-cluster/debug-cluster/" 
: 1492450048:0;echo "check for ready state" 
: 1492450048:0;echo "ssh admin@node" 
: 1492450048:0;echo "sudo journalctl -u kubelet" 
: 1492450829:0;ssh admin@ip-172-24-121-242  
: 1492529446:0;kill %9
: 1492529469:0;ps -ef | grep proxy
: 1492529478:0;kill -9 75079
: 1492535399:0;history | grep rds
: 1492535407:0;history | grep aws | grpe rds
: 1492535412:0;history | grep aws | grep rds
: 1492614811:0;shipit ssh walkadoo-production
: 1492614913:0;shipit deployment logs 5236 | less
: 1492614949:0;history | grep knife | grep search
: 1492614980:0;knife search node "chef_environment:walkadoo-production" > Documents/walkadoo/wd-chef-node-search-041917.log
: 1492615107:0;knife search node "chef_environment:quitnet-production" > Documents/quitnet/qn-chef-node-search-041917.log
: 1492615115:0;more Documents/walkadoo/wd-chef-node-search-041917.log
: 1492615124:0;more Documents/walkadoo/wd-chef-node-search-041917.log | grep node
: 1492615137:0;cat Documents/walkadoo/wd-chef-node-search-041917.log | grep -i "node name"
: 1492615172:0;cat Documents/quitnet/qn-chef-node-search-041917.log | grep -i "node name"
: 1492615264:0;shipit env show quitnet-production
: 1492615275:0;shipit deployment logs 5240
: 1492616420:0;shipit ssh quitnet-production
: 1492622315:0;shipit deployment logs 5239
: 1492622695:0;shipit deployment logs 5239 > shipit-deployment-logs-5239-before-destroy.log
: 1492622815:0;shipit env show walkadoo-production 
: 1492624911:0;cd - 
: 1492626159:0;shipit deployment logs 5241
: 1492626777:0;kubectl --namespace iris-staging logs iris-staging-web-2181809286-cjfh3
: 1492626954:0;kubectl --namespace iris-staging logs iris-staging-web-2181809286-g8nv2
: 1492627284:0;kubectl get pods -o wide  --all-namespaces | grep "ip-172-20-142-230.ec2.internal"
: 1492627289:0;kubectl get pods -o wide  --all-namespaces | grep "ip-172-20-142-230.ec2.internal" | wc -l 
: 1492627374:0;kubectl get pods -o wide  --all-namespaces | grep "ip-172-20-142-230.ec2.internal" | grep Terminating
: 1492629216:0;kubectl get pods -o wide  --all-namespaces |  grep Terminating
: 1492629234:0;kubectl describe node ip-172-20-142-230.ec2.internal
: 1492699925:0;kubectl describe node ip-172-20-142-230.ec2.internal | tail -50 
: 1492700126:0;ssh admin@ip-172-20-142-230.ec2.internal
: 1492700240:0;ssh admin@ip-172-20-143-248.ec2.internal 
: 1492700311:0;ssh admin@ip-172-20-162-53.ec2.internal 
: 1492700352:0;ssh admin@ip-172-20-164-113.ec2.internal
: 1492700436:0;ssh admin@ip-172-20-209-91.ec2.internal
: 1492700682:0;ssh admin@ip-172-20-142-230.ec2.internal 
: 1492700724:0;ssh admin@ip-172-20-199-117.ec2.internal
: 1492703208:0;shipit deployment logs 5249
: 1492703502:0;docker pull quay.io/myhadmin/walkadoo:7d8babacfee42a1eae6c6d4eb9d99f6eb273df3c
: 1492703678:0;shipit env show walkadoo-production
: 1492703688:0;;hgrep
: 1492703698:0;hgrep
: 1492703738:0;shipit deployment create walkadoo-production 7d8babacfee42a1eae6c6d4eb9d99f6eb273df3c
: 1493055760:0;history | grep kubectl | grep edit
: 1493055788:0;kubectl edit deployment prometheus-rules --namespace prometheus
: 1493055845:0;kubectl edit configmap prometheus-rules --namespace prometheus
: 1493056295:0;kubectl get configmap prometheus-rules --namespace prometheus
: 1493056300:0;kubectl get configmap prometheus-rules --namespace prometheus -o json
: 1493057270:0;helm --help
: 1493057323:0;mkdir helm-charts
: 1493057333:0;helm fetch stable/prometheus
: 1493057343:0;ls -lt
: 1493057357:0;tar -tvfz prometheus-2.0.4.tgz
: 1493057372:0;tar -xvfz prometheus-2.0.4.tgz
: 1493057383:0;gunzip  prometheus-2.0.4.tgz
: 1493057390:0;tar -tvf prometheus-2.0.4.tar
: 1493057396:0;tar -xvf prometheus-2.0.4.tar
: 1493057440:0;more Chart.yaml
: 1493057445:0;ls templates
: 1493057463:0;more templates/alertmanager-configmap.yaml
: 1493057620:0;helm search
: 1493057687:0;brew upgrade
: 1493134523:0;locate prometheus
: 1493134531:0;/usr/local/Cellar/prometheus --help
: 1493134556:0;./prometheus --help
: 1493134567:0;more nohup.out
: 1493134723:0;ls -l 7d8babacfee42a1eae6c6d4eb9d99f6eb273df3c
: 1493134836:0;kubectl get pods -o wide  --all-namespaces | grpe prom
: 1493134897:0;cd ../Documents/kubernetes
: 1493134911:0;kubectl describe pod prometheus-2406627989-pthfn --namespace prometheus > prometheus-pod-describe-info.log
: 1493134917:0;vi prometheus-pod-describe-info.log
: 1493135246:0;history | grep attache
: 1493135247:0;history | grep attach
: 1493135274:0;kubectl attach prometheus-2406627989-pthfn --namespace prometheus 
: 1493135283:0;kubectl attach prometheus-2406627989-pthfn --namespace prometheus -i -t 
: 1493135414:0;history | grep prometheus | grep sh
: 1493135544:0;kubectl describe prometheus-2406627989-pthfn --namespace prometheus
: 1493135554:0;kubectl describe pod prometheus-2406627989-pthfn --namespace prometheus
: 1493135598:0;kubectl describe pod alertmanager-2060873422-pw2hj --namespace prometheus
: 1493135898:0;kubectl get deployment --namespace prometheus
: 1493136001:0;kubectl get pods -o wide  --all-namespaces | grep deis | grep builder
: 1493136016:0;kubectl describe pod deis-builder-157405133-lvwzg --namespace deis
: 1493136391:0;cd prometheus-1.5.2.linux-amd64
: 1493136397:0;./prometheus help
: 1493136403:0;prometheus help
: 1493136490:0;kubectl exec -ti prometheus-2406627989-pthfn --namespace prometheus /bin/sh 
: 1493136561:0;vi prom-deis-cluster-help-info
: 1493136638:0;prometheus -h > ~/Documents/kubernetes/prometheus-local-helpinfo
: 1493136657:0;diff -u ~/Documents/kubernetes/prometheus-local-helpinfo ~/Documents/kubernetesprom-deis-cluster-help-info
: 1493136663:0;diff -u ~/Documents/kubernetes/prometheus-local-helpinfo ~/Documents/kubernetes/prom-deis-cluster-help-info
: 1493139917:0;heml --version
: 1493139921:0;helm --version
: 1493139970:0;kubectl get deployment alertmanager  --namespace prometheus -o yaml
: 1493139985:0;kubectl get configmap alertmanager  --namespace prometheus -o yaml
: 1493140005:0;kubectl get configmaps --namespace prometheus
: 1493140017:0;kubectl get configmap prometheus-rules  --namespace prometheus 
: 1493140021:0;kubectl get configmap prometheus-rules  --namespace prometheus -o yaml
: 1493141885:0;cd helm
: 1493141887:0;ls -l 
: 1493141917:0;vi values.yml
: 1493141924:0;vi values.yaml
: 1493141948:0;vi Chart.yaml
: 1493142067:0;cd ../../
: 1493142074:0;mkdir myh-running
: 1493142077:0;cd myh-running
: 1493142089:0;helm inspect values deis/workflow | sed -n '1!p' > deis-workflow-current-values.yaml
: 1493142138:0;helm -h 
: 1493142151:0;helm history
: 1493142179:0;brew install helm v2.2.2
: 1493142197:0;brew search helm
: 1493142227:0;brew help
: 1493142262:0;brew uninstall kubernetes-helm
: 1493142293:0;brew install kubernetes-helm 2.2.2
: 1493142302:0;brew install kubernetes-helm v2.2.2
: 1493142318:0;brew install --help
: 1493142348:0;brew search kubernetes-helm
: 1493142463:0;brew versions kubernetes-helm
: 1493142468:0;brew version kubernetes-helm
: 1493142543:0;brew info kubernetes-helm
: 1493142569:0;brew switch kubernetes-helm 2.2.2
: 1493142758:0;more deis-workflow-current-values.yaml
: 1493142782:0;ls -ltrs
: 1493142784:0;cd templates
: 1493142796:0;grep -i storage *
: 1493142804:0;more server-pvc.yaml
: 1493144735:0;helm inspect values stable/prometheus
: 1493144823:0;cd ../../Downloads
: 1493144826:0;cd helm-charts
: 1493144836:0;ls -l prometheus
: 1493144866:0;helm inspect values stable/prometheus > helm-inspect-values-stable-prometheus-chart-all
: 1493144888:0;helm inspect values stable/prometheus | sed -n '1!p'
: 1493144917:0;helm inspect values stable/prometheus | sed -n '1!p' > helm-inspect-values-stable-prometheus-chart-sedded
: 1493144931:0;diff -u helm-inspect-values-stable-prometheus-chart-all helm-inspect-values-stable-prometheus-chart-sedded
: 1493144947:0;vi helm-inspect-values-stable-prometheus-chart-all
: 1493144964:0;vi helm-inspect-values-stable-prometheus-chart-sedded
: 1493146087:0;more helm-inspect-values-stable-prometheus-chart-sedded
: 1493218189:0;aws help 
: 1493218250:0;aws ec2 describe-volumes help
: 1493218291:0;aws ec2 describe-volumes --volumes-id vol-086d5572e6e29c56f
: 1493219442:0;history | grep kubectl | grep exec
: 1493222237:0;kubectl edit deployment prometheus --namespace prometheus 
: 1493228006:0;deis releases -a walkadoo-staging
: 1493228066:0;kubectl describe node ip-172-20-220-183.ec2.internal
: 1493229090:0;kubectl exec -ti prometheus-2406627989-jl3gm --namespace prometheus /bin/sh
: 1493230067:0;kubectl describe node ip-172-20-146-0.ec2.internal
: 1493230401:0;kubectl exec -ti prometheus-1233734967-sk7nt --namespace prometheus /bin/sh
: 1493230539:0;git log 
: 1493230544:0;git log  | head
: 1493230937:0;kubectl get deployment prometheus --namespace prometheus -o yaml 
: 1493230988:0;kubectl describe pod prometheus-1074187-jp8c7
: 1493231116:0;kubectl describe pod prometheus-1074187-jp8c7 --namespace prometheus
: 1493231174:0;kubectl exec -ti prometheus-1074187-jp8c7 --namespace prometheus /bin/sh
: 1493231337:0;kubectl delete pod prometheus-1074187-jp8c7 --namespace prometheus
: 1493231458:0;kubectl exec -ti prometheus-1074187-x2vv4 --namespace prometheus /bin/sh
: 1493231687:0;kubectl get events --timestamp
: 1493231696:0;history | grep timestamp
: 1493231716:0;kubectl get events --sort-by ".lastTimestamp"
: 1493231723:0;kubectl get events --sort-by ".lastTimestamp" --namespace prometheus
: 1493233355:0;kubectl describe pod prometheus-1074187-x2vv4 --namespace prometheus
: 1493233412:0;kubectl get deployment prometheus --namespace prometheus -o yaml > kubectl-get-deployment-prometheus-output-042617.yaml
: 1493233897:0;diff -u prometheus.yml kubectl-get-deployment-prometheus-output-042617.yaml
: 1493233906:0;cat kubectl-get-deployment-prometheus-output-042617.yaml
: 1493234280:0;aws ec2 describe-volumes --volume-id vol-086d5572e6e29c56f
: 1493234297:0;aws ec2 describe-volumes --volume-id vol-0cf1752303c245033
: 1493234447:0;aws ec2 create-snapshot help
: 1493303724:0;kubectl exec -ti prometheus-416900056-n29fc --namespace prometheus /bin/sh
: 1493303934:0;prometheus -h
: 1493304004:0;kubectl exec -ti prometheus-1074187-tpjch --namespace prometheus /bin/sh
: 1493311340:0;script look-at-disk-stats-stackoverflow.log
: 1493311346:0;NODE=$(kubectl get nodes -o=jsonpath="{.items[0].metadata.name}")
: 1493311349:0;echo $NODE
: 1493311388:0;curl -X "POST" -d '{"containerName":"/","subcontainers":true,"num_stats":1}' localhost:8001/api/v1/proxy/nodes/${NODE}:10255/stats/container
: 1493312974:0;cd ~/Downloads
: 1493312981:0;cd c5c478bae476adb32d48-43f5652fea2e6eda1ea07998c0f17f95ab5cdd7d
: 1493312986:0;more checkDockerDisks.sh
: 1493312992:0;vi checkDockerDisks.sh
: 1493313663:0;cp checkDockerDisks.sh ~/Documents/checkDockerDisks.sh
: 1493313690:0;vi ~/Documents/checkDockerDisks.sh
: 1493389731:0;brew search jq
: 1493389898:0;brew install jq
: 1493651066:0;more kubectl-get-deployment-prometheus-output-042617.yaml
: 1493651070:0;ls ../../
: 1493651087:0;mv kubectl-get-deployment-prometheus-output-042617.yaml kubectl-configmap-prometheus.042617.yaml
: 1493651094:0;mv kubectl-configmap-prometheus.042617.yaml ../../
: 1493651107:0;rm prometheus-2.0.4.tgz
: 1493651127:0;ls -l prometheus-rules
: 1493651138:0;cd prometheus-rules
: 1493651143:0;more sidekiq-metric-alert.rules
: 1493653225:0;more kubectl-get-secret-metrics-token
: 1493653242:0;more kubectl-edit-deployment-prometheus-all-yaml.yml
: 1493653281:0;history | grep prom
: 1493653321:0;kubectl get deployment prometheus --namespace prometheus
: 1493653351:0;kubectl get deployment prometheus --namespace prometheus -o yaml > kubectl-prom-deployment-staging.050117.yml
: 1493653411:0;kubectl get deployment prometheus --namespace prometheus -o yaml > kubectl-prom-deployment-production.050117.yml
: 1493653454:0;diff -u kubectl-prom-deployment-staging.050117.yml kubectl-prom-deployment-production.050117.yml
: 1493653478:0;vimdiff kubectl-prom-deployment-production.050117.yml kubectl-prom-deployment-staging.050117.yml
: 1493653712:0;deis apps help
: 1493653718:0;deis help
: 1493653740:0;deis config help
: 1493653757:0;deis config:list -a wbt-staging
: 1493653810:0;shipit env config_list wellbeingid-production | grep METRIC
: 1493653818:0;shipit env config_list wellbeingid-staging | grep METRIC
: 1493657832:0;vi metric-token-compare-wbid.txt
: 1493657978:0;kubectl get secret metrics-token  --namespace prometheus -o yaml 
: 1493658012:0;ls -l wbid*
: 1493658015:0;ls -l *wbid*
: 1493658021:0;more metric-token-compare-wbid.txt
: 1493658158:0;vi metric-token.txt
: 1493658233:0;kubectl get secrets 
: 1493658286:0;vi metric-token.yml
: 1493658456:0;;kgc
: 1493658499:0;kubectl create help
: 1493658504:0;kubectl create -h 
: 1493658511:0;kubectl create secret -h 
: 1493658539:0;kubectl create secret --help
: 1493658585:0;kubectl create -f ./metric-token.yml --namespace prometheus 
: 1493658610:0;kubectl create secret -f ./metric-token.yml --namespace prometheus 
: 1493658623:0;kubectl get secrets --namespace 
: 1493658788:0;cat metric-token.txt
: 1493658854:0;kubectl create secret generic metrics-token --from-file=./metric-token.txt --namespace prometheus 
: 1493658916:0;kubectl get secrets --namespace prometheus -o yaml
: 1493659482:0;diff kubectl-prom-deployment-staging.050117.yml test-prometheus-prod-chgs.yml
: 1493659663:0;git staged diff
: 1493659670:0;git staged --diff
: 1493659676:0;git diff staged
: 1493659681:0;history | grep staged
: 1493659764:0;cp prometheus.yml volumes.yml
: 1493659864:0;cp kubectl-prom-deployment-staging.050117.yml volumes.yml
: 1493659869:0;rm volumes.yml
: 1493659876:0;vi volumes.yml
: 1493660014:0;kubectl describe pod prometheus-2573548552-nc7hs --namespace prometheus
: 1493660181:0;echo "YWQ3NzhiZTNhYmRhYWJkMGI4NTZjOGI3NTYzYTA4Y2Q1MDIzYjQwMDMyMWFjYTZiMjEzZjZmY2FhNTA0MjNhMAo=" | base64 --decode
: 1493660291:0;grep -i fail kubectl-prom-deployment-staging.050117.yml
: 1493660527:0;vi kubectl-prom-deployment-staging.050117.yml
: 1493661021:0;history | grep aws | grep ebs
: 1493661027:0;history | grep aws | grep volume
: 1493661049:0;aws ec2 describe-volumes --volume-id vol-079edf3a5429b8769
: 1493661298:0;history | grep exec
: 1493661325:0;kubectl exec -ti prometheus-2592034474-4wzc8 --namespace prometheus /bin/sh
: 1493661662:0;kubectl get pods -o wide  --all-namespaces | grep prometh
: 1493661731:0;kubectl describe pod prometheus-1575380483-9v806 --namespace prometheus > kubectl-desc-prod-pod-prometheus-ebs-check-prometheus-1575380483-9v806.050117.txt
: 1493661787:0;kubectl get deployment prometheus --namespace prometheus -o yaml > kubectl-deployment-prod-prometheus.050117.yml
: 1493661871:0;grep failure prometheus.yml
: 1493661884:0;grep -i ebs prometheus.yml
: 1493661891:0;vi prometheus.yml
: 1493662007:0;git add prometheus.yml
: 1493662052:0;more volumes.yml
: 1493735500:0;kubectl edit configmap prometheus-rules  --namespace prometheus 
: 1493735605:0;kubectl get configmap prometheus-config --namespace prometheus -o yaml > prometheus-configmap-staging.050217.yml
: 1493735651:0;more prometheus-configmap-staging.050217.yml
: 1493735726:0;rm prometheus-configmap-staging.050217.yml
: 1493735754:0;more prom.yaml
: 1493735780:0;more test-prometheus-prod-chgs.yml
: 1493735803:0;mv test-prometheus-prod-chgs.yml test-prometheus-prod-chgs.050117.yml
: 1493735831:0;more kubectl-namespace-prom-get-configmap.040417.yaml
: 1493735849:0;grep metric *prom*
: 1493735904:0;more kubectl-edit-deployment-prometheus-after-replace-config-map-kubectl-v1.5.2.yml
: 1493735915:0;ls -l *prom*
: 1493736062:0;prometheus-config-map-050217.before.changes.yml
: 1493736302:0;vi prometheus-config-map-050217.before.changes.yml
: 1493736353:0;cp prometheus.yml prometheus-staging.yml
: 1493736422:0;vimdiff ~/Documents/kubernetes/prometheus-config-map-050217.before.changes.yml prometheus-staging.yml
: 1493736457:0;ls -l prometheus-config
: 1493736508:0;head kubectl-configmap-prometheus.041217.yaml
: 1493736531:0;vi ~/Documents/kubernetes/prometheus-config-map-050217.before.changes.yml
: 1493736542:0;vimdiff kubectl-configmap-prometheus.041217.yaml ~/Documents/kubernetes/prometheus-config-map-050217.before.changes.yml
: 1493736620:0;mv ~/Documents/kubernetes/prometheus-config-map-050217.before.changes.yml ~/Documents/kubernetes/prometheus-config-map-screenscrap-050217.before.changes.yml
: 1493736689:0;vi jobname-wbid-metrics
: 1493737521:0;history | grep DEIS_PROFILE
: 1493737568:0;DEIS_PROFILE=production deis auth:login https://deis.meyouhealth.com --ssl-verify=true --username=heidi.schmidt --password=JxgcoU6HrRzuxtpymMrVshya
: 1493737606:0;deis auth:login https://deis.meyouhealth.com --ssl-verify=true --username=heidi.schmidt --password=JxgcoU6HrRzuxtpymMrVshya
: 1493737694:0;deis auth:login https://deis.meyouhealth.com --ssl-verify=true --username=heidi.schmidt --password=iuUepxW8rnaa
: 1493737772:0;history | grep PROFILE
: 1493737795:0;DEIS_PROFILE=production deis auth:login https://deis.meyouhealth.com --ssl-verify=true --username=heidi.schmidt --password=iuUepxW8rnaa
: 1493737838:0;DEIS_PROFILE=production deis releases -a wbt-production
: 1493738579:0;kubectl config delete-context k8s.meyouhealth.com
: 1493738583:0;kubectl config delete-cluster k8s.meyouhealth.com
: 1493738599:0;kubectl config get-context
: 1493738755:0;export DEIS_PROFILE=production
: 1493738761:0;deis whom
: 1493739567:0;kubectl describe pod prometheus-1575380483-9v806
: 1493739574:0;kubectl describe pod prometheus-1575380483-9v806 --namespace prometheus
: 1493739789:0;cd k8s-ops/add-ons/prometheus/prometheus-config
: 1493739871:0;kubectl get configmap prometheus-config --namespace prometheus -o yaml > prometheus-config-prod-working-050217.yml
: 1493739882:0;more kubectl-configmap-prometheus.041217.yaml
: 1493739905:0;mv kubectl-configmap-prometheus.041217.yaml kubectl-configmap-original-prometheus.041217.yaml
: 1493739915:0;more kubectl-configmap-prometheus.wbid-prd-stg.041317.yml
: 1493739944:0;mv kubectl-configmap-prometheus.wbid-prd-stg.041317.yml prometheus-config-staging-working-041317.yml
: 1493739954:0;more wbid-metrics.yml
: 1493744295:0;kubectl delete pod prometheus-1575380483-9v806 --namespace prometheus
: 1493744340:0;kubectl describe pod prometheus-1575380483-tds0q --namespace prometheus
: 1493744392:0;kubectl --namespace prometheus logs prometheus-1575380483-tds0q
: 1493744822:0;kubectl exec -ti prometheus-1575380483-tds0q --namespace prometheus /bin/sh
: 1493744993:0;kubectl delete secret metrics-token --namespace prometheus
: 1493745060:0;more metric-token.txt
: 1493745068:0;more metric-token
: 1493745085:0;mv metric-token.txt metrics-token
: 1493745119:0;kubectl create secret generic metrics-token --from-file=./metrics-token --namespace prometheus
: 1493745127:0;kubectl get secrets --namespace prometheus
: 1493745141:0;kubectl get secret metrics-token --namespace prometheus -o yaml
: 1493745164:0;kubectl delete pod prometheus-1575380483-tds0q --namespace prometheus
: 1493747001:0;kubectl describe pod prometheus-1575380483-kwp2p
: 1493747008:0;kubectl describe pod prometheus-1575380483-kwp2p --namespace prometheus
: 1493747025:0;kubectl exec -ti prometheus-1575380483-kwp2p --namespace prometheus /bin/sh
: 1493747245:0;kubectl --namespace prometheus logs prometheus-1575380483-kwp2p
: 1493747692:0;git add jobname-wbid-metrics
: 1493747780:0;more prometheus-staging.yml
: 1493747805:0;cp prometheus-staging.yml prometheus-production.yml
: 1493747860:0;diff prometheus.yml prometheus-staging.yml
: 1493747865:0;diff prometheus.yml prometheus-production.yml
: 1493747873:0;git add prometheus-staging.yml
: 1493747878:0;git add prometheus-production.yml
: 1493826498:0;kubectl delete pod prometheus-1575380483-kwp2p --namespace prometheus
: 1493832993:0;more prometheus-config
: 1493833005:0;more jobname-wbid-metrics
: 1493833448:0;history | grep curl
: 1493833458:0;curl https://account.myhstg.com/metrics -H "Authorization: Bearer ad778be3abdaabd0b856c8b7563a08cd5023b400321aca6b213f6fcaa50423a0"
: 1493833468:0;curl https://account.meyouhealth.com/metrics -H "Authorization: Bearer ad778be3abdaabd0b856c8b7563a08cd5023b400321aca6b213f6fcaa50423a0"
: 1493833645:0;kubectl get pods -o wide  --all-namespaces |
: 1493833669:0;kubectl exec -ti prometheus-1575380483-ql6jg --namespace prometheus /bin/sh
: 1493907818:0;history walkadoo-production
: 1493907866:0;mysql -u meyouhealth -p -P 3306 -h walkadoo-production2.cduvhvvpw6oo.us-east-1.rds.amazonaws.com -vvv -D walkadoo_production -A 
: 1493907916:0;mysql -u walkadoo-prd-app -p -P 3306 -h walkadoo-production2.cduvhvvpw6oo.us-east-1.rds.amazonaws.com -vvv -D walkadoo_production -A 
: 1493908231:0;ssh admin@ip-172-24-143-203.ec2.internal
: 1493908559:0;history | grep walkadoo-production
: 1493920776:0;brew search yamllint
: 1493920802:0;sudo pip install yamllint
: 1493920832:0;pip install yamllint
: 1493920842:0;which yamllint
: 1493920860:0;yamllint prometheus.yml
: 1493920874:0;yamllint prometheus-production.yml
: 1493921049:0;cp yaml.vim ../.vim/
: 1493921062:0;vi .vimrc
: 1493921176:0;more ~/.vim/yaml.vim
: 1493921369:0;vi prometheus-production.yml
: 1493921632:0;vim --version
: 1493923500:0;yamllint -f parsible prometheus.yml
: 1493923561:0;cd Documents/kubernetes
: 1493923569:0;yamllint -d relaxed prometheus-config-map-screenscrap-050217.before.changes.yml
: 1493923583:0;yamllint --help
: 1493923642:0;yamllint -d relaxed yamllint-test-prom-configmap-get.yml
: 1493923684:0;yamllint -d test.yml
: 1493923737:0;more kubectl-deployment-prod-prometheus.050117.yml
: 1493923752:0;yamllint -d relaxed kubectl-deployment-prod-prometheus.050117.yml
: 1493923779:0;more kubectl-prom-deployment-staging.050117.yml
: 1493924009:0;shipit ssh quitnet-staging
: 1493924095:0;DEIS_PROFILE=staging deis releases -a quitnet-staging
: 1493924211:0;kubectl --namespace quitnet logs quitnet-staging-web-1717690452-2jj1w
: 1493924222:0;kubectl --namespace quitnet-staging logs quitnet-staging-web-1717690452-2jj1w
: 1493926112:0;kubectl get configmap prometheus-config --namespace prometheus -o yaml > yamllint-test-prom-configmap-get.yml
: 1493926120:0;vi yamllint-test-prom-configmap-get.yml
: 1493926139:0;more prometheus-config-map-screenscrap-050217.before.changes.yml
: 1493926151:0;cp prometheus-config-map-screenscrap-050217.before.changes.yml test.yml
: 1493926166:0;cp test.yml test-screenscrape-original.yml
: 1493926537:0;rm test.yml 
: 1493926543:0;cp test1.yml test.yml
: 1493926605:0;cp test.yml test1.yml
: 1493926706:0;more test.yml
: 1493926722:0;vi test.yml
: 1493926735:0;yamllint -d relaxed test.yml
: 1493927059:0;kubectl get configmap prometheus-config --namespace prometheus -o yaml 
: 1493927078:0;kubectl edit configmap prometheus-config --namespace prometheus -o yaml 
: 1493927468:0;cat test.yml
: 1493927593:0;kubectl get configmap prometheus-config --namespace prometheus -o yaml > prom-cfg-map-yaml-staging.yml 
: 1493927617:0;kubectl get configmap prometheus-config --namespace prometheus -o json
: 1493927634:0;kubectl create --help
: 1493927644:0;kubectl create configmap --help
: 1493927848:0;kubectl delete configmap prometheus-config --namespace prometheus 
: 1493927857:0;more prom-cfg-map-yaml-staging.yml
: 1493927920:0;ls -ltra prometheus.yml
: 1493927926:0;kubectl create configmap prometheus-config --namespace prometheus --from-file=/Users/heidischmidt/Documents/git/k8s-ops/add-ons/prometheus/prometheus-config/prometheus.yml
: 1493927944:0;kubectl get configmap prometheus-config --namespace prometheus
: 1493927970:0;more test-combined.yml
: 1493927985:0;cp test-combined.yml test2.yml
: 1493927989:0;vi test2.yml
: 1493928028:0;grep -i kind prometheus.yml
: 1493928067:0;kubectl create configmap prometheus-config --namespace prometheus --from-file=test2.yml
: 1493928126:0;kubectl delete configmap prometheus-config --namespace prometheus
: 1493928140:0;mv test2.yml prometheus.yml
: 1493928150:0;kubectl create configmap prometheus-config --namespace prometheus --from-file=prometheus.yml
: 1493930024:0;yamllint -d relaxed prometheus.yml
: 1493930208:0;cat prometheus.yml
: 1494000767:0;kubectl --version
: 1494003886:0;git info
: 1494003892:0;git --help
: 1494003914:0;git log
: 1494003936:0;ls -l .git
: 1494003957:0;more .git/description
: 1494003987:0;more .promu.yml
: 1494003993:0;more .git
: 1494008658:0;git clone https://github.com/meyouhealth/k8s-ops.wiki.git
: 1494009827:0;cd aws
: 1494009831:0;ls -lr
: 1494009855:0;more config
: 1494010445:0;ls -l .??*
: 1494010573:0;.deis:
: 1494010573:0;total 24
: 1494010573:0;-rw-------  1 heidischmidt  staff  160 May  2 11:08 client.json
: 1494010573:0;-rw-------  1 heidischmidt  staff  160 May  2 11:09 production.json
: 1494010573:0;-rw-------  1 heidischmidt  staff  155 Apr  5 11:02 staging.json
: 1494010590:0;motd
: 1494207992:0;kubectl config view
: 1494252411:0;cd kube-state-metrics
: 1494252415:0;more service.yml
: 1494252673:0;cp ../k8s-ops/add-ons/prometheus/prometheus.yml prometheus-pushgateway.yml
: 1494252837:0;ls -l ~/Downloads/*dar*
: 1494252883:0;gunzip pushgateway-0.3.1.darwin-amd64.tar.gz
: 1494252888:0;tar -xvf pushgateway-0.3.1.darwin-amd64.tar
: 1494252898:0;file pushgateway
: 1494256363:0;ls -l *
: 1494257332:0;kubectl describe pod prometheus-pushgateway-2406918764-vqnlp --namespace prometheus
: 1494257390:0;more prometheus-pushgateway.yml
: 1494257437:0;kubectl get pods --namespace prometheus 
: 1494257511:0;kubectl --namespace prometheus get deployments
: 1494257582:0;docker pull quay.io/prometheus/pushgateway:latest
: 1494257611:0;ls -l ~/Downloads
: 1494257648:0;sudo docker inspect quay.io/prometheus/pushgateway
: 1494257801:0;sudo docker start 434efa6ed9db
: 1494257884:0;docker run -i -t 434efa6ed9db /bin/bash
: 1494257935:0;history | grep docker | grep attach
: 1494257938:0;history | grep docker | grep exec
: 1494257980:0;sudo docker exec -it 08b2d36bbd7b bash
: 1494257989:0;sudo docker exec -it 08b2d36bbd7b /bin/sh
: 1494258236:0;cp prometheus-pushgateway.yml prometheus-pushgateway-v2.yml
: 1494258315:0;kubectl describe pod prometheus-pushgateway-1288815453-304t9 --namespace prometheus
: 1494258723:0;kubectl --namespace prometheus logs prometheus-pushgateway-1288815453-304t9
: 1494258799:0;kubectl delete deployment prometheus-pushgateway --namespace prometheus
: 1494258821:0;vi prometheus-pushgateway.yml
: 1494258841:0;cp prometheus-pushgateway.yml prometheus-pushgateway-v3.yml
: 1494258875:0;kubectl get pods --namespace prometheus
: 1494258895:0;ubectl describe pod prometheus-pushgateway-2543103020-h4r9b --namespace prometheus 
: 1494266346:0;kubectl describe pod prometheus-pushgateway-2543103020-h4r9b --namespace prometheus 
: 1494268692:0;kubectl get replicationsets --namespace prometheus
: 1494268704:0;kubectl get replicationset --namespace prometheus
: 1494268715:0;kubectl get replicaset --namespace prometheus
: 1494268728:0;kubectl get replicaset prometheus-pushgateway-2543103020 --namespace prometheus  -o yaml
: 1494268945:0;grep prometheus-pushgateway *.yaml
: 1494268948:0;grep prometheus-pushgateway *.yml
: 1494269080:0;kubectl get deployment prometheus-pushgateway --namespace prometheus
: 1494269102:0;kubectl get deployment prometheus-pushgateway --namespace prometheus -o yaml > prometheus-pushgateway-kubectl-deployment.yml
: 1494269112:0;diff -u prometheus-pushgateway-v3.yml prometheus-pushgateway-kubectl-deployment.yml
: 1494269125:0;vimdiff prometheus-pushgateway-v3.yml prometheus-pushgateway-kubectl-deployment.yml
: 1494269288:0;vi pushgateway-service.yml
: 1494269295:0;yamllint pushgateway-service.yml
: 1494269370:0;grep quay prometheus-pushgateway.yml
: 1494269482:0;mv pushgateway-service.yml prometheus-pushgateway-service.yml
: 1494269514:0;kubectl get services --namespace prometheus - o wide
: 1494269519:0;kubectl get services --namespace prometheus -o wide
: 1494269699:0;nslookup a5ae40763341f11e7b9e40a575b27341-1732845930.us-east-1.elb.amazonaws.com
: 1494270988:0;kubectl delete service prometheus-pushgateway --namespace prometheus
: 1494271365:0;ls *service*
: 1494271377:0;cp prometheus-pushgateway-service.yml prometheus-pushgateway-service-v1-elb.yml
: 1494271390:0;kubectl --namespace iris-staging get pods
: 1494271420:0;kubectl --namespace iris-staging exec -ti iris-staging-web-2183840902-dc5hr /bin/sh 
: 1494271748:0;kubectl --namespace prometheus exec -ti prometheus-3274335614-8w0ws /bin/sh 
: 1494272723:0;history | grep service
: 1494338269:0;cat prometheus-pushgateway.yml
: 1494338301:0;cat prometheus-pushgateway-service.yml
: 1494339224:0;sudo docker ps
: 1494339241:0;cd ../../kubernetes
: 1494339264:0;kubectl get pods -o wide  --namespace prometheus
: 1494339730:0;kubectl edit configmap prometheus --namespace prometheus
: 1494339738:0;kubectl edit config-map prometheus --namespace prometheus
: 1494339921:0;kubectl get configmap prometheus-config --namespace prometheus -o yaml > prometheus-config-map-050917.yml
: 1494339953:0;kubectl delete pod prometheus-3274335614-8w0ws --namespace prometheus
: 1494339977:0;kubectl exec -ti prometheus-pushgateway-1288815453-304t9 --namespace prometheus /bin/sh
: 1494339987:0;kubectl exec -ti prometheus-pushgateway-1288815453-304t9 --namespace prometheus /bin/sh -previous 
: 1494339991:0;kubectl exec -ti prometheus-pushgateway-1288815453-304t9 --namespace prometheus /bin/sh --previous 
: 1494342990:0;kubectl --namespace prometheus logs prometheus-pushgateway-2543103020-h4r9b 
: 1494343262:0;curl http://prometheus-pushgateway.prometheus.svc.cluster.local:9091/metrics
: 1494343282:0;curl http://100.107.126.43:9091/metrics
: 1494343515:0;curl http://prometheus-pushgateway.prometheus.svc.cluster.local:80/metrics
: 1494343537:0;curl http://100.107.126.43:80/metrics
: 1494343620:0;curl -L http://100.107.126.43:80/metrics
: 1494343669:0;curl https://100.107.126.43:80/metrics
: 1494343919:0;kubectl delete pod prometheus-3274335614-x3kd4 --namespace prometheus
: 1494343961:0;kubectl --namespace prometheus logs prometheus-3274335614-sljz2
: 1494343980:0;kubectl --namespace prometheus get pods -l app=prometheus -o name | sed 's/^.*\///' | xargs -I{} kubectl --namespace prometheus port-forward {} 9090:9090
: 1494344425:0;kubectl exec -ti prometheus-1575380483-ql6jg --namespace prometheus /bin/bash
: 1494344458:0;kubectl exec -ti iris-staging-clockwork-383607268-fh5fq --namespace iris-staging
: 1494344510:0;ls -l *push*
: 1494344517:0;pushgateway --help
: 1494344522:0;./pushgateway --help
: 1494344569:0;kubectl describe pod prometheus-pushgateway-2543103020-h4r9b --namespace prometheus
: 1494344589:0;kubectl get logs pod prometheus-pushgateway-2543103020-h4r9b --namespace prometheus
: 1494344600:0;kubectl get logs prometheus-pushgateway-2543103020-h4r9b --namespace prometheus
: 1494344625:0;kubectl --namespace prometheus logs prometheus-pushgateway-2543103020-h4r9b
: 1494344653:0;kubectl exec -ti prometheus-pushgateway-2543103020-h4r9b --namespace prometheus /bin/bash
: 1494344659:0;kubectl exec -ti prometheus-pushgateway-2543103020-h4r9b --namespace prometheus /bin/sh
: 1494344971:0;history | grep pushgateway
: 1494345016:0;cd cd pushgateway-0.3.1.darwin-amd64
: 1494345018:0;cd pushgateway-0.3.1.darwin-amd64
: 1494345156:0;kubectl get deployment prometheus-pushgateway --namespace prometheus -o yaml 
: 1494345198:0;kubectl delete pod prometheus-pushgateway-1271508221-vx4m7 --namespace prometheus
: 1494345231:0;kubectl describe prometheus-pushgateway-1271508221-vxw4l --namespace prometheus
: 1494345264:0;kubectl describe pod prometheus-pushgateway-1271508221-vxw4l --namespace prometheus
: 1494345284:0;kubectl --namespace prometheus logs prometheus-pushgateway-1271508221-vxw4l
: 1494345337:0;kubectl edit deployment prometheus-pushgateway --namespace prometheus
: 1494345390:0;kubectl delete pod prometheus-pushgateway-1271508221-vxw4l --namespace prometheus
: 1494345412:0;kubectl describe pod prometheus-pushgateway-4073959041-z0q7v --namespace prometheus
: 1494345469:0;kubectl exec -ti prometheus-pushgateway-4073959041-z0q7v --namespace prometheus /bin/sh
: 1494345593:0;kubectl get logs prometheus-pushgateway-4073959041-z0q7v --namespace prometheus
: 1494345600:0;kubectl get log prometheus-pushgateway-4073959041-z0q7v --namespace prometheus
: 1494345617:0;kubectl --namespace prometheus logs prometheus-pushgateway-4073959041-z0q7v
: 1494348415:0;kubectl get deployment prometheus --namespace prometheus 
: 1494348421:0;kubectl get deployment prometheus --namespace prometheus -o yaml
: 1494348606:0;kubectl --namespace prometheus logs prometheus-pushgateway-1453501962-0wlwm
: 1494348625:0;kubectl exec -ti prometheus-pushgateway-1453501962-0wlwm --namespace prometheus /bin/sh
: 1494348831:0;kubectl --namespace prometheus logs prometheus-pushgateway-956601339-p6l74
: 1494348942:0;kubectl exec -ti iris-staging-clockwork-383607268-fh5fq --namespace iris-staging /bin/bash
: 1494350990:0;kubectl get service --namespace prometheus
: 1494351000:0;kubectl get service --namespace prometheus prometheus-pushgateway -o yaml
: 1494351017:0;kubectl get service --namespace prometheus prometheus-pushgateway -o yaml > fixed-prom-pushgateway-service.yml
: 1494351837:0;kubectl exec -ti prometheus-pushgateway-956601339-p6l74 --namespace prometheus /bin/sh
: 1494351996:0;kubectl --namespace prometheus logs prometheus-pushgateway-2976552050-nxrkb
: 1494352011:0;kubectl describe pod prometheus-pushgateway-2976552050-nxrkb --namespace prometheus
: 1494352236:0;kubectl --namespace prometheus get pods -l app=prometheus -o name
: 1494352249:0;kubectl --namespace prometheus get pods -l app=prometheus-pushgateway -o name
: 1494352669:0;kubectl exec -ti iris-staging-clockwork-383607268-fh5fq --namespace iris-staging /bin/sh
: 1494353490:0;kubectl get deployment prometheus-pushgateway --namespace prometheus -o yaml
: 1494354524:0;kubectl get configmap prometheus --namespace prometheus -o yaml
: 1494424523:0;which git
: 1494424526:0;git --version
: 1494425689:0;kubectl explain svc
: 1494427125:0;kubectl get pods -o wide  --all-namespaces | grep iris-staging
: 1494427158:0;kubectl exec -ti iris-staging-clockwork-1307533827-13j1n --namespace iris-staging nslookup kubernetes.default
: 1494427175:0;kubectl exec -ti iris-staging-clockwork-1307533827-13j1n --namespace iris-staging nslookup prometheus-pushgateway.prometheus
: 1494427248:0;kubectl get pods --namespace=kube-system -l k8s-app=kube-dns
: 1494427361:0;kubectl get pods -l app=iris-staging
: 1494427379:0;kubectl get pods -l  --namespace iris-staging
: 1494427391:0;kubectl get pods -l app=iris-staging --namespace iris-staging
: 1494427432:0;kubectl get svc
: 1494427438:0;kubectl get svc --namespace prometheus 
: 1494427484:0;kubectl get service prometheus-pushgateway --namespace prometheus  -o json
: 1494430021:0;pushgateway -h
: 1494431816:0;kubectl get pods -o wide  --all-namespaces | grep new
: 1494860798:0;;kprpg
: 1494860988:0;more prometheus.yml
: 1494861214:0;kubectl delete prometheus-3274335614-sljz2 --namespace prometheus 
: 1494861221:0;kubectl delete pod prometheus-3274335614-sljz2 --namespace prometheus 
: 1494868854:0;mkdir pushgateway
: 1494868932:0;cp fixed-prom-pushgateway-service.yml ~/Documents/git/k8s-ops/add-ons/prometheus/pushgateway/prometheus-pushgateway-service.yml
: 1494868945:0;more prometheus-pushgateway-kubectl-deployment.yml
: 1494868959:0;more prometheus-pushgateway-v3.yml
: 1494868984:0;cp prometheus-pushgateway-v3.yml prometheus-pushgateway-debugging.yml
: 1494868993:0;cp prometheus-pushgateway-v3.yml prometheus-pushgateway-deployment.yml
: 1494869047:0;kubectl edit configmap prometheus-pushgateway --namespace prometheus 
: 1494869090:0;vi prometheus-pushgateway-deployment.yml
: 1494869300:0;cp prometheus-pushgateway-deployment.yml ~/Documents/git/k8s-ops/add-ons/prometheus/pushgateway/prometheus-pushgateway-deployment.yml
: 1494869364:0;git add prometheus-pushgateway-service.yml 
: 1494869370:0;git add prometheus-pushgateway-deployment.yml
: 1494869605:0;history | grep request
: 1494869666:0;script process
: 1494869900:0;git checkout -b pushgateway
: 1494869981:0;git add pushgateway
: 1494870018:0;git reset 
: 1494870048:0;mv process ../
: 1494870059:0;script ../process2 
: 1494870087:0;git add pushgateway/
: 1494870219:0;git reset --help
: 1494870232:0;git reset prometheus-pushgateway-service.yml
: 1494870279:0;more fixed-prom-pushgateway-service.yml
: 1494870300:0;cp prometheus-pushgateway-service.yml /Users/heidischmidt/Documents/git/k8s-ops/add-ons/prometheus/pushgateway
: 1494870319:0;more prometheus-pushgateway-service.yml
: 1494870330:0;git diff --stages
: 1494870706:0;kubectl edit deployment prometheus-pushgateway --namespace prometheus 
: 1494870734:0;kubectl edit config-map prometheus --namespace prometheus 
: 1494870741:0;kubectl edit configmap prometheus --namespace prometheus 
: 1494870780:0;vi pushgateway-metric.yml
: 1494870802:0;cat pushgateway-metric.yml
: 1494871087:0;more ../process
: 1494871091:0;cat ../process
: 1494871182:0;cat ../process2
: 1494871498:0;cd prometheus
: 1494871517:0;more pushgateway-metric.yml
: 1494871583:0;mv pushgateway-metric.yml /Users/heidischmidt/Documents/git/k8s-ops/add-ons/prometheus/prometheus-config/jobname-pushgateway-metrics
: 1494871617:0;git reset jobname-pushgateway-metrics
: 1494871621:0;vi jobname-pushgateway-metrics
: 1494871641:0;git add jobname-pushgateway-metrics
: 1494871654:0;git commit 
: 1494871718:0;git push origin pushgateway 
: 1494871928:0;kubectl delete pod prometheus-3274335614-38z3m --namespace prometheus 
: 1494872102:0;history | grep kubectl | grep create
: 1494872197:0;cp prometheus-pushgateway-deployment.yml prometheus-pushgateway.yml
: 1494872210:0;kubectl --namespace prometheus create -f prometheus-pushgateway.yml
: 1494872248:0;kubectl --namespace prometheus logs prometheus-pushgateway-2921373215-48vwn
: 1494872267:0;kubectl create service --namespace prometheus -f prometheus-pushgateway-service.yml
: 1494872291:0;kubectl --namespace prometheus create -f prometheus-pushgateway-service.yml
: 1494872310:0;kubectl get services --namespace prometheus
: 1494872355:0;kubectl describe pod prometheus-pushgateway-2921373215-48vwn --namespace prometheus
: 1494872373:0;cd prometheus-config
: 1494872380:0;cat jobname-pushgateway-metrics
: 1494872695:0;kubectl delete pod prometheus-1575380483-ql6jg --namespace prometheus 
: 1494872779:0;kubectl describe pod prometheus-1575380483-8m1lh --namespace prometheus
: 1494872862:0;cd pushgateway
: 1494872933:0;kubectl get services prometheus-pushgateway --namespace prometheus -o yaml 
: 1494872966:0;vi prometheus-pushgateway-service.yml
: 1494872995:0;git add prometheus-pushgateway-service.yml
: 1494873047:0;git push origin pushgateway
: 1494873108:0;kubectl edit services prometheus-pushgateway --namespace prometheus 
: 1494873137:0;kubectl get services prometheus-pushgateway --namespace prometheus 
: 1494877239:0;kubectl get services prometheus-pushgateway --namespace prometheus -o yaml
: 1494949402:0;history | grep deis 
: 1494949486:0;DEIS_PROFILE=staging deis releases -a dailychallenge-staging --help
: 1494951001:0;ssh -t heidi@54.146.72.167
: 1494957555:0;aws rds describe-pending-maintenance-actions
: 1494957600:0;aws rds describe-db-instances --db-instance-identifier wilder-upgrade-5619a-test
: 1494957613:0;date > wbid-5619a-test.txt
: 1494957711:0;tail wbid-5619a-test.txt
: 1494958119:0;cat wbid-5619a-test.txt
: 1494958127:0;aws rds describe-db-instances --db-instance-identifier wilder-upgrade-5619a-test 
: 1494958642:0;date >> wbid-5619a-test.txt
: 1494958644:0;aws rds describe-db-instances --db-instance-identifier wilder-upgrade-5619a-test >> wbid-5619a-test.txt
: 1494958653:0;tail -20 wbid-5619a-test.txt
: 1494958864:0;history | grep walkadoo
: 1494963710:0;kubectl get deployment wilder-production-jobs --namespace wilder-production
: 1494963781:0;DEIS_PROFILE=production deis releases -a wilder-production 
: 1494986851:0;cd ./Downloads
: 1494986861:0;cd ../Downloads
: 1494986875:0;vimdiff prometheus-kubernetes-ex.yml prometheus-coreos-tweaked.yml
: 1494987696:0;deis config:list -a wilder-production
: 1494987704:0;deis config:list -a wilder-production | grep -i PORT
: 1494987807:0;kubectl exec -ti wilder-production-web-1813695379-t0kbt --namespace wilder-production nslookup wilder-production-web
: 1494987816:0;kubectl exec -ti wilder-production-web-1813695379-t0kbt --namespace wilder-production nslookup wilder-production
: 1494987839:0;kubectl exec -ti wilder-production-web-1813695379-t0kbt --namespace wilder-production netstat -l -p
: 1494987848:0;kubectl exec -ti wilder-production-web-1813695379-t0kbt --namespace wilder-production netstat 
: 1494988509:0;tail -20 wbid-5619a-read-upgrade-test.txt
: 1494988528:0;moer wbid-5619a-read-upgrade-test.txt
: 1494988531:0;more wbid-5619a-read-upgrade-test.txt
: 1494988588:0;date >> wbid-5619a-read-upgrade-test.txt
: 1494988592:0;aws rds describe-db-instances --db-instance-identifier wilder-upgrade-5619a-test-read1 >> wbid-5619a-read-upgrade-test.txt
: 1494988599:0;grep DBInstanceStatus wbid-5619a-read-upgrade-test.txt
: 1494989929:0;date >> wbid-5619a-read-upgrade-promoted.txt
: 1494989942:0;aws rds describe-db-instances --db-instance-identifier wilder-upgrade-5619a-test-read1 >> wbid-5619a-read-upgrade-promoted.txt
: 1494989983:0;while true ; date >> wbid-5619a-read-upgrade-promoted.txt; aws rds describe-db-instances --db-instance-identifier wilder-upgrade-5619a-test-read1 >> wbid-5619a-read-upgrade-promoted.txt ; sleep 30;  done
: 1494989991:0;while true ; date >> wbid-5619a-read-upgrade-promoted.txt; aws rds describe-db-instances --db-instance-identifier wilder-upgrade-5619a-test-read1 >> wbid-5619a-read-upgrade-promoted.txt ; sleep 30;done
: 1494990027:0;while true ; do echo "$(date)" ; date >> wbid-5619a-read-upgrade-promoted.txt; aws rds describe-db-instances --db-instance-identifier wilder-upgrade-5619a-test-read1 >> wbid-5619a-read-upgrade-promoted.txt ; sleep 30;  done
: 1494990374:0;more wbid-5619a-read-upgrade-promoted.txt
: 1494990394:0;grep -i read wbid-5619a-read-upgrade-promoted.txt
: 1494990440:0;while true ; do echo "$(date)" ; date >> wbid-5619a-read-upgrade-promoted.txt; aws rds describe-db-instances --db-instance-identifier wilder-upgrade-5619a-test >> wbid-5619a-read-upgrade-promoted.txt ; sleep 30;  done
: 1494990451:0;tail -200 wbid-5619a-read-upgrade-promoted.txt
: 1495028122:0;aws rds describe-db-instances --db-instance-identifier wilder-production2-read1
: 1495028140:0;cd ~/Documents
: 1495028148:0;date >> wilder-production2-read1.create.log
: 1495028157:0;aws rds describe-db-instances --db-instance-identifier wilder-production2-read1 >> wilder-production2-read1.create.log
: 1495028205:0;while true ; do echo "$(date)" ; date >> wilder-production2-read1.create.log; aws rds describe-db-instances --db-instance-identifier wilder-production2-read1 >> wilder-production2-read1.create.log ; sleep 30;  done
: 1495028694:0;tail -100 wilder-production2-read1.create.log
: 1495029178:0;grep DBInstanceStatus wilder-production2-read1.create.log
: 1495029854:0;kubectl edit deployment iris-production-web --namespace iris-production
: 1495029930:0;kubectl edit configmap iris-production-web --namespace iris-production
: 1495032563:0;more .my.cnf
: 1495032847:0;while true ; do echo "$(date)" ; date >> wilder-production2-read1.upgrade.log; aws rds describe-db-instances --db-instance-identifier wilder-production2-read1 >> wilder-production2-read1.upgrade.log ; sleep 30;  done
: 1495033052:0;more wilder-production2-read1.upgrade.log
: 1495033416:0;tail -200 wilder-production2-read1.upgrade.log
: 1495034042:0;grep DBInstanceStatus wilder-production2-read1.upgrade.log
: 1495034054:0;grep DBInstanceStatus wilder-production2-read1.upgrade.log > 1
: 1495034098:0;vi 2
: 1495034111:0;wc -l 1
: 1495034113:0;wc -l 2
: 1495034117:0;paste 1 2
: 1495034200:0;paste 1 2 3
: 1495034230:0;vi 3
: 1495034241:0;head 3
: 1495042216:0;ssh ubuntu@54.235.27.46
: 1495042239:0;history | grep "ec2-54-235-27-46"
: 1495042261:0;ssh heidischmidt@ec2-54-235-27-46.compute-1.amazonaws.com
: 1495044812:0;kubectl get pods -o wide  --all-namespaces | grep kibana
: 1495045023:0;aws --help
: 1495045044:0;aws help | grep search
: 1495045306:0;curl -XGET https://search-myh-k8s-test-gbf5axa5rro7ajiq6tuvqancde.us-east-1.es.amazonaws.com:9200/_cluster/health\?pretty\=true
: 1495045328:0;curl -XGET http://search-myh-k8s-test-gbf5axa5rro7ajiq6tuvqancde.us-east-1.es.amazonaws.com:9200/_cluster/health\?pretty\=true
: 1495045347:0;nslookup search-myh-k8s-test-gbf5axa5rro7ajiq6tuvqancde.us-east-1.es.amazonaws.com
: 1495045357:0;curl http://search-myh-k8s-test-gbf5axa5rro7ajiq6tuvqancde.us-east-1.es.amazonaws.com
: 1495045373:0;curl -XGET http://search-myh-k8s-test-gbf5axa5rro7ajiq6tuvqancde.us-east-1.es.amazonaws.com
: 1495045389:0;curl -XGET http://search-myh-k8s-test-gbf5axa5rro7ajiq6tuvqancde.us-east-1.es.amazonaws.com/_cluster/health\?
: 1495045408:0;curl -XGET http://search-myh-k8s-test-gbf5axa5rro7ajiq6tuvqancde.us-east-1.es.amazonaws.com/_cluster/health\?pretty\=true
: 1495045858:0;curl -XGET https://search-myh-k8s-test-gbf5axa5rro7ajiq6tuvqancde.us-east-1.es.amazonaws.com/_cluster/health\?pretty\=true
: 1495046513:0;kubectl get services --namespace iris-production
: 1495046526:0;kubectl get services --namespace iris-production -o yaml
: 1495115913:0;ls -lta
: 1495115924:0;mv wilder-production2-read1.upgrade.log Documents/wilder/
: 1495115935:0;more deis-rel-wilder-production.log
: 1495115941:0;rm *rel*.log
: 1495115954:0;mv iris-prod-messages-table-investigation-performance.log Documents/iris
: 1495115962:0;more pbcopy
: 1495115966:0;rm pbcopy
: 1495115975:0;more list
: 1495115977:0;rm list
: 1495115984:0;more unicorn_wd_list
: 1495115994:0;mv unicorn_wd_list Documents/walkadoo
: 1495116003:0;mv wd*.log Documents/walkadoo
: 1495116021:0;mv wbid-*.log Documents/wbid/
: 1495116027:0;rm 1 
: 1495116029:0;rm 2
: 1495116031:0;rm 3
: 1495116065:0;ssh admin@ip-172-24-148-106.ec2.internal
: 1495130447:0;telnet wilder-production2.cduvhvvpw6oo.us-east-1.rds.amazonaws.com 3306
: 1495130465:0;telnet wilder-production2-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com 3306
: 1495222877:0;kubectl get pods -o wide  --all-namespaces grep builder
: 1495222917:0;kubectl --namespace deis logs deis-builder-157405133-xscpd
: 1495222932:0;nslookup 172.20.199.67
: 1495222948:0;kubectl get nodes -o wide | grep p-172-20-199-67
: 1495222962:0;kubectl describe node ip-172-20-199-67.ec2.internal
: 1495222973:0;kubectl describe nodes ip-172-20-199-67.ec2.internal
: 1495222985:0;kubectl describe nodes 
: 1495223580:0;curl https://myh-deis-staging-builder-ywhet.s3.amazonaws.com
: 1495223598:0;aws s3 ls myh-deis-staging-builder
: 1495223656:0;kubectl --namespace quitnet-staging logs quitnet-staging-web-2579881708-4jq3j
: 1495223695:0;kubectl --namespace quitnet-staging logs quitnet-staging-web-1032838820-8ncd8
: 1495223773:0;kubectl delete pod quitnet-staging-clockwork-2231139402-8fsmp --namespace quitnet-staging
: 1495223927:0;kubectl --namespace deis logs slugbuild-quitnet-staging-b1358370-42617044 > deis-logs-slugbuild-quitnet-staging-b1358370-42617044.log
: 1495223931:0;kubectl --namespace deis logs slugbuild-quitnet-staging-b1358370-42617044
: 1495224875:0;kubectl --namespace kube-system logs kube-apiserver-ip-172-20-209-91.ec2.internal
: 1495224937:0;kubectl get events --namespace quitnet-staging
: 1495224959:0;kubectl get events --namespace quitnet-staging > kubectl-get-events-quitnet-staging.051917.log
: 1495328278:0;aws cloudwatch describe-alarms-for-metric --metric-name StatusCheckFailed
: 1495328339:0;aws cloudwatch describe-alarms-for-metric --metric-name StatusCheckFailed --namespace AWS/EC2
: 1495328381:0;aws cloudwatch describe-alarms-for-metric --namespace AWS/EC2
: 1495328434:0;aws cloudwatch describe-alarms > cloudwatch-describe-alarms-all.052017.txt
: 1495328497:0;aws cloudwatch set-alarm-state help
: 1495328791:0;aws cloudwatch describe-alarms --metric-name "StatusCheckFailed" --generate-cli-skeleton > aws-cloudwatch-describe-alarms-generate-cli-skeleton.052017.txt
: 1495328801:0;aws cloudwatch describe-alarms --metric-name "StatusCheckFailed"
: 1495328815:0;aws cloudwatch describe-alarms --generate-cli-skeleton > aws-cloudwatch-describe-alarms-generate-cli-skeleton.052017.txt
: 1495328820:0;more aws-cloudwatch-describe-alarms-generate-cli-skeleton.052017.txt
: 1495328902:0;aws cloudwatch describe-alarms  --generate-cli-skeleton help
: 1495328915:0;aws cloudwatch describe-alarms  --generate-cli-skeleton output
: 1495328926:0;aws cloudwatch describe-alarms output
: 1495463292:0;kubectl get pods -o wide  --all-namespaces | grep listener
: 1495465641:0;kubectl get pods -o wide  --all-namespaces | grep gcm
: 1495465651:0;kubectl get pods -o wide  --all-namespaces | grep houston
: 1495465664:0;kubectl get pods -o wide  --all-namespaces | grep twilio
: 1495465733:0;kubectl get pods -o wide  --all-namespaces | grep jobs
: 1495465774:0;kubectl get pods -o wide  --all-namespaces | grep clock
: 1495466894:0;aws rds describe-db-instances --db-instance-identifier Promoting the read replica -- It did not work as I expected. It did not make the former master the read replica. 
: 1495466920:0;aws rds describe-db-instances --db-instance-identifier shared-staging-mysql-test
: 1495466938:0;while true ; do echo "$(date)" ; date >> wilder-production2-read1.upgrade.log; aws rds describe-db-instances --db-instance-identifier wilder-production2-read1 >> shared-staging-mysql-test.log ; sleep 30;  done
: 1495466956:0;while true ; do echo "$(date)" ; date >> wilder-production2-read1.upgrade.log; aws rds describe-db-instances --db-instance-identifier shared-staging-mysql-test>> shared-staging-mysql-test.log ; sleep 30;  done
: 1495467000:0;rm shared-staging-mysql-test.log
: 1495467002:0;while true ; do echo "$(date)" ; date >> shared-staging-mysql-test.log ; aws rds describe-db-instances --db-instance-identifier shared-staging-mysql-test >> shared-staging-mysql-test.log ; sleep 30;  done
: 1495469011:0;while true ; do echo "$(date)" ; date >> shared-staging-mysql-test-check-main-on-replica-create.log ; aws rds describe-db-instances --db-instance-identifier shared-staging-mysql-test >> shared-staging-mysql-test-check-main-on-replica-create.log ; sleep 30;  done
: 1495469027:0;while true ; do echo "$(date)" ; date >> shared-staging-mysql-test-read1-replica.log ; aws rds describe-db-instances --db-instance-identifier shared-staging-mysql-test-read1 >> shared-staging-mysql-test-read1-replica.log ; sleep 30;  done
: 1495471384:0;while true ; do echo "$(date)" ; date >> shared-staging-mysql-test-read1-replica-upgrade.log ; aws rds describe-db-instances --db-instance-identifier shared-staging-mysql-test-read1 >> shared-staging-mysql-test-read1-replica-upgrade.log ; sleep 30;  done
: 1495473376:0;ssh ubuntu@ec2-54-235-27-46.compute-1.amazonaws.com
: 1495476214:0;while true ; do echo "$(date)" ; date >> shared-staging-mysql-test-upgrade-in-place-aws-desc.log ; aws rds describe-db-instances --db-instance-identifier shared-staging-mysql-test >> shared-staging-mysql-test-upgrade-in-place-aws-desc.log ; sleep 30;  done
: 1495476278:0;ssh admin@ip-172-20-146-0
: 1495476297:0;ssh admin@ip-172-20-146-0.ec2.internal
: 1495476398:0;more shared-staging-mysql-test-upgrade-in-place-aws-desc.log
: 1495476414:0;cat shared-staging-mysql-test-upgrade-in-place-aws-desc.log | grep DBInstanceStatus
: 1495553543:0;helm ls
: 1495553554:0;history | grep helm
: 1495558780:0;ls -l config
: 1495558785:0;ls -l config/initializers
: 1495559943:0;;dlsproc
: 1495559950:0;DEIS_PROFILE=staging deis ps:list -a
: 1495559965:0;DEIS_PROFILE=staging deis ps:list -a airflow-staging
: 1495560012:0;for i in `DEIS_PROFILE=staging deis apps | grep -v "^==="` \
do \
DEIS_PROFILE=staging deis ps:list -a $i \
done 
: 1495560096:0;or i in `DEIS_PROFILE=staging deis apps | grep -v "^==="`
: 1495560096:0;DEIS_PROFILE=staging deis ps:list -a $i
: 1495560131:0;for i in `DEIS_PROFILE=production deis apps | grep -v "^==="`\
do\
DEIS_PROFILE=staging deis ps:list -a $i\
done
: 1495560152:0;DEIS_PROFILE=production deis apps | grep -v "^==="
: 1495560180:0;for i in `DEIS_PROFILE=production deis apps | grep -v "^==="`\
do\
DEIS_PROFILE=production deis ps:list -a $i\
done
: 1495560275:0;DEIS_PROFILE=staging deis logs 
: 1495560291:0;kubectl --namespace=deis get pods
: 1495560324:0;kubectl --namespace=deis logs deis-logger-fluentd-z7r8h
: 1495560352:0;kubectl --namespace=deis exec deis-logger-fluentd-z7r8h ls /var/log/containers
: 1495561309:0;brew list deis
: 1495561312:0;brew list helm
: 1495561323:0;brew list kubectl 
: 1495561340:0;brew list 
: 1495561382:0;history | grep pip
: 1495564344:0;shipit console  wellbeingid-production
: 1495564853:0;shipit env wellbeingid-production 
: 1495564859:0;shipit show env wellbeingid-production 
: 1495564867:0;history | grep shipit
: 1495565683:0;shipit deployment create wellbeingid-production 65553de7326a91826feb615c95abfe20915dc8b7
: 1495565779:0;shipit deployment log 5318
: 1495633224:0;for i in `DEIS_PROFILE=staging deis apps | grep -v "^==="`\
do\
DEIS_PROFILE=staging deis ps:list -a $i\
done
: 1495633258:0;DEIS_PROFILE=staging deis apps:list \

: 1495633642:0;DEIS_PROFILE=staging deis releases -a hello200-staging\

: 1495633659:0;DEIS_PROFILE=staging deis config:list  -a hello200-staging\

: 1495634503:0;DEIS_PROFILE=staging deis config:set -a hello200-staging MAINTENANCE_MODE=true 
: 1495634600:0;DEIS_PROFILE=staging deis ps:list  -a hello200-staging\

: 1495634620:0;DEIS_PROFILE=staging deis config:list  -a hello200-staging | grep MAINT\

: 1495634798:0;more /etc/resolv.conf
: 1495635280:0;cd bamboo
: 1495635286:0;cd bamboo-server
: 1495635307:0;mv Gemfile.lock gemfile.lock.heidi
: 1495635317:0;git checkout Gemfile.lock
: 1495635795:0;rbenv env
: 1495635807:0;history | grep rbenv | grpe install
: 1495635813:0;history | grep rbenv | grep install
: 1495635822:0;rbenv install 2.4.1
: 1495636080:0;grep myh-util Gem*
: 1495636111:0;cd quitnet-server
: 1495636132:0;cd deployment-api
: 1495636170:0;mv Gemfile.lock Gemfile.lock.heidi 
: 1495636183:0;git stash Gemfile.lock
: 1495636199:0;git stash save Gemfile.lock
: 1495636223:0;git pull 
: 1495636237:0;grep myh Gem*
: 1495636353:0;cd chef-meyouhealth
: 1495636366:0;cd chef-meyouhealth-rabbitmq
: 1495636377:0;cd chef-repo
: 1495636385:0;cd ../chef-walkadoo
: 1495636418:0;cd reporting
: 1495636432:0;rbenv install 2.3.4
: 1495637698:0;DEIS_PROFILE=staging deis config:unset -a hello200-staging MAINTENANCE_MODE 
: 1495638193:0;DEIS_PROFILE=staging deis config:list -a iris-staging | grep MAINTENA
: 1495638894:0;more .gem*
: 1495639851:0;git clone git@github.com:meyouhealth/wilder.git
: 1495639897:0;grep myh Gemfile
: 1495639899:0;grep myh Gemfile*
: 1495646668:0;which docker
: 1495646801:0;uname -r
: 1495646827:0;docker --version
: 1495646981:0;brew list docker
: 1495647035:0;which systemctl
: 1495647039:0;which sysctl 
: 1495647044:0;sysctl --help
: 1495647047:0;sysctl -a
: 1495647083:0;docker ps 
: 1495647190:0;ls -l /var/run
: 1495647193:0;ls -l /var/run/docker.sock
: 1495647201:0;more /var/run/docker.sock
: 1495647209:0;strings /var/run/docker.sock
: 1495647286:0;more /etc/group
: 1495647292:0;more /etc/group | grep dock
: 1495647341:0;id
: 1495647353:0;ps -ef | grep dock
: 1495647382:0;ls -ltra /Users/heidischmidt/Library/Containers/com.docker.docker/Data/s60
: 1495648176:0;sudo docker images
: 1495648214:0;docker search centos
: 1495648222:0;docker search ubuntu
: 1495648266:0;docker search kinesis
: 1495648326:0;brew search elasticsearch
: 1495648340:0;brew install elasticsearch
: 1495648376:0;docker pull hello-world
: 1495648434:0;docker pull hello-world:latest
: 1495648468:0;docker run 48b5124b2768
: 1495648530:0;brew doctor
: 1495648566:0;brew upgrade Xcode
: 1495648575:0;brew list xcode
: 1495649021:0;docker inspect hello-world
: 1495653044:0;shipit env config_list wellbeingid-staging | grep -i ftp
: 1495720277:0;DEIS_PROFILE=staging deis config:set -a iris-staging MAINTENANCE_MODE=true 
: 1495720324:0;shipit config-list wellbeingid-staging
: 1495720331:0;shipit config_list wellbeingid-staging
: 1495720346:0;history | grep shipit | grep config 
: 1495720368:0;DEIS_PROFILE=staging deis apps:list
: 1495720387:0;DEIS_PROFILE=staging deis config:set -a wbt-staging MAINTENANCE_MODE=true 
: 1495720423:0;shipit env config_set wellbeingid-staging MAINTENANCE_MODE=true
: 1495720433:0;shipit env config_set wellbeingid-staging MAINTENANCE_MODE true
: 1495720489:0;shipit create deployment wellbeingid-staging f4bd281cc1620ac8b248fbea08f6cb336424991d
: 1495720501:0;history | grep shipit | grep deployment
: 1495720557:0;DEIS_PROFILE=staging deis config:list -a iris-staging 
: 1495720561:0;DEIS_PROFILE=staging deis config:list -a iris-staging  | grep MAIN
: 1495720568:0;DEIS_PROFILE=staging deis config:list -a wbt-staging  | grep MAIN
: 1495722875:0;shipit deployment logs 5327
: 1495724008:0;shipit env config_unset wellbeingid-staging MAINTENANCE_MODE 
: 1495724023:0;shipit env config_list wellbeingid-staging | grep MAIN
: 1495724054:0;shipit deployment create wellbeingid-staging f4bd281cc1620ac8b248fbea08f6cb336424991d
: 1495726080:0;DEIS_PROFILE=staging deis config:unset -a wbt-staging MAINTENANCE_MODE
: 1495726216:0;DEIS_PROFILE=staging deis config:unset -a iris-staging MAINTENANCE_MODE
: 1495736879:0;docker image rmi ca5784ed6a77
: 1495736892:0;docker image rmi cd55ec630bb1
: 1495736905:0;docker search elasticsearch
: 1495736920:0;docker pull elasticsearch
: 1495737225:0;docker inspect elasticsearch
: 1495738169:0;docker run docker/whalesay
: 1495738256:0;docker run docker/whalesay cowsay Whoot!
: 1495738321:0;docker inspect docker/whalesay
: 1495738557:0;ifconfig
: 1495738747:0;docker run -it elasticsearch:latest /bin/bash 
: 1495739128:0;docker inspect dazzling_wozniak
: 1495739171:0;docke rps
: 1495739174:0;docker ps
: 1495739191:0;ping 172.17.0.2
: 1495739224:0;telnet 172.17.0.2 9200
: 1495739287:0;brew search elinks
: 1495739298:0;brew install elinks
: 1495740684:0;brew install awscli
: 1495740779:0;brew install aws-shell
: 1495740807:0;brew link --overwrite awscli
: 1495740819:0;brew link --overwrite --dry-run awscli
: 1495740840:0;aws rds describe-db-instances --db-instance-identifier shared-staging-mysql
: 1495740852:0;aws-shell --help
: 1495740878:0;aws-shell -p heidi-test
: 1496152106:0;kubectl get pods -o wide  --all-namespaces | grep elastic
: 1496164604:0;aws rds describe-db-instances --db-instance-identifier reporting
: 1496164623:0;aws rds describe-db-instances --db-instance-identifier reporting-production
: 1496165110:0;aws rds describe-db-instances --db-instance-identifier reporting-production-rds
: 1496166455:0;DEIS_PROFILE=staging deis config:list -a walkadoo-staging | grep PUSH
: 1496166483:0;DEIS_PROFILE=production deis config:list -a walkadoo-production | grep PUSH
: 1496166516:0;kubectl get pods -o wide  --all-namespaces | grep walkad
: 1496166617:0;DEIS_PROFILE=production deis config:set -a walkadoo-production PROMETHEUS_PUSH_GATEWAY_URL=http://prometheus-pushgateway.prometheus.svc.cluster.local:9091
: 1496166705:0;DEIS_PROFILE=production deis config:list -a walkadoo-production | grep PRO
: 1496172402:0;kubectl delete pod prometheus-pushgateway-2921373215-48vwn --namespace prometheus
: 1496172838:0;shipit config:list env reporting-production
: 1496172846:0;shipit config_list env reporting-production
: 1496172855:0;history | grep shipit | grpe config_list
: 1496172861:0;history | grep shipit | grep config_list
: 1496172882:0;shipit env config_list reporting-production | grep MAIN
: 1496172907:0;history | grep reporting-production
: 1496173006:0;DEIS_PROFILE=staging deis apps
: 1496173340:0;DEIS_PROFILE=staging deis releases -a walkadoo-staging
: 1496238216:0;while true ; do echo "$(date)" ; date >> shared-prod-mysql-upgrade-aws-desc.log ; aws rds describe-db-instances --db-instance-identifier mysql-shared-production >> shared-prod-mysql-upgrade-aws-desc.log ; sleep 30;  done
: 1496238772:0;tail -100 shared-prod-mysql-upgrade-aws-desc.log
: 1496238915:0;shipit deployment create wellbeingid-staging b32771e5ea18ef7a7833516110a7e1e0c9598576
: 1496239151:0;shipit deployment logs 5339
: 1496239666:0;shipit console legacy-dashboard
: 1496240604:0;ssh heidi.schmidt@ec2-54-160-100-223.compute-1.amazonaws.com
: 1496241100:0;ssh ubuntu@ec2-54-160-116-153.compute-1.amazonaws.com
: 1496241523:0;ssh ubuntu@ec2-54-196-151-95.compute-1.amazonaws.com
: 1496241558:0;ssh ubuntu@ec2-54-198-177-51.compute-1.amazonaws.com
: 1496241618:0;ssh ubuntu@ec2-54-163-155-155.compute-1.amazonaws.com
: 1496242329:0;DEIS_PROFILE=staging deis users
: 1496242358:0;DEIS_PROFILE=production deis users
: 1496242655:0;deis whoami
: 1496242673:0;DEIS_PROFILE=production deis whoami
: 1496242680:0;DEIS_PROFILE=staging deis whoami
: 1496242740:0;ls -l .deis/
: 1496245904:0;kubectl config use-contexts lrywh.k8s.myhstg.com
: 1496245919:0;kubectl get pods -o wide  --all-namespaces | grep data
: 1496246011:0;kubectl get pods -o wide  --all-namespaces | grep da
: 1496246039:0;kubectl describe pod prometheus-pushgateway-2921373215-25xvj
: 1496246047:0;kubectl describe pod prometheus-pushgateway-2921373215-25xvj --namespace prometheus
: 1496248731:0;DEIS_PROFILE=production deis release -a walkadoo-production
: 1496248737:0;DEIS_PROFILE=production deis releases -a walkadoo-production
: 1496250364:0;DEIS_PROFILE=production deis releases -a walkadoo-production | head -10 
: 1496250386:0;DEIS_PROFILE=production deis ps:list -a walkadoo-production 
: 1496250397:0;deis ps --help
: 1496253846:0;ssh ubuntu@10.144.36.33
: 1496254144:0;DEIS_PROFILE=staging deis apps 
: 1496254569:0;DEIS_PROFILE=production deis ps:list 
: 1496254580:0;DEIS_PROFILE=production deis ps:list -a dailychallenge-production
: 1496254589:0;DEIS_PROFILE=staging deis ps:list -a dailychallenge-staging
: 1496254606:0;DEIS_PROFILE=staging deis releases -a dailychallenge-staging
: 1496255234:0;history | grep knife | grep bluepill
: 1496255759:0;kubectl get pods -o wide  --all-namespaces | grep dail
: 1496257504:0;ping ec2-54-158-30-198.compute-1.amazonaws.com
: 1496258120:0;docker run hello-world:latest
: 1496258140:0;docker search whalesay
: 1496258394:0;ssh ubuntu@ec2-54-158-30-198.compute-1.amazonaws.com
: 1496283642:0;kubectl get pods -o wide  --all-namespaces | grep quit
: 1496283662:0;kubectl get events --namespace quitnet-production
: 1496283840:0;DEIS_PROFILE=production deis ps:list -a iris-production
: 1496283886:0;DEIS_PROFILE=production deis ps:list -a quitnet-production
: 1496284016:0;kubectl get events --namespace iris-production
: 1496284082:0;kubectl --namespace iris-production logs iris-production-web-836594119-rjr5x
: 1496284487:0;kubectl --namespace iris-production logs iris-production-jobs-2578382482-1m57q
: 1496284604:0;kubectl describe node ip-172-24-185-142.ec2.internal
: 1496284707:0;kubectl get pods -o wide  --all-namespaces | grep iris-prod
: 1496284791:0;kubectl get pods -o wide  --all-namespaces | grep quitnet | grep ip-172-24-185-142.ec2.internal
: 1496284821:0;kubectl get pods -o wide  --all-namespaces | grep quitnet | grep ip-172-24-148-106.ec2.internal
: 1496284860:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-206-96.ec2.internal
: 1496285181:0;ssh admin@ip-172-24-185-142.ec2.internal)
: 1496285186:0;ssh admin@ip-172-24-185-142.ec2.internal
: 1496286494:0;kubectl --namespace iris-production logs --tail 100 -f iris-production-web-836594119-cf16v
: 1496286514:0;kubectl --namespace iris-production logs iris-production-web-836594119-cf16v
: 1496286611:0;kubectl --namespace iris-production logs --tail 100 -f iris-production-web-836594119-cf16v > Documents/kubernetes/iris-production-web-836594119-cf16v-logs.log
: 1496286624:0;more Documents/kubernetes/iris-production-web-836594119-cf16v-logs.log
: 1496286652:0;kubectl delete pod iris-production-web-836594119-rjr5x --namespace iris-production
: 1496286708:0;kubectl --namespace iris-production logs iris-production-web-836594119-j6557
: 1496286741:0;kubectl delete pod iris-production-web-836594119-cf16v --namespace iris-production
: 1496286793:0;kubectl --namespace iris-production logs iris-production-web-836594119-d7f8x
: 1496286861:0;kubectl delete pod iris-production-clockwork-2752579365-5z7n7 --namespace iris-production
: 1496286923:0;kubectl --namespace iris-production logs iris-production-clockwork-2752579365-5r79q
: 1496286938:0;kubectl delete pod iris-production-wbidaccountchangeslistener-1380000561-59jtt --namespace iris-production
: 1496286992:0;kubectl delete pod iris-production-jobs-2578382482-1m57q --namespace iris-production
: 1496287017:0;kubectl --namespace iris-production logs iris-production-wbidaccountchangeslistener-1380000561-63ldk --namespace iris-production
: 1496287199:0;kubectl delete pod quitnet-production-web-1144980538-lg0lw --namespace quitnet-production
: 1496287227:0;kubectl delete pod quitnet-production-web-1144980538-gxh2b --namespace quitnet-production
: 1496287316:0;kubectl delete pod quitnet-production-web-1144980538-cwhdr --namespace quitnet-production
: 1496287412:0;kubectl delete pod quitnet-production-houston-1014931085-nvvcv --namespace quitnet-production
: 1496287430:0;kubectl delete pod quitnet-production-conciergelistener-4181831329-319tm --namespace quitnet-production
: 1496287473:0;kubectl delete pod quitnet-production-clockwork-4167541129-w8bff --namespace quitnet-production
: 1496287499:0;kubectl delete pod quitnet-production-rabbitmq-3275857813-4clvx --namespace quitnet-production
: 1496287517:0;kubectl delete pod quitnet-production-jobs-2561317125-0bdfq --namespace quitnet-production
: 1496326820:0;kubectl --namespace elasticsearch-staging get pods -l app=elasticsearch-staging
: 1496326847:0;kubectl --namespace elasticsearch-staging get pods -l app=elasticsearch-staging -o name | sed 's/^.*\///' | xargs -I{} kubectl --namespace elasticsearch-staging port-forward {} 8080:8080
: 1496327770:0;kubectl --namespace elasticsearch-production get pods -l app=elasticsearch-production
: 1496327806:0;kubectl --namespace elasticsearch-production get pods -l app=elasticsearch-production -o name | sed 's/^.*\///' | xargs -I{} kubectl --namespace elasticsearch-production port-forward {} 8080:8080
: 1496332890:0;history | grep ssh 
: 1496332901:0;ssh-add -l
: 1496333191:0;grep MYH .??*
: 1496333215:0;cp .shipit_profile .shipit_profile-onelogin
: 1496333220:0;vi .shipit_profile
: 1496333347:0;source .shipit_profile
: 1496333401:0;vi .zshrc
: 1496333471:0;shipit deployment logs 5345
: 1496334938:0;env | grep SHIP
: 1496343038:0;docker rmi 434efa6ed9db 
: 1496343122:0;docker rm 434efa6ed9db
: 1496343129:0;docker rmi 434efa6ed9db
: 1496343135:0;docker --help
: 1496343191:0;docker rm sleepy_noether
: 1496343201:0;docker rm zealous_davinci
: 1496343217:0;docker rm blissful_hugle
: 1496343220:0;docker ps -a
: 1496344145:0;ssh heidischmidt@54.204.40.159
: 1496344211:0;ssh ubuntu@ec2-54-204-40-159.compute-1.amazonaws.com
: 1496344222:0;history | grep ssh
: 1496344232:0;ssh ubuntu@ec2-54-237-111-111.compute-1.amazonaws.com
: 1496344244:0;ssh heidischmidt@ec2-54-237-111-111.compute-1.amazonaws.com
: 1496344274:0;ssh heidischmidt@ec2-54-204-40-159.compute-1.amazonaws.com
: 1496344481:0;shipit ssh deployment-api
: 1496344493:0;shipit ssh legacy-deploy-dashboard
: 1496344508:0;shipit ssh myh-builder
: 1496344611:0;shipit console legacy-deploy-dashboard
: 1496344624:0;shipit console myh-builder
: 1496344639:0;shipit console reporting-production
: 1496344723:0;shipit create deployment wellbeingid-staging 869b4c3a64f7ca363d2b251c535514c048755050
: 1496344811:0;exit
: 1496344821:0;source .zshrc
: 1496345423:0;env | grep MYH
: 1496345430:0;env | grep DEP
: 1496345435:0;cat .shipit_profile
: 1496345469:0;export MYH_BUILDER_URL=https://myh-builder.meyouhealth.com/api/v2/
: 1496345474:0;export MYH_BUILDER_USERNAME=meyouhealth
: 1496345481:0;export MYH_BUILDER_PASSWORD=vNJXsyQMXi3s
: 1496345487:0;export DEPLOYMENT_API_URL=https://deployment-api.meyouhealth.com/
: 1496345493:0;export DEPLOYMENT_API_USERNAME=meyouhealth
: 1496345498:0;export DEPLOYMENT_API_PASSWORD=yW2azfoYLxrz
: 1496345546:0;MYH_BUILDER_URL=https://myh-builder.meyouhealth.com/api/v2/
: 1496345546:0;MYH_BUILDER_USERNAME=meyouhealth
: 1496345546:0;MYH_BUILDER_PASSWORD=vNJXsyQMXi3s
: 1496345547:0;DEPLOYMENT_API_URL=https://deployment-api.meyouhealth.com/
: 1496345547:0;DEPLOYMENT_API_USERNAME=meyouhealth
: 1496345547:0;DEPLOYMENT_API_PASSWORD=yW2azfoYLxrz
: 1496345560:0;echo $MYH_BUILDER_USERNAME
: 1496345568:0;echo $MYH_BUILDER_PASSWORD
: 1496345577:0;echo $DEPLOYMENT_API_URL
: 1496345586:0;echo $DEPLOYMENT_API_USERNAME
: 1496345596:0;echo $DEPLOYMENT_API_PASSWORD
: 1496345707:0;more .shipit_profile
: 1496345736:0;grep MYH .zshrc
: 1496345744:0;grep DEP .zshrc
: 1496346091:0;history | grep shipit | grep config
: 1496346098:0;shipit env config_list wellbeingid-production
: 1496346118:0;shipit deployment create wellbeingid-production 869b4c3a64f7ca363d2b251c535514c048755050
: 1496346183:0;shipit deployment logs 5346
: 1496346194:0;Running chef-client on node: i-7eb69282
: 1496346302:0;shipit env config_list wellbeingid-staging
: 1496346327:0;shipit deployment create wellbeingid-staging 869b4c3a64f7ca363d2b251c535514c048755050
: 1496346351:0;shipit deployment logs 5347
: 1496346589:0;history | grep ec2-54-160-100-223.compute-1.amazonaws.com
: 1496346599:0;ssh ubuntu@ec2-54-160-100-223.compute-1.amazonaws.com
: 1496346612:0;ssh heidischmidt@ec2-54-160-100-223.compute-1.amazonaws.com
: 1496410204:0;shipit deployment create wellbeingid-staging 869b4c3a64f7ca363d2b251c535514c048755050 
: 1496427782:0;kubectl get pods -o wide  --all-namespaces | grep web
: 1496427794:0;ssh admin@ip-172-24-184-178.ec2.internal
: 1496427998:0;elinks http://localhost:9090
: 1496429861:0;find . -name "Docker*" -print
: 1496430155:0;cat docker-compose.yml
: 1496431342:0;grep travis *
: 1496431348:0;more .travis.yml
: 1496431534:0;docker pull quay.io/myhadmin/wellbeingid:869b4c3a64f7ca363d2b251c535514c048755050
: 1496431592:0;docker images
: 1496431602:0;docker inspect quay.io/myhadmin/wellbeingid
: 1496431613:0;docker inspect 0eb316df68e9
: 1496431640:0;docker version
: 1496431713:0;which df -Ph image
: 1496431719:0;which dfimage
: 1496775300:0;kubectl --namespace walkadoo-production logs walkadoo-production-web-2510265908-1653j
: 1496775313:0;kubectl --namespace walkadoo-production logs walkadoo-production-web-2510265908-1653j --timestamp
: 1496775323:0;kubectl --namespace walkadoo-production logs walkadoo-production-web-2510265908-1653j --timestamps
: 1496775467:0;kubectl --namespace walkadoo-production logs walkadoo-production-web-2510265908-1653j --timestamps | grep "2017-06-06T17:"
: 1496775474:0;kubectl --namespace walkadoo-production logs walkadoo-production-web-2510265908-1653j --timestamps | grep "2017-06-06T17"
: 1496775482:0;kubectl --namespace walkadoo-production logs walkadoo-production-web-2510265908-1653j --timestamps | head
: 1496775504:0;kubectl --namespace walkadoo-production logs walkadoo-production-web-2510265908-1653j --timestamps | more
: 1496842928:0;cd kops
: 1496842943:0;echo $CDPATH
: 1496842947:0;more .inputrc
: 1496842980:0;cat .inputrc
: 1496843037:0;kubectl --namespace prometheus get pods -l app=prometheus -o name | sed 's/^.*\///' | xargs -I{} kubectl --namespace prometheus port-forward {} 9090:9090 & 
: 1496846021:0;kill %1
: 1496846382:0;kubectl get pods -o wide  --all-namespaces | grep push
: 1496846386:0;kubectl --namespace prometheus get pods -l app=prometheus-pushgateway -o name | sed 's/^.*\///' | xargs -I{} kubectl --namespace prometheus port-forward {} 9091:9091
: 1496850174:0;kubectl edit deployment prometheus-config --namespace prometheus 
: 1496850280:0;kubectl edit configmap prometheus-config --namespace prometheus 
: 1496862346:0;kubectl get pods -o wide  --all-namespaces | head
: 1496863370:0;kubectl edit deployment prometheus --namespace prometheus
: 1497275941:0;kubectl get nodes -o wide  | grep 178
: 1497276520:0;kubectl --namespace deis logs deis-logger-fluentd-ztnvr
: 1497276757:0;kubectl describe pod deis-logger-fluentd-ztnvr --namespace deis
: 1497277709:0;kubectl --namespace deis logs deis-logger-fluentd-ph017
: 1497277743:0;kubectl describe pod deis-logger-fluentd-ph017 --namespace deis
: 1497281420:0;kubectl --namespace wbt-staging logs wbt-staging-web-3245186202-3zrhp 
: 1497281453:0;kubectl --namespace deis logs deis-builder-2775829755-95zhv
: 1497281490:0;kubectl get pods -o wide  --all-namespaces | wbt-staging
: 1497281637:0;kubectl delete pod wbt-staging-web-3245186202-3zrhp --namespace wbt-staging
: 1497281822:0;kubectl delete pod slugbuild-wbt-staging-c4faf6cd-b7115c45 --namespace deis
: 1497281835:0;kubectl get pods -o wide  --all-namespaces | grep wbt-staging-web
: 1497282252:0;kubectl get pods -o wide  --all-namespaces | grep slug
: 1497282478:0;kubectl --namespace deis logs wbt-staging-web-3245186202-3zrhp
: 1497282888:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-20-174-62.ec2 > node-issues-ip-172-20-174-62.ec2
: 1497282965:0;kubectl --namespace deis logs deis-logger-fluentd-x3v92
: 1497282995:0;kubectl --namespace deis logs deis-monitor-telegraf-ltnmf | uniq -c 
: 1497283020:0;kubectl --namespace deis logs deis-monitor-telegraf-ltnmf | awk -F "ERROR" '{priont $2}'
: 1497283023:0;kubectl --namespace deis logs deis-monitor-telegraf-ltnmf | awk -F "ERROR" '{print $2}'
: 1497283028:0;kubectl --namespace deis logs deis-monitor-telegraf-ltnmf | awk -F "ERROR" '{print $2}' | uniq -c
: 1497283054:0;vi node-issues-ip-172-20-174-62.ec2
: 1497283066:0;kubectl --namespace deis logs deis-monitor-telegraf-ltnmf
: 1497283071:0;kubectl --namespace deis logs deis-monitor-telegraf-ltnmf | tail -3 
: 1497283076:0;kubectl --namespace deis logs deis-monitor-telegraf-ltnmf | tail -3  >> node-issues-ip-172-20-174-62.ec2
: 1497283109:0;kubectl --namespace deis logs deis-nsqd-3597503299-0ldjf
: 1497283131:0;kubectl --namespace deis logs slugbuild-wbt-staging-c4faf6cd-b7115c45
: 1497283138:0;kubectl --namespace deis logs slugbuild-wbt-staging-c4faf6cd-b7115c45 >> node-issues-ip-172-20-174-62.ec2
: 1497283164:0;kubectl --namespace hurby-server-staging logs hurby-server-staging-web-163913225-tg2xg
: 1497283188:0;kubectl --namespace kube-system logs calico-node-dfzrj
: 1497283198:0;kubectl --namespace kube-system logs calico-node-dfzrj calico-node
: 1497283210:0;kubectl --namespace kube-system logs calico-node-dfzrj 
: 1497283216:0;kubectl --namespace kube-system logs calico-node-dfzrj install-cni
: 1497283264:0;kubectl --namespace kube-system logs kube-dns-782804071-n2szx kubedns
: 1497283275:0;kubectl --namespace kube-system logs kube-dns-782804071-n2szx
: 1497283280:0;kubectl --namespace kube-system logs kube-dns-782804071-n2szx dnsmasq
: 1497283288:0;kubectl --namespace kube-system logs kube-dns-782804071-n2szx dnsmasq-metrics
: 1497283293:0;kubectl --namespace kube-system logs kube-dns-782804071-n2szx healthz
: 1497283311:0;kubectl --namespace kube-system logs kube-proxy-ip-172-20-174-62.ec2.internal
: 1497283320:0;kubectl --namespace kube-system logs tiller-deploy-3066893457-tg2p0
: 1497283337:0;kubectl --namespace prometheus logs alertmanager-2060873422-pw2hj
: 1497283386:0;kubectl --namespace walkadoo-staging logs walkadoo-staging-houston-3960157136-zwh34
: 1497283402:0;kubectl --namespace wbt-staging logs wbt-staging-web-3245186202-3zrhp
: 1497283424:0;kubectl --namespace wilder-staging logs wilder-staging-clockwork-2721989370-472ls 
: 1497283433:0;kubectl --namespace wilder-staging logs wilder-staging-web-2642191772-699fr
: 1497283456:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-20-174-62.ec2
: 1497283506:0;kubectl describe node ip-172-20-174-62.ec2.internal
: 1497283511:0;kubectl describe node ip-172-20-174-62.ec2.internal >> node-issues-ip-172-20-174-62.ec2
: 1497283638:0;kubectl get events --all-namespaces >> node-issues-ip-172-20-174-62.ec2
: 1497283657:0;date >> node-issues-ip-172-20-174-62.ec2
: 1497283666:0;kubectl get events >> node-issues-ip-172-20-174-62.ec2
: 1497283815:0;ssh admin@ip-172-20-174-62.ec2.internal
: 1497283944:0;scp admin@ip-172-20-174-62.ec2.internal:/var/log/daemon.log ip-172-20-174-62-daemon.log 
: 1497283989:0;scp admin@ip-172-20-174-62.ec2.internal:/var/log/syslog.log ip-172-20-174-62-syslog.log 
: 1497283996:0;scp admin@ip-172-20-174-62.ec2.internal:/var/log/syslog ip-172-20-174-62-syslog.log 
: 1497284050:0;scp admin@ip-172-20-174-62.ec2.internal:/var/log/containers.log.pos ip-172-20-174-62-containers.log.pos 
: 1497284107:0;scp admin@ip-172-20-174-62.ec2.internal:/var/log/messages ip-172-20-174-62-messages
: 1497284126:0;scp admin@ip-172-20-174-62.ec2.internal:/var/log/kube-proxy.log ip-172-20-174-62-kube-proxy.log
: 1497284155:0;scp admin@ip-172-20-174-62.ec2.internal:/var/log/kern.log ip-172-20-174-62-kern.log
: 1497284199:0;scp admin@ip-172-20-174-62.ec2.internal:/var/log/daemon.log.1 ip-172-20-174-62-daemon.log.1
: 1497284261:0;scp admin@ip-172-20-174-62.ec2.internal:/var/log/nrsysmond.log ip-172-20-174-62-nrsysmond.log
: 1497284330:0;kubectl get pods -o wide  --all-namespaces | grep -v Running >> node-issues-ip-172-20-174-62.ec2
: 1497284556:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-20-220-183
: 1497284614:0;kubectl get pods -o wide  --all-namespaces | grep  ip-172-20-174-62
: 1497284634:0;kubectl get pods -o wide  --all-namespaces | grep  dc-staging-reminder
: 1497284648:0;kubectl get pods -o wide  --all-namespaces | grep  walkadoo-staging-houston
: 1497288285:0;grep AWS ip-172-20-174-62*
: 1497288736:0;more ip-172-20-174-62-kern.log
: 1497288739:0;more ip-172-20-174-62-daemon.log
: 1497289135:0;kubectl config use-contexts neywk.k8s.meyouhealth.com
: 1497293753:0;cd walkadoo
: 1497293788:0;find . -name secrets.yml -print
: 1497293790:0;cd config
: 1497293792:0;more secrets.yml
: 1497293806:0;more secrets.yml | grep push
: 1497293811:0;vi secrets.yml
: 1497293833:0;head -123 secrets.yml
: 1497293842:0;head -125 secrets.yml
: 1497363860:0;ssh openvpnas@vpn.meyouhealth.com
: 1497374681:0;more ip-172-20-174-62-messages
: 1497374713:0;tail ip-172-20-174-62-nrsysmond.log
: 1497375811:0;ku version
: 1497377319:0;kubectl --namespace prometheus get pods -l app=prometheus -o name | sed 's/^.*\///' | xargs -I{} kubectl --namespace prometheus port-forward {} 9090:9090 
: 1497378893:0;kubectl get pods -o wide  --all-namespaces | grp kublet
: 1497378896:0;kubectl get pods -o wide  --all-namespaces | grep kublet
: 1497378900:0;kubectl get pods -o wide  --all-namespaces | grep kub
: 1497379271:0;kubectl get nodes
: 1497379295:0;kubectl describe node ip-172-24-143-203.ec2.internal
: 1497379383:0;kubectl version --short && helm list|grep workflow
: 1497379629:0;kubectl describe node ip-172-24-167-93.ec2.internal
: 1497379996:0;history | grep journal
: 1497446341:0;history | grep "172.16.1.117"
: 1497446435:0;cd wilder
: 1497446763:0;while true ; do echo "$(date)" ; date >> wilder-mysql-upgrade-aws-desc.log ; aws rds describe-db-instances --db-instance-identifier wilder-production2  >> wilder-mysql-upgrade-aws-desc.log ; sleep 30;  done
: 1497447135:0;cd Documents/wilder
: 1497447141:0;more wilder-mysql-upgrade-aws-desc.log
: 1497447471:0;DEIS_PROFILE=production deis ps:list -a wilder-production
: 1497447551:0;kubectl --namespace wilder-production logs wilder-production-jobs-3309532437-wm9x1
: 1497447560:0;history | grep tail
: 1497447584:0;kubectl --namespace wilder-production logs --tail 100 -f wilder-production-jobs-3309532437-wm9x1 
: 1497447731:0;kubectl get pods -o wide  --all-namespaces | grpe wilder
: 1497447815:0;kubectl --namespace wilder-production logs --tail 100 -f wilder-production-web-2419974203-436mb
: 1497448065:0;kubectl --namespace wilder-production logs --tail 100 -f wilder-production-web-2419974203-4n8b3
: 1497448073:0;grep DBInstanceStatus wilder-mysql-upgrade-aws-desc.log
: 1497448749:0;mv shared-prod-mysql-upgrade-aws-desc.log Documents
: 1497448756:0;more nginx_wd-list
: 1497448762:0;ls -l nginx_wd-list
: 1497448770:0;rm nginx_wd-list
: 1497448787:0;more deis-k8s-history.txt
: 1497448797:0;rm deis-k8s-history.txt
: 1497448806:0;ls -l test
: 1497448810:0;ls -l test/integration
: 1497448814:0;ls -l test/integration/default
: 1497448883:0;ssh -A -L 9999:RDS_HOSTNAME:3306 heidischmidt@BACKGROUND_JOB_SERVER -N
: 1497448917:0;ssh -A -L 9999:wbwire-test-55.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 heidischmidt@10.167.186.59 -N
: 1497452423:0;DEIS_PROFILE=staging deis config:list -a wbt-staging
: 1497452428:0;DEIS_PROFILE=staging deis config:list -a wbt-staging | grep MAINT
: 1497452437:0;history | grep MAIN
: 1497464015:0;DEIS_PROFILE=staging deis ps:list -a dc-staging
: 1497464397:0;history | grep kube | grep tail
: 1497464592:0;kubectl describe services
: 1497464603:0;kubectl describe services --all-namespaces
: 1497464659:0;kubectl describe services --namespace deis
: 1497464677:0;kubectl describe services --namespace deis deis-workflow-manager
: 1497465324:0;more node-issues-ip-172-20-174-62.ec2
: 1497466680:0;grep AWS ip-172-20-174-62-*
: 1497466720:0;ssh admin@ip-172-24-138-162.ec2.internal
: 1497466792:0;egrep -wi --color 'OK|INFO:|ERROR:|CRITICAL' ip-172-20-174-62-*
: 1497489446:0;hostnamectl status
: 1497489454:0;brew list elasticsearch
: 1497489470:0;ls -l /usr/local/Cellar/elasticsearch/5.4.0/libexec/bin/
: 1497490813:0;date +%V
: 1497491147:0;helm get values deis-workflow-berwt
: 1497491656:0;kubectl get nodes -o wide | grep "ip-172-24-163-171.ec2.internal"
: 1497491664:0;ssh admin@ip-172-24-163-171.ec2.internal
: 1497491952:0;kubectl describe node ip-172-24-163-171.ec2.internal  
: 1497538392:0;kubectl --namespace deis logs deis-monitor-telegraf-xlmc2
: 1497538515:0;ssh console deployment-api
: 1497539022:0;shipit deployment logs 5354
: 1497539420:0;kubectl get pods -o wide  --all-namespaces | grep -v Running > new-node-issue-ip-172-24-167-93.ec2.internal
: 1497539460:0;for i in `cat new-node-issue-ip-172-24-167-93.ec2.internal`\
do 
: 1497539563:0;cat new-node-issue-ip-172-24-167-93.ec2.internal | awk '{print 'kubectl --namespace ', $1, ' logs ', $2," 
: 1497539575:0;cat new-node-issue-ip-172-24-167-93.ec2.internal | awk '{print 'kubectl --namespace ', $1, ' logs ', $2,"}' 
: 1497539592:0;cat new-node-issue-ip-172-24-167-93.ec2.internal | awk '{print 'kubectl --namespace ', $1, ' logs ', $2}' 
: 1497539607:0;cat new-node-issue-ip-172-24-167-93.ec2.internal | awk '{print "kubectl --namespace ", $1, " logs ", $2}' 
: 1497539633:0;cat new-node-issue-ip-172-24-167-93.ec2.internal | awk '{print "kubectl --namespace ", $1, " logs ", $2}'  > get-logs-trble-node-issue-ip-172-24-167-93.ec2.internal
: 1497539638:0;vi get-logs-trble-node-issue-ip-172-24-167-93.ec2.internal
: 1497539669:0;for i in `cat get-logs-trble-node-issue-ip-172-24-167-93.ec2.internal`\
do \
$i
: 1497539699:0;for i in `cat get-logs-trble-node-issue-ip-172-24-167-93.ec2.internal`\
do \
$i >> logs\
done
: 1497539719:0;kubectl --namespace  dc-production  logs  dc-production-jobs-1288779390-7ml58
: 1497539736:0;kubectl --namespace  deis  logs  deis-logger-fluentd-j4xg2
: 1497539743:0;cat get-logs-trble-node-issue-ip-172-24-167-93.ec2.internal
: 1497539759:0;more new-node-issue-ip-172-24-167-93.ec2.internalnew-node-issue-ip-172-24-167-93.ec2.internal
: 1497539772:0;grep insight-production-conciergelistener-1889753692-dgh5b get-logs-trble-node-issue-ip-172-24-167-93.ec2.internal
: 1497539778:0;kubectl --namespace  insight-production  logs  insight-production-conciergelistener-1889753692-dgh5b
: 1497539805:0;kubectl --namespace  insight-production  logs  insight-production-conciergelistener-1889753692-dgh5b > no-logs-insight-production-conciergelistener-1889753692-dgh5b
: 1497539822:0;kubectl delete pod insight-production-conciergelistener-1889753692-dgh5b
: 1497539961:0;vi nothing-on-node-ip-172-24-167-93
: 1497540136:0;kubectl get pods -o wide  --all-namespaces | grep fluent
: 1497540296:0;kubectl get logs --help
: 1497540441:0;kubectl --namespace dc-production logs dc-production-web-1420075428-t5256
: 1497540621:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-167-93.ec2.internal | grep Running
: 1497540951:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-167-93.ec2.internal > podlist-ip-172-24-167-93.ec2.internal
: 1497540959:0;date >> podlist-ip-172-24-167-93.ec2.internal
: 1497541351:0;scp admin@ip-172-24-167-93.ec2.internal:/tmp/list list-ip-172-24-167-93.ec2.internal
: 1497541359:0;mv list-ip-172-24-167-93.ec2.internal ../
: 1497541374:0;cat list-ip-172-24-167-93.ec2.internal
: 1497541381:0;cat list-ip-172-24-167-93.ec2.internal | grep log
: 1497541405:0;mkdir logs-ip-172-24-167-93.ec2.internal
: 1497541415:0;cp ../list-ip-172-24-167-93.ec2.internal .
: 1497541418:0;cat list-ip-172-24-167-93.ec2.internal | grep log | grep -v gz
: 1497541429:0;cat list-ip-172-24-167-93.ec2.internal | grep log | grep -v gz | awk '{print $9}'
: 1497541482:0;for i in `cat list-ip-172-24-167-93.ec2.internal | grep log | grep -v gz | awk '{print $9}'`\
do \
scp list-ip-172-24-167-93.ec2.internalscp admin@ip-172-24-167-93.ec2.internal:/var/log/$i $i\
done
: 1497541535:0;for i in `cat list-ip-172-24-167-93.ec2.internal | grep log | grep -v gz | awk '{print $9}'`\
do \
scp admin@ip-172-24-167-93.ec2.internal:/var/log/$i $i\
done
: 1497541613:0;cd Documents/logs-ip-172-24-167-93.ec2.internal
: 1497541632:0;scp admin@ip-172-24-167-93.ec2.internal:/tmp/journal* 
: 1497541641:0;scp admin@ip-172-24-167-93.ec2.internal:/tmp/journalctl-u-kubelet.log 
: 1497541645:0;scp admin@ip-172-24-167-93.ec2.internal:/tmp/journalctl-u-kubelet.log .
: 1497541792:0;scp admin@ip-172-24-167-93.ec2.internal:/tmp/*docker* .
: 1497541805:0;scp admin@ip-172-24-167-93.ec2.internal:/tmp/docker-ps-auxwww.log .
: 1497541818:0;scp admin@ip-172-24-167-93.ec2.internal:/tmp/sudo-docker-ps.log .
: 1497541843:0;kubectl get nodes -o wide >> nodes-at-this-time
: 1497541871:0;kubectl version && helm list|grep workflow >> versions-at-this-time
: 1497541876:0;more versions-at-this-time
: 1497541884:0;vi versions-at-this-time
: 1497541931:0;scp admin@ip-172-24-167-93.ec2.internal:/var/log/messages .
: 1497542232:0;kubectl patch node ip-172-24-167-93.ec2.internal  -p '{"spec":{"unschedulable":true}}'
: 1497542269:0;ssh admin@ip-172-24-167-93.ec2.internal
: 1497542477:0;kubectl delete pod dc-production-jobs-1288779390-7ml58
: 1497542487:0;kubectl delete pod dc-production-jobs-1288779390-b5483
: 1497542496:0;kubectl delete pod dc-production-jobs-1288779390-d6qrf
: 1497542550:0;kubectl drain ip-172-24-167-93.ec2.internal
: 1500058256:0;    -o jsonpath='{range .items[?(.spec.nodeName=="ip-172-24-167-93.ec2.internal")]}{.metadata.name} {end}'
: 1500058256:0;    -o jsonpath='{range .items[?(.spec.nodeName=="ip-172-24-167-93.ec2.internal")]}{.metadata.namespace} {.metadata.name} {end}'
: 1497542722:0;kubectl get pods --all-namespaces \\
: 1500058256:0;   -o jsonpath='{range .items[?(.spec.nodeName=="gke-cluster-1-b4c97d4d-node-psh2")]}{@.metadata.namespace} {.metadata.name} {end}' | \
: 1497542751:0;kubectl get pods --all-namespaces -o jsonpath='{range .items[?(.spec.nodeName=="ip-172-24-167-93.ec2.internal")]}{@.metadata.namespace} {.metadata.name} {end}' | xargs -n 2
: 1497542782:0;kubectl get pods --all-namespaces -o jsonpath='{range .items[?(.spec.nodeName=="ip-172-24-167-93.ec2.internal")]}{@.metadata.namespace} {.metadata.name} {end}' | xargs -n 2 | xargs -I % sh -c "kubectl delete pods --namespace=%"
: 1497543143:0;kubectl uncordon ip-172-24-167-93.ec2.internal
: 1497543277:0;kubectl delete pod walkadoo-production-conciergelistener-1436509537-h97z0 --namespace walkadoo-production
: 1497543330:0;kubectl delete pod --namespace walkadoo-production walkadoo-production-conciergelistener-1436509537-h97z0
: 1497543345:0;kubectl delete pod --namespace walkadoo-production walkadoo-production-gcm-1993493993-ct3lc
: 1497543437:0;kubectl get pods -o wide  --all-namespaces | grep Unknown
: 1497543585:0;kubectl describe node ip-172-24-205-22.ec2.internal
: 1497543607:0;kubectl describe node ip-172-24-205-22.ec2.internal > node-issue-ip-172-24-205-22.ec2.internal
: 1497543616:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-167-93.ec2.internal
: 1497543623:0;kubectl get nodes -o wide odes
: 1497543710:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-205-22.ec2.internal
: 1497543726:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-205-22.ec2.internal > pods-on-ip-172-24-205-22.ec2.internal
: 1497543745:0;kubectl get nodes -o wide | grep ip-172-24-205-22.ec2.internal
: 1497543781:0;kubectl get pods -o wide  --all-namespaces | egrep -i "walkadoo|dc"
: 1497544059:0;kubectl delete pod --namespace walkadoo-production walkadoo-production-gcm-1993493993-1q65j
: 1497544071:0;kubectl delete pod --namespace walkadoo-production walkadoo-production-wbidaccountchangelistener-1279292041-92km5
: 1497544178:0;kubectl delete pod --namespace walkadoo-production walkadoo-production-web-3062151915-rs068
: 1497544623:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-138-162.ec2.internal 
: 1497544642:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-138-162.ec2.internal  > pods-on-ip-172-24-138-162.ec2.internal 
: 1497544661:0;kubectl describe node ip-172-24-138-162.ec2.internal > node-desc-ip-172-24-138-162.ec2.internal 
: 1497544666:0;moe node-desc-ip-172-24-138-162.ec2.internal
: 1497544670:0;more node-desc-ip-172-24-138-162.ec2.internal
: 1497544796:0;kubectl get nodes -o wide > nodes-after-3-terminated-06-15-17-1239PM
: 1497548664:0;aws s3 ls | grep hes
: 1497548673:0;aws s3 ls | grep heidi
: 1497548678:0;aws s3 ls 
: 1497548777:0;aws s3 ls | grep node
: 1497548907:0;aws s3 cp * k8s-node-logs/logs-ip-172-24-167-93.ec2.internal/
: 1497548919:0;aws s3 cp help
: 1497548970:0;aws s3 cp s3://k8s-node-logs/logs-ip-172-24-167-93.ec2.internal/ *
: 1497548981:0;aws s3 cp s3://k8s-node-logs/logs-ip-172-24-167-93.ec2.internal/ alternatives.log
: 1497549059:0;more new-node-issue-ip-172-24-167-93.ec2.internal
: 1497549079:0;aws s3 cp new-node-issue-ip-172-24-167-93.ec2.internal s3://k8s-node-logs/logs-ip-172-24-167-93.ec2.internal/ 
: 1497549108:0;aws s3 cp node-issue-ip-172-24-205-22.ec2.internal s3://k8s-node-logs/logs-ip-172-24-167-93.ec2.internal/ 
: 1497549171:0;aws s3 cp node-issue-ip-172-24-205-22.ec2.internal s3://k8s-node-logs/ip-172-24-205-22.ec2.internal 
: 1497549189:0;aws s3 cp *ip-172-24-205-22.ec2.internal* s3://k8s-node-logs/ip-172-24-205-22.ec2.internal 
: 1497549216:0;aws s3 cp pods-on-ip-172-24-205-22.ec2.internal s3://k8s-node-logs/ip-172-24-205-22.ec2.internal/
: 1497549272:0;aws s3 ls //k8s-node-logs/ip-172-24-205-22.ec2.internal/
: 1497549292:0;ls -l*205*22
: 1497549295:0;ls -l *205*22
: 1497549314:0;aws s3 ls s3://k8s-node-logs/ip-172-24-205-22.ec2.internal/
: 1497549325:0;aws s3 cp node-issue-ip-172-24-205-22.ec2.internal  s3://k8s-node-logs/ip-172-24-205-22.ec2.internal/
: 1497549464:0;ls -l *ip*162*
: 1497549522:0;aws s3 cp node-desc-ip-172-24-138-162.ec2.internal s3://k8s-node-logs/logs-ip-172-24-138-162.ec2.internal/
: 1497549540:0;aws s3 cp pods-on-ip-172-24-138-162.ec2.internal s3://k8s-node-logs/logs-ip-172-24-138-162.ec2.internal/
: 1497549579:0;aws s3 ls s3://k8s-node-logs/logs-ip-172-24-138-162.ec2.internal/
: 1497549719:0;aws s3 cp ip-172-20-174-62-containers.log.pos s3://k8s-node-logs/logs-ip-172-20-174-62.ec2.internal/
: 1497549736:0;aws s3 cp ip-172-20-174-62-daemon.log s3://k8s-node-logs/logs-ip-172-20-174-62.ec2.internal/
: 1497549749:0;aws s3 cp ip-172-20-174-62-daemon.log.1 s3://k8s-node-logs/logs-ip-172-20-174-62.ec2.internal/
: 1497550529:0;aws s3 cp ip-172-20-174-62-kern.log s3://k8s-node-logs/logs-ip-172-20-174-62.ec2.internal/
: 1497550552:0;aws s3 cp ip-172-20-174-62-kube-proxy.log s3://k8s-node-logs/logs-ip-172-20-174-62.ec2.internal/
: 1497550566:0;aws s3 cp ip-172-20-174-62-messages s3://k8s-node-logs/logs-ip-172-20-174-62.ec2.internal/
: 1497550586:0;aws s3 cp ip-172-20-174-62-nrsysmond.log s3://k8s-node-logs/logs-ip-172-20-174-62.ec2.internal/
: 1497550624:0;aws s3 cp ip-172-20-174-62-syslog.log s3://k8s-node-logs/logs-ip-172-20-174-62.ec2.internal/
: 1497550824:0;aws s3 cp node-issues-ip-172-20-174-62.ec2 s3://k8s-node-logs/logs-ip-172-20-174-62.ec2.internal/
: 1497551328:0;cd logs-ip-172-24-167-93.ec2.internal
: 1497551331:0;more list-ip-172-24-167-93.ec2.internal
: 1497561992:0;shipit deployment logs 5357
: 1497562081:0;ssh ubuntu@54.81.188.182
: 1497562273:0;shipit create deployment help
: 1497562285:0;history | grep shipit | grep create
: 1497562310:0;shipit deployment create reporting-production 759748e
: 1497598079:0;kubectl get nodes -o wide | grep ip-172-24-202-180.ec2.internal
: 1497598113:0;kubectl describe node ip-172-24-202-180.ec2.internal > Documents/node-desc-ip-172-24-202-180.ec2.internal
: 1497598129:0;kubectl get nodes -o wide | grep ip-172-24-202-180.ec2.internal > Documents/node-list-w-ip-172-24-202-180.ec2.internal
: 1497598155:0;kubectl get pods -o wide  --all-namespaces | grep -v Running > Documents/pods-on-ip-172-24-202-180.ec2.internal
: 1497598244:0;ssh admin@ip-172-24-202-180.ec2.internal
: 1497598375:0;vi docker-procs-ps-auxww-on-ip-172-24-202-180.ec2.internal
: 1497598475:0;ls -ltra | tail -10
: 1497598487:0;kubectl get nodes -o wide | grep ip-172-24-202-180
: 1497598539:0;more pods-on-ip-172-24-202-180.ec2.internal
: 1497598700:0;mkdir logs-ip-172-24-202-180.ec2.internal
: 1497598728:0;mv docker*ip-172-24-202-180.ec2.internal logs-ip-172-24-202-180.ec2.internal
: 1497598738:0;mv nodes*ip-172-24-202-180.ec2.internal logs-ip-172-24-202-180.ec2.internal
: 1497598754:0;mv node*ip-172-24-202-180.ec2.internal logs-ip-172-24-202-180.ec2.internal
: 1497598759:0;ls *ip-172-24-202-180.ec2.internal
: 1497598771:0;mv pods-on-ip-172-24-202-180.ec2.internal logs-ip-172-24-202-180.ec2.internal
: 1497598773:0;cd ip-172-24-202-180.ec2.internal
: 1497598779:0;ls -lrt
: 1497598803:0;mkdir logs-ip-172-20-174-62.ec2.internal 
: 1497598817:0;mv ip-172-20-174-62-* logs-ip-172-20-174-62.ec2.internal
: 1497598837:0;ls logs-ip-172-24-167-93.ec2.internal
: 1497598921:0;more kube-get-pods-all-namespaces-show-labels-during-reboot.log
: 1497598969:0;mkdir logs
: 1497598974:0;mv *.log logs/
: 1497598988:0;more nodes-after-3-terminated-06-15-17-1239PM
: 1497599316:0;mkdir k8s-node-logs-other
: 1497599338:0;mv node-issues-ip-172-20-174-62.ec2 node-issues-ip-172-20-174-62.ec2.internal 
: 1497599343:0;ls  *internal
: 1497599380:0;mv list-ip-172-24-167-93.ec2.internal logs-ip-172-24-167-93.ec2.internal
: 1497599395:0;ls *162*internal
: 1497599409:0;mv *162*internal k8s-node-logs-other
: 1497599431:0;mv new-node-issue-ip-172-24-167-93.ec2.internal logs-ip-172-24-167-93.ec2.internal
: 1497599458:0;mv node-issue-ip-172-24-205-22.ec2.internal k8s-node-logs-other
: 1497599465:0;mv node-issues-ip-172-20-174-62.ec2.internal k8s-node-logs-other
: 1497599476:0;ls *22*internal
: 1497599482:0;mv pods-on-ip-172-24-205-22.ec2.internal k8s-node-logs-other
: 1497599513:0;ls -ltrah 
: 1497599527:0;mv nodes-after-3-terminated-06-15-17-1239PM k8s-node-logs-other
: 1497639031:0;kubectl get pods -o wide  --all-namespaces | grep proxy
: 1497639330:0;kubectl get pods -o wide  --all-namespaces | grep contro
: 1497639542:0;kubectl get pods -o wide  --all-namespaces | grep job
: 1497639604:0;cd k8s-node-logs-other
: 1497639617:0;more node-issue-ip-172-24-205-22.ec2.internal
: 1497639638:0;more pods-on-ip-172-24-205-22.ec2.internal
: 1497639646:0;more pods-on-ip-172-24-138-162.ec2.internal
: 1497639667:0;ls -l logs-ip-172-24-202-180.ec2.internal
: 1497639674:0;more logs-ip-172-24-202-180.ec2.internal/pods-on-ip-172-24-202-180.ec2.internal
: 1497639815:0;history | grep label
: 1497639874:0;kubectl get pods -o wide  --all-namespaces --show-labels | grep dc
: 1497640102:0;kubectl get rc --all-namespaces
: 1497640107:0;kubectl get rc 
: 1497640423:0;kubectl get deployment wilder-production-jobs --namespace wilder-production -o yaml
: 1497684533:0;kubectl describe node ip-172-24-159-136.ec2.internal
: 1497684606:0;ssh admin@ip-172-24-159-136.ec2.internal
: 1497684723:0;kubectl get pods -o wide  --all-namespacess | grep ip-172-24-159-136.ec2.internal
: 1497684953:0;kubectl --namespace concierge-production logs concierge-production-jobs-2351575752-26sfx
: 1497684996:0;history | grep timestamp | tail -10
: 1497685014:0;kubectl --namespace concierge-production logs concierge-production-jobs-2351575752-26sfx --timestamp
: 1497685068:0;kubectl --namespace concierge-production logs concierge-production-jobs-2351575752-26sfx --timestamps
: 1497685148:0;kubectl --namespace concierge-production logs concierge-production-web-25468398-s0p1c --timestamps
: 1497685338:0;kubectl get nodes -o wide | grep ip-172-24-140-239.ec2.internal
: 1497685363:0;kubectl describe ip-172-24-140-239.ec2.internal
: 1497685759:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-163-171
: 1497686204:0;kubectl get pods -o wide  --all-namespaces | grep super
: 1497686556:0;kubectl get events --all-namespaces | grep CGF
: 1497686642:0;kubectl get pods -o wide  --all-namespaces | grep -v 1d
: 1497686952:0;kubectl get events | head -2
: 1497686957:0;kubectl get events | grep ContainerGCFailed
: 1497687047:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-163-171.ec2.internal
: 1497687349:0;kubectl get pods -o wide  --all-namespaces | grep 100.114.181.207
: 1497687381:0;kubectl get nodes -o wide | grep ip-172-24-163-171
: 1497687662:0;kubectl get pods -o wide  --all-namespaces | grep walkadoo-production-wbidaccountchangelistener
: 1497687680:0;kubectl get nodes -o wide | grep ip-172-24-194-49.ec2.internal
: 1497687736:0;kubectl get nodes -o wide | grep ip-172-24-194-49.ec2.interna
: 1497687746:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-159-136.ec2.internal
: 1497687782:0;kubectl get events | grep GCFailed
: 1497687834:0;kubectl get nodes -o wide | grep ip-172-24-210-103.ec2.internal
: 1497709602:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-210-103.ec2.internal
: 1497709915:0;kubectl get nodes -o wide | grep ip-172-24-163-113.ec2.internal
: 1497710560:0;kubectl get nodes -o wide | grep ip-172-24-220-58.ec2.internal
: 1497712982:0;ssh admin@ip-172-24-136-4.ec2.internal
: 1497713085:0;history | grep 54.81.188.182
: 1497749363:0;kubectl get pods -o wide  --all-namespaces | grep quitnet-production-rabbitmq
: 1497749510:0;ssh admin@ip-172-24-185-180.ec2.internal
: 1497749744:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-185-180.ec2.internal
: 1497750053:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-205-53.ec2.internal
: 1497750142:0;ssh admin@ip-172-24-205-53.ec2.internal
: 1497750483:0;kubectl get nodes -o wide | grep ip-172-24-219-111.ec2.internal
: 1497750504:0;kubectl get nodes -o wide | grep ip-172-24-178-85.ec2.internal
: 1497750633:0;kubectl get events | grep Failed
: 1497803044:0;kubectl get events | grep ip-172-24-158-97
: 1497803057:0;kubectl get nodes -o wide | grep ip-172-24-158-97
: 1497803110:0;ssh admin@ip-172-24-158-97.ec2.internal
: 1497803260:0;kubectl describe node ip-172-24-158-97
: 1497803777:0;mkdir logs-ip-172-24-158-97.ec2.internal
: 1497803811:0;scp admin@ip-172-24-158-97.internal:/var/log/messages .
: 1497803825:0;scp admin@ip-172-24-158-97.ec2.internal:/var/log/messages .
: 1497803833:0;scp admin@ip-172-24-158-97.ec2.internal:/var/log/syslog .
: 1497803849:0;scp admin@ip-172-24-158-97.ec2.internal:/var/log/kern.log .
: 1497803856:0;scp admin@ip-172-24-158-97.ec2.internal:/var/log/*.log .
: 1497803867:0;scp admin@ip-172-24-158-97.ec2.internal:/var/log/daemon.log .
: 1497803884:0;scp admin@ip-172-24-158-97.ec2.internal:/tmp/journalctl-kublet-ip-172-24-158-97 .
: 1497804015:0;scp admin@ip-172-24-158-97.ec2.internal:/tmp/processes-ip-172-24-158-97 .
: 1497804031:0;scp admin@ip-172-24-158-97.ec2.internal:/tmp/free-mem-ip-172-24-158-97 .
: 1497804044:0;scp admin@ip-172-24-158-97.ec2.internal:/tmp/docker-ps-ip-172-24-158-97 .
: 1497804066:0;scp admin@ip-172-24-158-97.ec2.internal:/tmp/docker-image-info-ip-172-24-158-97 .
: 1497804089:0;scp admin@ip-172-24-158-97.ec2.internal:/var/log/containers.log.pos .
: 1497804104:0;scp admin@ip-172-24-158-97.ec2.internal:/var/log/kube-proxy.log .
: 1497804125:0;scp admin@ip-172-24-158-97.ec2.internal:/var/log/nrsysmond.log .
: 1497804199:0;scp admin@ip-172-24-158-97.ec2.internal:/var/log/kern.log.1 .
: 1497804229:0;scp admin@ip-172-24-158-97.ec2.internal:/var/log/dpkg.log.1 .
: 1497804337:0;vi uptime-info
: 1497804601:0;kubectl delete pod hello200-production-wbidaccountchangelistener-876057480-9l3m8 --namespace hello200-production
: 1497804644:0;kubectl delete pod --namespace dc-production dc-production-jobs-1097151118-cmrx3
: 1497804658:0;kubectl delete pod --namespace dc-production dc-production-jobs-1097151118-cmrp8
: 1497804673:0;kubectl delete pod --namespace dc-production dc-production-jobs-1097151118-6324r
: 1497804679:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-158-97
: 1497804811:0;more free-mem-ip-172-24-158-97
: 1497804823:0;more after-moving-pods-memory
: 1497804940:0;vi after
: 1497804944:0;vi after-moving-pods-memory
: 1497885711:0;ssh heidischmidt@54.81.188.182
: 1497892334:0;DEIS_PROFILE=production deis config-list -a concierge-production | grep -i wbt
: 1497892361:0;DEIS_PROFILE=production deis config:list -a concierge-production | grep -i wbt
: 1497892373:0;DEIS_PROFILE=production deis config:list -a dcmobile-production | grep -i wbt
: 1497892383:0;DEIS_PROFILE=production deis config:list -a dc-production | grep -i wbt
: 1497892423:0;DEIS_PROFILE=production deis config:list -a elasticsearch-production | grep -i wbt
: 1497892434:0;DEIS_PROFILE=production deis config:list -a hello200-production| grep -i wbt
: 1497892448:0;DEIS_PROFILE=production deis config:list -a hurby-server-production | grep -i wbt
: 1497892458:0;DEIS_PROFILE=production deis config:list -a insight-production | grep -i wbt
: 1497892469:0;DEIS_PROFILE=production deis config:list -a iris-production | grep -i wbt
: 1497892481:0;DEIS_PROFILE=production deis config:list -a quitnet-production | grep -i wbt
: 1497892498:0;DEIS_PROFILE=production deis config:list -a superset | grep -i wbt
: 1497892509:0;DEIS_PROFILE=production deis config:list -a walkadoo-production | grep -i wbt
: 1497892569:0;DEIS_PROFILE=production deis config:get PASSIVE_DATABASE_HOST -a wbt-production 
: 1497892584:0;DEIS_PROFILE=production deis config:get -a wbt-production PASSIVE_DATABASE_HOST
: 1497892588:0;DEIS_PROFILE=production deis config:get -a wbt-production 
: 1497892598:0;DEIS_PROFILE=production deis config:list -a wbt-production PASSIVE_DATABASE_HOST
: 1497892605:0;DEIS_PROFILE=production deis config:list -a wbt-production 
: 1497892643:0;DEIS_PROFILE=production deis config:list -a wilder-production | grep wbt
: 1497892647:0;DEIS_PROFILE=production deis config:list -a wilder-production | grep -i wbt
: 1497892797:0;DEIS_PROFILE=production deis config:set -a wbt-production PASSIVE_DATABASE_HOST=wbt-production2-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com
: 1497892846:0;DEIS_PROFILE=production deis config:list -a wbt-production | grep -i wbt
: 1497892945:0;kubectl get pods -o wide  --all-namespaces | grep wbt 
: 1497893672:0;DEIS_PROFILE=production deis config:set -a wbt-production PASSIVE_DATABASE_HOST=wbt-production-read2.cduvhvvpw6oo.us-east-1.rds.amazonaws.com
: 1497898763:0;kubectl get deployment --namespace kube-system kube-dns-autoscaler
: 1497898768:0;kubectl get deployment --namespace kube-system kube-dns-autoscaler -o yaml
: 1497898784:0;kubectl get deployment --namespace kube-system 
: 1497898879:0;kubectl get deployment --namespace kube-system kubernetes-dashboard -o yaml
: 1497899018:0;ssh ip-172-24-140-239.ec2.internal
: 1497899373:0;kubectl get deployments --namespace dc-production dc-production-web -o yaml
: 1497899476:0;kubectl get config-map --namespace dc-production dc-production-web -o yaml
: 1497899491:0;history | grep config-map
: 1497899506:0;kubectl get config-maps --namespace prometheus
: 1497899549:0;history | grep kubectl | grep config
: 1497899569:0;kubectl get configmap --allnamespaces
: 1497899574:0;kubectl get configmap 
: 1497899587:0;kubectl get configmaps
: 1497899602:0;kubectl get configmap --namespace prometheus
: 1497899609:0;kubectl get configmap --namespace dc-production
: 1497966704:0;kubectl get events --all-namespaces | grep Failed
: 1497966790:0;kubectl config list-contexts
: 1497966792:0;kubectl config list-context
: 1497966797:0;kubectl config -h
: 1497966919:0;ssh admin@ip-172-20-172-17.ec2.internal
: 1497967069:0;kubectl get pods -o wide  --all-namespaces | grep wbt-staging
: 1497967249:0;kubectl describe nodes ip-172-20-143-165.ec2.internal
: 1497967267:0;kubectl describe nodes ip-172-20-209-89.ec2.internal
: 1497967271:0;kubectl get pods -o wide  --all-namespaces | grep iris
: 1497967605:0;while true ; do echo "$(date)" ; date >> wbt-mysql-upgrade-aws-desc.log ; aws rds describe-db-instances --db-instance-identifier wbt-production2  >> wbt-mysql-upgrade-aws-desc.log ; sleep 30;  done
: 1497967662:0;DEIS_PROFILE=production deis ps:list -a wbt-production
: 1497968045:0;tail -100 wbt-mysql-upgrade-aws-desc.log
: 1497968053:0;kubectl get pods -o wide --all-namespaces --show-labels
: 1497968059:0;kubectl get pods -o wide --all-namespaces --show-labels | grep wbt
: 1497968075:0;more Documents/wbt-mysql-upgrade-aws-desc.log
: 1497968082:0;tail -100 Documents/wbt-mysql-upgrade-aws-desc.log
: 1497969504:0;kubectl get pods -o wide  --all-namespaces | grep 172.24.185.136
: 1497969525:0;kubectl get pods -o wide  --all-namespaces | grep 172.24.163.113
: 1497969559:0;kubectl get pods -o wide  --all-namespaces | grep 172.24.194.49
: 1497970603:0;kops list 
: 1497970610:0;kops get cluster
: 1497970615:0;kops get cluster --help
: 1497971175:0;kubectl describe node ip-172-24-140-239.ec2.internal
: 1497971206:0;kops get cluster -o yaml 
: 1497971236:0;kops get cluster -o yaml | grep kubelet
: 1497971244:0;kops get cluster -o yaml | grep evict
: 1497971442:0;kops describe --help
: 1497971479:0;kops toolbox --help
: 1497971506:0;kops get --help
: 1497971524:0;kops get federations
: 1497971547:0;kops get secrets
: 1497971681:0;kubectl get deployments --namespace dc-production dc-production-jobs -o yaml
: 1497972063:0;kops get instancegroups -o wide
: 1497972069:0;kops get instancegroups -o yaml
: 1497972116:0;kubectl describe pod wbt-production-web-3894890722-k1sm8 --namespace wbt-production
: 1497974317:0;which fping
: 1497974328:0;ssh admin@ip-172-24-140-239.ec2.internal
: 1497974462:0;kubectl drain --help
: 1497979497:0;cd logs-ip-172-24-158-97.ec2.internal
: 1498485089:0;DEIS_PROFILE=production deis ps:list -a wellbeingid-production
: 1498485099:0;shipit env list 
: 1498485406:0;shipit deployment logs 5361
: 1498485504:0;shipit deployment create wellbeingid-production e525c0f8
: 1498485698:0;ssh -t heidischmidt@54.224.55.76
: 1498485812:0;ssh -t heidischmidt@54.234.118.19
: 1498485848:0;ssh -t heidischmidt@54.146.72.167
: 1498488858:0;while true ; do echo "$(date)" ; date >> walkadoo-production2-read1-mysql-upgrade-aws-desc.log ; aws rds describe-db-instances --db-instance-identifier walkadoo-production2-read1  >> walkadoo-production2-read1-mysql-upgrade-aws-desc.log ; sleep 30;  done
: 1498489511:0;kubectl get pods -o wide  --all-namespaces | grep walkadoo | grep Running
: 1498489569:0;kubectl describe node ip-172-24-221-193.ec2.internal
: 1498489692:0;kubectl delete pod walkadoo-production-twilio-826757997-9nljr --namespace walkadoo-production
: 1498489719:0;kubectl delete pod walkadoo-production-web-3062151915-1cjq9 --namespace walkadoo-production
: 1498489723:0;kubectl get pods -o wide  --all-namespaces | grep walkadoo | grep -v Running
: 1498489741:0;kubectl get logs walkadoo-production-web-3062151915-kkqj5 --namespace walkadoo-production
: 1498489763:0;kubectl --namespace walkadoo-production logs walkadoo-production-web-3062151915-kkqj5
: 1498489778:0;kubectl delete pod walkadoo-production-web-3062151915-kkqj5 --namespace walkadoo-production
: 1498489827:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-221-193
: 1498489880:0;kubectl delete dc-production-jobs-2245604360-1s961 --namespace dc-production
: 1498489892:0;kubectl delete pod dc-production-jobs-2245604360-1s961 --namespace dc-production
: 1498489907:0;kubectl delete pod dc-production-jobs-2245604360-1sgkp --namespace dc-production
: 1498489922:0;kubectl delete pod dc-production-jobs-2245604360-hmtq5 --namespace dc-production
: 1498489939:0;kubectl delete pod dc-production-web-1882300206-0sqrl --namespace dc-production
: 1498489944:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-221-193\\
: 1500058256:0;
: 1498489968:0;kubectl delete pod dc-production-web-1882300206-ngqcq --namespace dc-production
: 1498489986:0;kubectl delete pod dc-production-web-1882300206-sfptv --namespace dc-production
: 1498490002:0;kubectl delete pod dc-production-web-1882300206-xzkzz --namespace dc-production
: 1498490026:0;kubectl delete pod deis-logger-fluentd-k217x --namespace deis
: 1498490047:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-221-193\

: 1498490070:0;kubectl delete pod dc-production-jobs-2245604360-jq31d --namespace dc-production
: 1498490082:0;kubectl get pods -o wide  --all-namespaces | grep "Evicted"\

: 1498490107:0;ssh admin@ip-172-24-221-193.ec2.internal
: 1498490557:0;kubectl get pods -o wide  --all-namespaces | grep 172.24.172.178
: 1498490575:0;kubectl get nodes -o wide | grep 172.24.172.178
: 1498490617:0;kubectl get nodes -o wide | grep 172.24.156.9
: 1498490664:0;kubectl get nodes -o wide | grep 172.24.222.212
: 1498490690:0;kubectl get nodes -o wide | grep 10.145.46.88
: 1498490707:0;kubectl get nodes -o wide | grep 172.24.182.5
: 1498490724:0;kubectl get nodes -o wide | grep 172.16.1.117
: 1498497049:0;kubectl get nodes -o wide | grep ip-172-24-221-193
: 1498497077:0;kubectl get pods -o wide  --all-namespaces | grep wellbeing\

: 1498497081:0;kubectl get pods -o wide  --all-namespaces | grep well\

: 1498498897:0;brew update 
: 1498500047:0;man kubectl-get
: 1498500080:0;kubectl get all --all-namespaces
: 1498500119:0;kubectl get all --all-namespaces | more
: 1498500201:0;kubectl get all --namespace wbt-production
: 1498500230:0;kubectl get pods -o wide  --all-namespaces | grep wbt-production 
: 1498500316:0;kubectl get rs --all
: 1498500324:0;kubectl get rs 
: 1498500765:0;kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="ExternalIP")].address}'
: 1498500790:0;kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="Name")].address}'
: 1498500798:0;kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="NAME")].address}'
: 1498501361:0;kubectl get nodes -o jsonpath='{range .items[*]}{@.metadata.name}:{range @.status.conditions[*]}{@.type}={@.status};{end}{end}' | tr ';' "\n"
: 1498501516:0;kubectl get nodes -o jsonpath
: 1498501519:0;kubectl get nodes -o json
: 1498501578:0;kubectl get nodes -o jsonpath='{range .items[*]}{@.metadata.name}:{range @.status.conditions[*]}{@.type}={@.status};{end}{end}' > Documents/kubectl-node-manip-for-scriptlets
: 1498501599:0;kubectl get nodes -o json >> Documents/kubectl-node-manip-for-scriptlets
: 1498501648:0;vi Documents/kubectl-node-manip-for-scriptlets
: 1498506681:0;aws rds describe-db-instances --db-instance-identifier dc-production-read1
: 1498506725:0;while true ; do echo "$(date)" ; date >> dc-production-read1-upgrade-aws-desc.log ; aws rds describe-db-instances --db-instance-identifier dc-production-read1  >> dc-production-read1-mysql-upgrade-aws-desc.log ; sleep 30;  done
: 1498569411:0;tail -100 dc-production-read1-mysql-upgrade-aws-desc.log
: 1498569653:0;tail -200 dc-production-read1-mysql-upgrade-aws-desc.log
: 1498573476:0;while true ; do echo "$(date)" ; date >> test-dc-upgrade-5635-multi-az-upgrade-aws-desc.log ; aws rds describe-db-instances --db-instance-identifier test-dc-upgrade-5635-multi-az  >> test-dc-upgrade-5635-multi-az-upgrade-aws-desc.log ; sleep 30;  done
: 1498584159:0;shipit ssh reporting-production
: 1498588958:0;shipit deployment logs 5362
: 1498588968:0;shipit deployment logs 5362 | more
: 1498588991:0;shipit deployment logs 5362 | less
: 1498589037:0;shipit env config_list wellbeingid-production | grep -i DB
: 1498589050:0;shipit env config_list wellbeingid-production | grep -i DATA
: 1498589455:0;rm test-dc-upgrade-5635-multi-az-upgrade-aws-desc.log
: 1498589465:0;history | grep psql 
: 1498589474:0;clear
: 1498590399:0;grep conn *
: 1498591784:0;grep -i analyze *
: 1498591793:0;more analyze-table.log
: 1498591840:0;more analyze-table.sql
: 1498591867:0;cd ../wbid
: 1498591870:0;ls -ltra *.sql 
: 1498591881:0;more wbid-src-schema.sql
: 1498591895:0;grep "CREATE TABLE" wbid-src-schema.sql
: 1498591899:0;grep "CREATE TABLE" wbid-src-schema.sql | wc -l 
: 1498592054:0;cd -
: 1498592073:0;more pg-stats-tables.sql
: 1498592152:0;more pg-stats-tables-all.sql
: 1498663259:0;which jq
: 1498663560:0;more Documents/kubectl-node-manip-for-scriptlets
: 1498663578:0;history | grep jsonpath
: 1498663588:0;kubectl get nodes -o jsonpath='{range .items[*]}{@.metadata.name}:{range @.status.conditions[*]}{@.type}={@.status};{end}{end}'
: 1498663660:0;kubectl get all --all-namespaces > Documents/kubectl-get-all-all-namespaces.json
: 1498663687:0;kubectl get all --all-namespaces -o json
: 1498663715:0;kubectl get all --all-namespaces -o json > Documents/kubectl-get-all-all-namespaces.json
: 1498663784:0;jq '.[0]' Documents/kubectl-get-all-all-namespaces.json
: 1498663814:0;jq '.[]' Documents/kubectl-get-all-all-namespaces.json
: 1498663847:0;jq '.[kind]' Documents/kubectl-get-all-all-namespaces.json
: 1498663856:0;more Documents/kubectl-get-all-all-namespaces.json
: 1498663870:0;jq '.[items]' Documents/kubectl-get-all-all-namespaces.json
: 1498663907:0;man jq
: 1498664036:0;jq '.name' Documents/kubectl-get-all-all-namespaces.json
: 1498664067:0;jq '.items' Documents/kubectl-get-all-all-namespaces.json | less
: 1498664099:0;jq '.items.name' Documents/kubectl-get-all-all-namespaces.json | less
: 1498664127:0;jq '.items.kind' Documents/kubectl-get-all-all-namespaces.json 
: 1498664173:0;jq '.name.name' Documents/kubectl-get-all-all-namespaces.json 
: 1498664194:0;jq '.items[0]' Documents/kubectl-get-all-all-namespaces.json | less
: 1498664865:0;jq .items[0] Documents/kubectl-get-all-all-namespaces.json | less
: 1498664879:0;jq .items Documents/kubectl-get-all-all-namespaces.json | less
: 1498664919:0;vi Documents/kubectl-get-all-all-namespaces.json
: 1498665126:0;jq '.' Documents/kubectl-get-all-all-namespaces.json
: 1498665142:0;jq '.items' Documents/kubectl-get-all-all-namespaces.json
: 1498665253:0;jq '.items.kind' Documents/kubectl-get-all-all-namespaces.json
: 1498665264:0;jq -s '.items.kind' Documents/kubectl-get-all-all-namespaces.json
: 1498665362:0;jq -s '.items[].kind[]' Documents/kubectl-get-all-all-namespaces.json
: 1498665373:0;jq  '.items[].kind[]' Documents/kubectl-get-all-all-namespaces.json
: 1498665392:0;jq  '.items[].kind[].Pod[]' Documents/kubectl-get-all-all-namespaces.json
: 1498669953:0;aws elasticcache
: 1498669965:0;aws elasticache
: 1498669991:0;aws elasticache describe-cache-clusters
: 1498670040:0;aws elasticache describe-cache-clusters > Documents/elasticache-clusters-all-062817.txt
: 1498670343:0;aws elasticache modify-cache-cluster help
: 1498671782:0;cat Documents/elasticache-clusters-all-062817.txt | grep  EngineVersion | sort | uniq -c
: 1498671843:0;cat Documents/elasticache-clusters-all-062817.txt | grep NumCacheNodes
: 1498671908:0;vi Documents/elasticache-clusters-all-062817.txt
: 1498745346:0;kubectl get secrets
: 1498745370:0;kubectl get secrets --all-namespaces
: 1498745430:0;kops help
: 1498745444:0;history | grpe kops
: 1498747122:0;while true ; do echo "$(date)" ; date >> wd-prod2-upgrade-aws-desc.log ; aws rds describe-db-instances --db-instance-identifier walkadoo-production2  >> wd-prod2-upgrade-aws-desc.log ; sleep 30;  done
: 1498747579:0;more wd-prod2-upgrade-aws-desc.log
: 1498748190:0;tail -100 wd-prod2-upgrade-aws-desc.log
: 1498748193:0;DEIS_PROFILE=production deis ps:list -a walkadoo-production
: 1498748302:0;kubectl get pods -o wide  --all-namespaces | grep walkadoo | grep 167
: 1498748308:0;kubectl get pods -o wide  --all-namespaces | grep walkadoo | grep 208
: 1498748324:0;kubectl get pods -o wide  --all-namespaces | grep walkadoo | grep 218
: 1498748345:0;kubectl get pods -o wide  --all-namespaces | grep walkadoo | grep 142
: 1498753644:0;kubectl get contexts
: 1498753688:0;kubectl get pods -o wide  --all-namespaces | grpe wbt
: 1498753729:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-20-135-141.ec2.internal
: 1498758161:0;history | grep shared
: 1498758174:0;ssh -A -L 9997:127.0.0.1:9997 ubuntu@52.86.252.208 ssh -L 9997:shared-staging-mysql.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@54.81.235.76
: 1498758192:0;ssh -A -L 9997:127.0.0.1:9997 heidischmidt@52.86.252.208 ssh -L 9997:shared-staging-mysql.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306 -N heidischmidt@54.81.235.76
: 1498758220:0;history | grep shared |Âƒ€grep postgres
: 1498758235:0;history | grep postgres | grep shared
: 1498758839:0;history | grpe psql
: 1498758843:0;history | grep psql
: 1498758995:0;cd Documents/postgres
: 1498759002:0;cd pggrants
: 1498759013:0;more pg_grants_deis-staging-new.sql
: 1498759381:0;more pg_grants_deis-staging-new.sql | sed s/deis_staging/consolidated_staging/g
: 1498760666:0;kubectl get pods -o wide  --all-namespaces | grep kube
: 1498840193:0;kubectl get nodes -o jsonpath='{.items[*].status.addresses}'
: 1498840288:0;kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="InternalIP")].address}'
: 1498840392:0;kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="Hostname address")].address}'
: 1498840401:0;kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="Hostname address")]}'
: 1498840425:0;kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="LegacyHostIP")].address}'
: 1498840522:0;kubectl get nodes -o wide -o jsonpath 
: 1498840533:0;kubectl get nodes -o wide -o jsonpath '{.items[*].status.addresses}'
: 1498840544:0;kubectl get nodes -o wide -o jsonpath='{.items[*].status.addresses}'
: 1498841234:0;kops get clusters
: 1498841242:0;kubectl config view 
: 1498841806:0;ssh admin@ip-172-20-198-43.ec2.internal
: 1498845499:0;kubectl get pods -o wide  --all-namespaces | grep wilder
: 1498845536:0;kubectl --namespace wilder logs wilder-staging-web-2642191772-n2l2v 
: 1498845547:0;kubectl --namespace wilder-staging logs wilder-staging-web-2642191772-n2l2v 
: 1498870579:0;kubectl get nodes -o wide | grep 172.20.183.69
: 1498870596:0;kubectl describe node 172.20.183.69
: 1498870606:0;kubectl describe node ip-172-20-183-69.ec2.internal
: 1498870642:0;mkdir logs-ip-172-20-183-69.ec2.internal
: 1498870645:0;cd logs-ip-172-20-183-69.ec2.internal
: 1498870664:0;kubectl describe node ip-172-20-183-69.ec2.internal > kubectl-desc-node-ip-172-20-183-69.ec2.internal
: 1498870675:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-20-183-69.ec2.internal
: 1498870687:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-20-183-69.ec2.internal  > pods-ip-172-20-183-69.ec2.internal
: 1498870748:0;kubectl get events --all-namespaces > kubectl-get-events-all-namespaces
: 1498870776:0;kubectl get nodes -o wide | grep ip-172-20-186-232.ec2.internal
: 1498870788:0;kubectl describe node ip-172-20-186-232.ec2.internal
: 1498870978:0;kubectl --namespace deis logs deis-builder-848547108-c6vj7
: 1498871035:0;kubectl describe node ip-172-20-186-232.ec2.internal > kubectl-events-other-node-ip-172-20-186-232.ec2.internal
: 1498871052:0;kubectl delete pod deis-builder-848547108-c6vj7 
: 1498871076:0;kubectl delete pod deis-builder-848547108-c6vj7 --namespace deis
: 1498871085:0;kubectl get pods -o wide  --all-namespaces | grep deis-builder
: 1498871116:0;vi deis-builder-pod-kicked-to-see-if-events-clear-up-on-2nd-node
: 1498992758:0;kubectl get pods -o wide  --all-namespaces > all-pods-now
: 1498992934:0;cat all-pods-now| grep ip-172-20-155-230.ec2.internal
: 1498993034:0;kubectl get events > all-events-now
: 1498993043:0;kubectl get events --all-namespaces >> all-events-now
: 1498993252:0;grep deis-builder all-pods-now
: 1499097028:0;brew list
: 1499097058:0;locate postgres
: 1499097088:0;ls -l ../Cellar/postgresql/9.6.2/bin/postgres
: 1499097107:0;ls -l /usr/local/bin/Cellar/postgresql/9.6.2/bin/postgres
: 1499097112:0;ls -ltra /usr/local/bin/postgres
: 1499097116:0;ls -ltra /usr/local/bin/postgres/*
: 1499097123:0;ls -ltra /usr/local/bin/postgres*
: 1499097134:0;locate postgres.conf
: 1499097143:0;brew list postgres
: 1499097156:0;ls -l /usr/local/Cellar/postgresql/
: 1499097161:0;ls -l /usr/local/Cellar/postgresql/9.6.2
: 1499097169:0;ls -l /usr/local/Cellar/postgresql/9.6.2/bin
: 1499097354:0;brew services 
: 1499097360:0;brew services list
: 1499097453:0;brew update
: 1499108441:0;kubectl get nodes | awk '{print $1}'
: 1499108452:0;kubectl get nodes | awk '{print $1}' | grep ip
: 1499108517:0;for i in `kubectl get nodes | awk '{print $1}' | grep ip`\
do \
ssh admin@${node} sudo docker ps > containers-${node}\
done
: 1499108531:0;for node in `kubectl get nodes | awk '{print $1}' | grep ip`\
do \
ssh admin@${node} sudo docker ps > containers-${node}\
done
: 1499108556:0;rm containers-
: 1499108561:0;more containers-ip-172-20-143-248.ec2.internal
: 1499108640:0;ssh admin@ip-172-20-130-151.ec2.internal sudo docker ps 
: 1499108647:0;ssh admin@ip-172-20-130-151.ec2.internal which iotop
: 1499109029:0;for node in `kubectl get nodes | awk '{print $1}' | grep ip`\
do \
ssh admin@${node} sudo apt-get install iotop -y > iotop-install-all-k8s-containers-${node}\
done
: 1499109047:0;more iotop-install-all-k8s-containers-ip-172-20-*
: 1499109566:0;kubectl get nodes 
: 1499110549:0;aws cloudwatch describe-alarm-history
: 1499110570:0;aws cloudwatch describe-alarm-history | grep INSUFFICIENT_DATA
: 1499110583:0;aws cloudwatch describe-alarm-history 
: 1499110673:0;aws cloudwatch describe-alarm-history help
: 1499110996:0;aws cloudwatch describe-alarm-history | jq '.'
: 1499111051:0;aws cloudwatch describe-alarm-history | grep -B 4 awsebs-vol 
: 1499111125:0;aws cloudwatch describe-alarm-history > ../aws-cloudwatch-ebs-vol-output.txt
: 1499111131:0;more ../aws-cloudwatch-ebs-vol-output.txt
: 1499111165:0;mkdir ../docker-sleuthing
: 1499111175:0;mv containers-ip-172-20-* ../docker-sleuthing
: 1499111183:0;rm iotop-install-all-k8s-containers-ip-172-20-*
: 1499111185:0;ls -l
: 1499280381:0;kubectl get nodes -o wide  | grep ip-172-24-167-0.ec2.internal
: 1499280389:0;ssh admin@ip-172-24-167-0.ec2.internal
: 1499281009:0;ssh admin@172.24.167.0
: 1499283241:0;kubectl get nodes -o wide  | grep 167
: 1499283261:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-167-0.ec2.internal
: 1499351313:0;kubectl get pods -o wide  --all-namespaces > Documents/pod-list-1030AM-thurs-06-2017
: 1499351347:0;more Documents/pod-list-1030AM-thurs-06-2017
: 1499351368:0;more Documents/pod-list-1030AM-thurs-06-2017 | grep Terminating
: 1499351511:0;grep ip-172-24-182-5.ec2.internal Documents/node-list-1030AM-thurs-06-2017
: 1499351685:0;grep ip-172-24-182-5.ec2.internal Documents/pod-list-1030AM-thurs-06-2017
: 1499353015:0;kubectl --namespace dc-production logs dc-production-jobs-248526588-cw3gp -p
: 1499353019:0;kubectl --namespace dc-production logs dc-production-jobs-248526588-cw3gp 
: 1499353053:0;kubectl delete pod --namespace dc-production dc-production-jobs-248526588-cw3gp
: 1499353064:0;kubectl delete pod --namespace dc-production dc-production-jobs-248526588-pdm2s
: 1499353115:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-182-5.ec2.internal
: 1499353403:0;vi Documents/events-ip-172-24-185-5.ec2.internal
: 1499353546:0;kubectl get pods -o wide  --all-namespaces > Documents/pod-list-11AM-thurs-06-2017
: 1499353561:0;kubectl get nodes -o wide > Documents/node-list-11AM-thurs-06-2017
: 1499353664:0;kubectl --namespace dc-production logs dc-production-clockwork-2010069904-1wgx9
: 1499361962:0;history | grep context
: 1499698286:0;kubectl get pods -o wide  --all-namespaces | grep -v Runninb
: 1499698289:0;kubectl get pods -o wide  --all-namespaces | grep -v Runnin
: 1499698495:0;kubectl get events --all-namespaces | grep ip-172-24-142-37.ec2.internal
: 1499698512:0;kubectl describe node ip-172-24-142-37.ec2.internal
: 1499698679:0;kubectl --namespace walkadoo-production logs walkadoo-production-web-973230320-9lg9x
: 1499698684:0;kubectl --namespace walkadoo-production logs walkadoo-production-web-973230320-9lg9x -p
: 1499699738:0;kubectl get nodes -o wide  | grep ip-172-24-142-37.ec2.internal
: 1499699945:0;kubectl get pods -o wide  --all-namespaces | grep dc-prod | sort +7n
: 1499699948:0;kubectl get pods -o wide  --all-namespaces | grep dc-prod | sort +7
: 1499699951:0;kubectl get pods -o wide  --all-namespaces | grep dc-prod | sort 
: 1499700005:0;kubectl get pods -o wide  --all-namespaces | grep dc-prod | sort > Documents/dc/k8s-prod-dc-pods-all-for-processing-pattern
: 1499700035:0;cat Documents/dc/k8s-prod-dc-pods-all-for-processing-pattern | awk '{print $6}'
: 1499700039:0;cat Documents/dc/k8s-prod-dc-pods-all-for-processing-pattern | awk '{print $7}'
: 1499700046:0;cat Documents/dc/k8s-prod-dc-pods-all-for-processing-pattern | awk '{print $8}'
: 1499700058:0;cat Documents/dc/k8s-prod-dc-pods-all-for-processing-pattern | awk '{print $8}' | sort | uniq -c
: 1499700089:0;vi Documents/dc/k8s-prod-dc-pods-all-for-processing-pattern-by-node-count
: 1499700131:0;vi Documents/dc/k8s-prod-dc-pods-all-for-processing-pattern
: 1499700659:0;kubectl get pods -o wide  --all-namespaces > Documents/dc/k8s-prod-all-pods-running-to-compare
: 1499700931:0;aws iam get-access-key-last-used help
: 1499700940:0;aws iam get-access-key-last-used 
: 1499700950:0;aws iam help
: 1499700959:0;aws iam list-access-keys
: 1499705846:0;cat Documents/dc/k8s-prod-dc-pods-all-for-processing-pattern | awk ' /ip/ {print $8}' 
: 1499705855:0;cat Documents/dc/k8s-prod-dc-pods-all-for-processing-pattern | awk ' /ip/ ' 
: 1499705876:0;cat Documents/dc/k8s-prod-dc-pods-all-for-processing-pattern | awk ' /Evicted/ {print $8}' 
: 1499706353:0;kubectl get pods -o wide  --all-namespaces | grep prometheus
: 1499706382:0;kubectl get deployments --namespace prometheus
: 1499706395:0;kubectl get deployments --namespace prometheus -o yaml prometheus
: 1499707375:0;history | grep yamllint
: 1499707441:0;find kubectl-deployment-prod-prometheus.050117.yml -print
: 1499707448:0;find . -name kubectl-deployment-prod-prometheus.050117.yml -print
: 1499707472:0;yamllint -d relaxed  ./Documents/kubernetes/kubectl-deployment-prod-prometheus.050117.yml
: 1499707480:0;more ./Documents/kubernetes/kubectl-deployment-prod-prometheus.050117.yml
: 1499707513:0;find . -name yamllint-test-prom-configmap-get.yml -print
: 1499707530:0;yamllint -d relaxed ./Documents/kubernetes/yamllint-test-prom-configmap-get.yml
: 1499707541:0;more ./Documents/kubernetes/yamllint-test-prom-configmap-get.yml
: 1499707937:0;kubectl get configmap prometheus-config --namespace prometheus -o yaml | grep -v "^ "
: 1499708725:0;kubectl --namespace dc-staging logs dc-staging-conciergelistener-1563114591-304rt
: 1499708951:0;kubectl get nodes -o wide | grep ip-172-20-170-102.ec2.internal
: 1499709080:0;vi Documents/dc/k8s-stg-node-oom-lost-07102107
: 1499709122:0;kubectl get nodes -o wide | grep 69
: 1499709182:0;vi Documents/dc/k8s-stg-node-oom-lost-07102107-dc-concierge-listener-failed-1st
: 1499709700:0;kubectl get pods -o wide  --all-namespaces | grpe listen
: 1499709705:0;kubectl get pods -o wide  --all-namespaces | grep listen
: 1499709729:0;kubectl --namespace dc-staging logs dc-staging-conciergelistener-1563114591-bddb6
: 1499710806:0;cd dc
: 1499710817:0;more k8s-stg-node-oom-lost-07102107
: 1499710834:0;cat k8s-stg-node-oom-lost-07102107 | awk '/scaling/'
: 1499710848:0;cat k8s-stg-node-oom-lost-07102107 | awk '/scaling/ {print $1}'
: 1499710856:0;cat k8s-stg-node-oom-lost-07102107 | grep -i scal
: 1499710881:0;cat k8s-stg-node-oom-lost-07102107-dc-concierge-listener-failed-1st
: 1499711990:0;kubectl get deployments --namespaces kube-system
: 1499712085:0;kubectl get pods -o wide  --all-namespaces | head -1
: 1499712104:0;kubectl get deployments --namespace kube-system
: 1499714832:0;kubectl get deployment --namespace kube-system
: 1499714859:0;kubectl get deployment --namespace kube-system -o yaml | rgep -i ewtc
: 1499714864:0;kubectl get deployment --namespace kube-system -o yaml | grep etc
: 1499714888:0;kubectl get config_map --namespace kube-system -o yaml | grep etc
: 1499714900:0;kubectl get configmap --namespace kube-system -o yaml | grep etc
: 1499714910:0;kubectl get configmap --namespace kube-system
: 1499714924:0;kubectl get configmap --namespace kube-system deis-workflow-ywhet.v2
: 1499714930:0;kubectl get configmap --namespace kube-system deis-workflow-ywhet.v2 -o yaml
: 1499745744:0;kubectl get pods -o wide  --all-namespaces > Documents/pod-list-1201am-tues-07112017
: 1499745766:0;kubectl get nodes -o wide > Documents/node-list-1201am-tues-07112017
: 1499745831:0;cat Documents/pod-list-1201am-tues-07112017 | awk ' /dc-prod/'
: 1499745841:0;cat Documents/pod-list-1201am-tues-07112017 | awk '! /Running/'
: 1499746416:0;mkdir logs-ip-172-24-142-37
: 1499746431:0;scp admin@ip-172-24-142-37/tmp/*.log . 
: 1499746506:0;scp admin@ip-172-24-142-37/tmp/containers.log.pos .
: 1499746514:0;scp admin@ip-172-24-142-37:/tmp/containers.log.pos .
: 1499746525:0;scp admin@ip-172-24-142-37.ec2.internal:/tmp/containers.log.pos .
: 1499746538:0;scp admin@ip-172-24-142-37.ec2.internal:/tmp/*.log* .
: 1499746554:0;scp admin@ip-172-24-142-37.ec2.internal:/tmp/daemon.log .
: 1499746603:0;scp admin@ip-172-24-142-37.ec2.internal:/tmp/docker-all-ip-172-24-142-37.log .
: 1499746619:0;scp admin@ip-172-24-142-37.ec2.internal:/tmp/docker-events-last-10m.log .
: 1499746635:0;scp admin@ip-172-24-142-37.ec2.internal:/tmp/iotop-300-iterations.log .
: 1499746656:0;scp admin@ip-172-24-142-37.ec2.internal:/tmp/kubelet.log .
: 1499746675:0;scp admin@ip-172-24-142-37.ec2.internal:/tmp/syslog .
: 1499746968:0;kubectl get pods -o wide  --all-namespaces | awk ' /dc-prod/'
: 1499747026:0;kubectl get pods -o wide  --all-namespaces | awk '! /Running/'
: 1499747070:0;kubectl get pods -o wide  --all-namespaces | awk '! /14d/'
: 1499747092:0;kubectl get pods -o wide  --all-namespaces | awk ' /h/'
: 1499747103:0;kubectl get pods -o wide  --all-namespaces | awk ' /h/' | more
: 1499781932:0;ssh admin@ip-172-24-142-37.ec2.internal
: 1499782007:0;more docker-all-ip-172-24-142-37.log
: 1499782035:0;kubectl get pods -o wide  --all-namespaces | grep 37
: 1499782119:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-142-37.ec2.internal
: 1499782140:0;kubectl get pods -o wide  --all-namespaces | grep concierge 
: 1499782403:0;cd logs-ip-172-24-142-37
: 1499782407:0;ls -ltra 
: 1499782412:0;more iotop-300-iterations.log
: 1499786490:0;;kcup
: 1499787040:0;DEIS_PROFILE=production deis ps:list -a dc-production
: 1499787121:0;DEIS_PROFILE=production deis config:list -a dc-production | grep DATA
: 1499787131:0;DEIS_PROFILE=production deis config:list -a dc-production | grep -i read
: 1499787187:0;DEIS_PROFILE=production deis config:list -a dc-production | grep -i dc-production3-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com
: 1499787216:0;DEIS_PROFILE=production deis config:list -a dc-production | grep URL
: 1499787224:0;DEIS_PROFILE=production deis config:list -a dc-production | grep PASS
: 1499787273:0;DEIS_PROFILE=production deis config:list -a dc-production > Documents/dc/deis-config-list-dc-production
: 1499787279:0;vi Documents/dc/deis-config-list-dc-production
: 1499787434:0;DEIS_PROFILE=production deis config:list -a dc-production | grep -i rds
: 1499787607:0;history | grep graf
: 1499787622:0;kubectl --namespace deis get deployment deis-monitor-grafana -o json | jq --raw-output '.spec.template.spec.containers[0].env[] | select(.name=="DEFAULT_USER_PASSWORD").value'
: 1499788207:0;ssh heidischmidt@54.234.134.25
: 1499790140:0;history | grpe mysql 
: 1499790145:0;history | grep mysql 
: 1499792915:0;cd chef-dailychallenge
: 1499792932:0;more Berksfile.lock
: 1499792954:0;more metadata.rb
: 1499792968:0;more templates/default/nginx-site.erb
: 1499793253:0;DEIS_PROFILE=production deis config:list -a walkadoo-production | grep -i pass
: 1499793355:0;DEIS_PROFILE=production deis config:list -a iris-production | grep -i url
: 1499793373:0;DEIS_PROFILE=production deis apps:list 
: 1499793383:0;DEIS_PROFILE=production deis config:list -a wbt-production | grep -i url
: 1499793395:0;DEIS_PROFILE=production deis config:list -a quitnet-production | grep -i url
: 1499793412:0;DEIS_PROFILE=production deis config:list -a dc-production
: 1499793578:0;DEIS_PROFILE=production deis config:list -a walkadoo-production
: 1499793586:0;DEIS_PROFILE=production deis config:list -a walkadoo-production | grep -i url
: 1499793766:0;kubectl get pods -o wide  --all-namespaces | grep dc-st
: 1499793926:0;DEIS_PROFILE=production deis config:set -a dc-production mysql2://dc_prod_app:K807954!96S7y@dc-production-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com/meyouhealth_production?encoding=utf8&pool=25&reconnect=true 
: 1499793979:0;history | grep config:set
: 1499794052:0;DEIS_PROFILE=production deis config:set -a dc-production PASSIVE_DATABASE_URL="mysql2://dc_prod_app:K8079547y@dc-production-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com/meyouhealth_production?encoding=utf8&pool=25&reconnect=true"
: 1499794112:0;DEIS_PROFILE=production deis config:set -a dc-production PASSIVE_DATABASE_URL='mysql2://dc_prod_app:K807954!96S7y@dc-production-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com/meyouhealth_production?encoding=utf8&pool=25&reconnect=true'
: 1499794649:0;DEIS_PROFILE=production deis config:list -a wbt-production | grep -i passive
: 1499794867:0;shipit env list
: 1499795673:0;history | grep 10.
: 1499795680:0;history | grep 10. | grep ssh
: 1499795697:0;ssh -t heidischmidt@54.225.238.195
: 1499795866:0;history | grep shipit | grep MAINT
: 1499795983:0;history | grep "shipit create deploy"
: 1499797010:0;DEIS_PROFILE=production deis config:list -a reporting-production | grep -i pass 
: 1499797037:0;shipit env list reporting-production
: 1499797214:0;shipit env config_set reporting-production DATABASES_DC_URL mysql2://reporting:vK4NA4X%236ygK%29rX8k%28jazwCa@dc-production-read1.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306/meyouhealth_production
: 1499797224:0;shipit env config_list reporting-production | grep -i dc-production
: 1499797257:0;shipit create deployment reporting-production 7280a18 
: 1499797308:0;history | grep "shipit" | grep deploy
: 1499797334:0;shipit deployment create reporting-production 7280a18
: 1499797458:0;shipit deployment logs 5371
: 1499873707:0;more docker-events-last-10m.log
: 1499873723:0;grep -i error docker-events-last-10m.log
: 1499873729:0;grep -i end docker-events-last-10m.log
: 1499873743:0;grep -i exec_start docker-events-last-10m.log
: 1499877803:0;mkdir logs-ip-172-20-130-151
: 1499877815:0;cd logs-ip-172-20-130-151
: 1499877835:0;scp admin@ip-172-20-130-151.ec2.internal:/tmp/ip-172*.log .
: 1499877846:0;scp admin@ip-172-20-130-151.ec2.internal:/tmp/* .
: 1499878707:0;which mysql_config_editor
: 1499878732:0;ls -ltra /usr/local/bin/mysql*
: 1499879238:0;ssh admin@ip-172-20-143-248.ec2.internal
: 1499881375:0;shipit deployment logs 5372
: 1499881435:0;shipit ssh wellbeing-staging
: 1499881443:0;shipit ssh wellbeingid-staging
: 1499881530:0;shipit deployment logs 5373
: 1499882513:0;ssh -t heidischmidt@54.204.40.159
: 1499882636:0;ssh -t heidischmidt@54.81.119.3
: 1499882691:0;shipit deployment logs 5374
: 1499883226:0;shipit console deployment-api
: 1499883375:0;shipit deployment create wellbeingid-staging 83e5c8a5cd4e57ddc1e017e9002eb5fa7057283f
: 1499883449:0;ssh admin@ip-172-20-163-188.ec2.internal
: 1499884188:0;more /etc/crontab
: 1499884195:0;man cron
: 1499884214:0;aws s3 ls
: 1499884247:0;aws s3 ls k8s-node-logs
: 1499887576:0;scp admin@ip-172-20-130-151.ec2.internal:/etc/cron.hourly/k8s-monitoring .
: 1499887601:0;kubectl get nodes -o wide | awk '! /master/'
: 1499887618:0;kubectl get nodes -o wide | awk '! /master/ {print $1}'
: 1499887635:0;kubectl get nodes -o wide | awk '! /master/ {print $1}' | tail -6
: 1499887693:0;for node in `kubectl get nodes -o wide | awk '! /master/ {print $1}' | tail -6`\
do\
scp k8s-monitoring admin@${node}:/etc/cron.hourly/k8s-monitoring\
done
: 1499887718:0;for node in `kubectl get nodes -o wide | awk '! /master/ {print $1}' | tail -6`\
do\
scp k8s-monitoring admin@${node}:/tmp/k8s-monitoring\
done
: 1499887739:0;for node in `kubectl get nodes -o wide | awk '! /master/ {print $1}' | tail -6`\
do admin@${node}:/tmp/k8s-monitoring\
done
: 1499887783:0;for node in `kubectl get nodes -o wide | awk '! /master/ {print $1}' | tail -6`\
do\
ssh admin@${node} bash sudo cp /tmp/k8s-monitoring /etc/cron.hourly/k8s-monitoring\
done
: 1499887815:0;ssh admin@ip-172-20-143-56.ec2.internal date
: 1499887827:0;ssh admin@ip-172-20-143-56.ec2.internal sudo cp /tmp/k8s-monitoring /etc/cron.hourly/k8s-monitoring
: 1499887843:0;ssh admin@ip-172-20-143-56.ec2.internal sudo cat /etc/cron.hourly/k8s-monitoring
: 1499887855:0;for node in `kubectl get nodes -o wide | awk '! /master/ {print $1}' | tail -6`\
do\
ssh admin@${node} sudo cp /tmp/k8s-monitoring /etc/cron.hourly/k8s-monitoring\
done
: 1499887882:0;for node in `kubectl get nodes -o wide | awk '! /master/ {print $1}' | tail -6`\
do\
ssh admin@${node} sudo cat  /etc/cron.hourly/k8s-monitoring\
done
: 1499887919:0;kubectl get nodes -o wide | awk '! /master/ {print $1}' 
: 1499887927:0;kubectl get nodes -o wide | awk '! /master/ {print $1}' | wc -l 
: 1499888820:0;kubectl --namespace dc-staging logs dc-staging-conciergelistener-1524547191-0wprx
: 1499888880:0;kubectl --namespace dc-staging logs dc-staging-conciergelistener-1524547191-0wprx --previous
: 1499889104:0;kubectl describe node ip-172-20-172-74.ec2.internal
: 1499947071:0;ssh admin@ip-172-20-202-25.ec2.internal
: 1499947705:0;kubectl describe node ip-172-24-201-130.ec2.internal
: 1499948012:0;kubectl get pods -o wide  --all-namespaces | grep 130
: 1499948527:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-20-172-74.ec2.internal
: 1499948538:0;kubectl get pods -o wide  --all-namespaces | ip-172-20-202-25.ec2.internal
: 1499948547:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-20-202-25.ec2.internal
: 1499948881:0;for node in `kubectl get nodes -o wide | awk '! /master/ {print $1}' | tail -12`\
do\
ssh admin@${node} hostname; free -m \
done
: 1499948911:0;for node in `kubectl get nodes -o wide | awk '! /master/ {print $1}' | tail -12`\
do\
ssh admin@${node} hostname; `free -m`; uptime \
done
: 1499948952:0;for node in `kubectl get nodes -o wide | awk '! /master/ {print $1}' | tail -12`\
do\
ssh admin@${node} free -m\
done
: 1499948990:0;for node in `kubectl get nodes -o wide | awk '! /master/ {print $1}' | tail -6`\
do\
ssh admin@${node} hostname; free -m\
done
: 1499949010:0;for node in `kubectl get nodes -o wide | awk '! /master/ {print $1}' | tail -6`\
do\
ssh admin@${node}  free -m\
done
: 1499949028:0;for node in `kubectl get nodes -o wide | awk '! /master/ {print $1}' | tail -6`\
do\
echo ${node}; ssh admin@${node}  free -m\
done
: 1499949724:0;kops get cluster -o yaml neywk.k8s.meyouhealth.com
: 1499949800:0;kops get cluster -o yaml lrywh.k8s.myhstg.com
: 1499953834:0;ssh admin@ip-172-20-172-74.ec2.internal
: 1499953921:0;scp admin@ip-172-20-172-74.ec2.internal:/tmp/logs-ip-172-20-172-74.tar .
: 1499953996:0;scp admin@ip-172-20-202-25.ec2.internal:/tmp/logs-ip-172-20-202-25.tar .
: 1499954351:0;kubectl delete pod dc-staging-jobs-2319034587-wr08r --namespace dc-staging
: 1499954581:0;kubectl --namespace deis logs deis-monitor-telegraf-w6wdq
: 1499955549:0;kubectl get pods -o wide  --all-namespaces | grep dc-staging
: 1499955614:0;kubectl get pods -o wide  --all-namespaces | grep dc-staging | grep con
: 1499955732:0;kubectl get pods -o wide  --all-namespaces | grep dc-staging 
: 1499956742:0;kops get instancegroup nodes -o yaml > test
: 1499956746:0;vi test
: 1499956997:0;diff -u test after-edit
: 1499957003:0;cat test
: 1499957136:0;kubectl get nodes -o wide  | wc -l
: 1499957151:0;kubectl get nodes -o wide  | grep -v master | wc -l 
: 1499957162:0;kubectl get nodes -o wide  | grep -v master 
: 1499957279:0;kops get instancegroup nodes -o yaml > after-edit
: 1499957283:0;more after-edit
: 1499957396:0;kubectl get nodes -o wide | grep ip-172-20-143-56.ec2.internal
: 1499958066:0;kops rolling-update cluster --yes
: 1499958198:0;kubectl get nodes -o wide  | grep 248
: 1499958316:0;kubectl get nodes -o wide | grep ip-172-20-202-221.ec2.internal
: 1499959772:0;kubectl --namespace deis logs deis-logger-176328999-mz76z
: 1499959796:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-20-182-236.ec2.internal
: 1499959818:0;kubectl get pods -o wide  --all-namespaces | grep deis-logger-176328999-mz76z
: 1499959871:0;more test
: 1499959890:0;ls test
: 1499959892:0;ls after-edit
: 1499959906:0;mv test ../kops-update-k8s-stg-cluster-test
: 1499959927:0;mv after-edit ../kops-update-k8s-stg-cluster-after-test-edit
: 1499960045:0;kops describe clusters
: 1499960048:0;kops describe cluster
: 1499960084:0;kops validate --help
: 1499960134:0;kops edit instancegroup nodes
: 1499960181:0;kops validate cluster
: 1499960444:0;kubectl get nodes -o wide | grep 221
: 1499961645:0;ls -l k8s-ops
: 1499961648:0;cd k8s-ops
: 1499961670:0;git pull remote origin
: 1499961677:0;git pull master
: 1499961710:0;cd add-ons
: 1499961924:0;kops edit cluster lrywh.k8s.myhstg.com
: 1499961982:0;more kops-update-k8s-stg-cluster-test
: 1499962015:0;kops get cluster lrywh.k8s.myhstg.com
: 1499969408:0;nslookup 10.0.55.206
: 1499969447:0;kubectl get nodes -o wide | grep 172.24.221.193
: 1499970852:0;aws elasticache describe-cache-clusters > Documents/elasticache-clusters-all-071317.txt
: 1499970860:0;vi Documents/elasticache-clusters-all-071317.txt
: 1499970914:0;head -300 Documents/elasticache-clusters-all-071317.txt | tail -50
: 1499970941:0;more elasticache-clusters-all-071317.txt
: 1499971028:0;aws elasticache describe-replication-groups > elasticache-desc-repl-groups-071317.txt
: 1499971041:0;more elasticache-desc-repl-groups-071317.txt
: 1499971070:0;aws elasticache create-replication-group help
: 1499971266:0;more Documents/elasticache-clusters-all-071317.txt
: 1499971498:0;more Documents/elasticache-clusters-all-071317.txt | grep -i replication
: 1499971535:0;more Documents/elasticache-clusters-all-071317.txt | grep -i description
: 1499971539:0;more Documents/elasticache-clusters-all-071317.txt | grep -i desc
: 1499971568:0;more Documents/elasticache-clusters-all-071317.txt | grep -i sg
: 1499971627:0;aws elasticache create-replication-group create-replication-group
: 1499971627:0;		          --replication-group-id  dwh-production 
: 1499971627:0;		          --replication-group-description  DWH Production Redis Replication Group 
: 1499971669:0;aws elasticache create-replication-group create-replication-group --replication-group-id  dwh-production --replication-group-description  "DWH Production Redis Replication Group" --replication-group-id  dwh-production
: 1499971731:0;aws elasticache create-replication-group --replication-group-id  dwh-production --replication-group-description  "DWH Production Redis Replication Group" --primary-cluster-id  dwh-production
: 1499971829:0;aws elasticache modify-replication-group help
: 1499971870:0;aws elasticache help 
: 1499971904:0;aws elasticache describe-replication-groups
: 1499972144:0;aws elasticache describe-replication-groups > Documents/elasticache-desc-repl-groups-071317.txt
: 1499972149:0;vi Documents/elasticache-desc-repl-groups-071317.txt
: 1499972208:0;head -250 Documents/elasticache-desc-repl-groups-071317.txt
: 1499976134:0;kubectl get pods -o wide  --all-namespaces | grep dwh
: 1499976158:0;kubectl --namespace dwh-production logs dwh-production-jobs-456421872-6tm5x
: 1500041384:0;ssh admin@ip-172-24-201-130.ec2.internal
: 1500041904:0;;kucp
: 1500044666:0;kubectl get nodes -o wide | grep ip-172-24-207-2.ec2.internal 
: 1500044748:0;aws-info i-055db1753b8bf52b0
: 1500044786:0;aws-info i-055db1753b8bf52b0 > Documents/cpu-80-percent-alert-i-055db1753b8bf52b0.071417
: 1500044806:0;aws ebs help
: 1500044846:0;more  Documents/cpu-80-percent-alert-i-055db1753b8bf52b0.071417
: 1500044856:0;more cpu-80-percent-alert-i-055db1753b8bf52b0.071417
: 1500044878:0;more cpu-80-percent-alert-i-055db1753b8bf52b0.071417 | grep vol
: 1500044883:0;more cpu-80-percent-alert-i-055db1753b8bf52b0.071417 | grep -i vol
: 1500044977:0;aws ebs describe-volumes
: 1500044996:0;aws ec2 describe-volumes
: 1500045080:0;aws ec2 describe-volumes > aws-ec2-describe-volumes.071417.txt
: 1500045170:0;aws ec2 describe-volumes --filters "CreateTime":
: 1500045210:0;aws ec2 describe-volumes --filters CreateTime="2017-07"
: 1500045235:0;history | grep filters
: 1500045293:0;aws ec2 describe-volumes --filters Name=CreateTime,Values="2017-07"
: 1500045310:0;aws ec2 describe-volumes --filters Name=create-time,Values="2017-07"
: 1500045333:0;aws ec2 describe-volumes --filters Name=create-time,Values="2017-07-13T14:57:25.561Z"
: 1500045386:0;aws ec2 describe-volumes --filters Name=create-time,Values="2017-07-*"
: 1500052397:0;aws ec2 describe-volumes --filters Name=create-time,Values="2017-07-*" > aws-ec2-describe-volumes-july2017.071417.txt
: 1500055152:0;kubectl list services --all-namespaces
: 1500055510:0;kops get cluster lrywh.k8s.myhstg.com -o yaml
: 1500057898:0;more aws-ec2-describe-volumes-july2017.071417.txt
: 1500057917:0;more aws-ec2-describe-volumes-july2017.071417.txt | awk /InstanceId/
: 1500057940:0;more aws-ec2-describe-volumes-july2017.071417.txt | awk -F ":" /InstanceId {print $2}/
: 1500057949:0;more aws-ec2-describe-volumes-july2017.071417.txt | awk -F ":" /InstanceId '{print $2}'/
: 1500057963:0;more aws-ec2-describe-volumes-july2017.071417.txt | awk -F ":" /InstanceId/ '{print $2}'
: 1500057974:0;more aws-ec2-describe-volumes-july2017.071417.txt | awk -F ":" /InstanceId/ 
: 1500057988:0;more aws-ec2-describe-volumes-july2017.071417.txt | awk -F ":" /InstanceId '{print $1}'/ 
: 1500057994:0;more aws-ec2-describe-volumes-july2017.071417.txt | awk -F ":" / InstanceId '{print $1}'/ 
: 1500058004:0;history | grep awk
: 1500058038:0;more aws-ec2-describe-volumes-july2017.071417.txt | awk -F ":" '/ InstanceId / {print $1}' 
: 1500058045:0;more aws-ec2-describe-volumes-july2017.071417.txt | awk -F ":" '/ InstanceId /' 
: 1500058055:0;more aws-ec2-describe-volumes-july2017.071417.txt | awk -F ":" '/InstanceId/' 
: 1500058063:0;more aws-ec2-describe-volumes-july2017.071417.txt | awk -F ":" '/InstanceId/ {print $1}' 
: 1500058067:0;more aws-ec2-describe-volumes-july2017.071417.txt | awk -F ":" '/InstanceId/ {print $2}' 
: 1500058090:0;more aws-ec2-describe-volumes-july2017.071417.txt | awk -F ":" '/InstanceId/ {print $2}' | sed s/",//g\
"
: 1500058107:0;more aws-ec2-describe-volumes-july2017.071417.txt | awk -F ":" '/InstanceId/ {print $2}' | sed s/\,//g\

: 1500058118:0;aws-info "i-055db1753b8bf52b0"
: 1500058128:0;more aws-ec2-describe-volumes-july2017.071417.txt | awk -F ":" '/InstanceId/ {print $2}' | sed s/\",//g\

: 1500058138:0;more aws-ec2-describe-volumes-july2017.071417.txt | awk -F ":" '/InstanceId/ {print $2}' | sed s/\",//g | sed s/\"//g\

: 1500058249:0;aws-info i-045a35b57d2ea097c
: 1500058256:0;aws-info i-045a35b57d
: 1500058267:0;for i in instance `cat aws-ec2-describe-volumes-july2017.071417.txt | awk -F ":" '/InstanceId/ {print $2}' | sed s/\",//g | sed s/\"//g`\
\
do \
aws-info ${instance} > ${instance}-aws-info.`date "+%Y-%m-%dT%H%M%S"`.txt\
done
: 1500058280:0;for i in instance `cat Documents/aws-ec2-describe-volumes-july2017.071417.txt | awk -F ":" '/InstanceId/ {print $2}' | sed s/\",//g | sed s/\"//g`\
\
do \
aws-info ${instance} > ${instance}-aws-info.`date "+%Y-%m-%dT%H%M%S"`.txt\
done
: 1500058294:0;cat Documents/aws-ec2-describe-volumes-july2017.071417.txt | awk -F ":" '/InstanceId/ {print $2}' | sed s/\",//g | sed s/\"//g
: 1500058331:0;cat Documents/aws-ec2-describe-volumes-july2017.071417.txt | awk -F ":" '/InstanceId/ {print $2}' | sed s/\",//g | sed s/\"//g > Documents/aws-ec2-instances-ebs-vols-k8s
: 1500058393:0;for i in `cat Documents/aws-ec2-instances-ebs-vols-k8s`\
do \
aws-info $i > $i-aws-info.`date "+%Y-%m-%dT%H%M%S"`.txt\
done
: 1500058451:0;rm *2017-07-14T145107.txt *2017-07-14T145120.txt *2017-07-14T145121.txt
: 1500058483:0;rm *aws-info.2017-07-14T1451*
: 1500058486:0;sl -tlra
: 1500058498:0;aws-info i-04ba0822ec4492869
: 1500058523:0;aws-info i-04ba0822ec4492869 > i-04ba0822ec4492869-aws-info.2017-07-14T145442.txt
: 1500058559:0;more i-04ba0822ec4492869-aws-info.2017-07-14T145442.txt
: 1500058583:0;mv i*aws-info*201707-14*.txt Documents
: 1500058606:0;mv i-*-aws-info*2017*07-14*.txt Documents
: 1500250873:0;ls -tra
: 1500250896:0;rm *aws-info*.txt
: 1500250919:0;ls -l i*txt
: 1500250929:0;more i-*.txt
: 1500250937:0;more i-*.txt | grep Name
: 1500250992:0;aws ec2 describe-volumes --filters Name=create-time,Values="2017-*" > aws-ec2-describe-volumes-all-2017.071417.txt
: 1500251148:0;aws-info i-06b6f1fbfe489f8e9
: 1500251450:0;for node in `kubectl get nodes -o wide | awk '! /master/ {print $1}' | tail -12`\
do\
scp k8s-monitoring admin@${node}:/tmp/k8s-monitoring\
done
: 1500251477:0;kubectl get nodes -o wide | tail -10
: 1500251507:0;for node in `kubectl get nodes -o wide | awk '! /master/ {print $1}' | tail -10`\
do\
ssh admin@${node} sudo cp /tmp/k8s-monitoring /etc/cron.hourly/k8s-monitoring\
done
: 1500251530:0;for node in `kubectl get nodes -o wide | awk '! /master/ {print $1}' | tail -12`\
do\
ssh admin@${node} sudo cat  /etc/cron.hourly/k8s-monitoring\
done
: 1500251861:0;kubectl get pods -o wide  --all-namespaces | grep 233
: 1500251878:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-156-233
: 1500251920:0;kubectl --namespace deis logs deis-monitor-telegraf-d4ln7
: 1500252470:0;rbenv set 2.1.5
: 1500252477:0;rbenv local 2.1.5
: 1500252507:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-156-223.ec2.internal > cpu-spiking-ip-172-24-156-223.ec2.internal-80percent-july14-15-2017
: 1500252531:0;kubectl describe node ip-172-24-156-223.ec2.internal >> cpu-spiking-ip-172-24-156-223.ec2.internal-80percent-july14-15-2017
: 1500252699:0;;kgnodes
: 1500252707:0;kubectl get nodes -o wide  | grep ip-172-24-218-141
: 1500252751:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-218-141 > events-show-dead-no-alerts-ip-172-24-218-141-pods.log
: 1500252763:0;kubectl describe node ip-172-24-218-141 >> events-show-dead-no-alerts-ip-172-24-218-141-pods.log
: 1500252770:0;more events-show-dead-no-alerts-ip-172-24-218-141-pods.log
: 1500252899:0;scp admin@ip-172-24-218-141.ec2.internal:/tmp/logs-ip-172-24-218-141.tar
: 1500252902:0;scp admin@ip-172-24-218-141.ec2.internal:/tmp/logs-ip-172-24-218-141.tar .
: 1500299523:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-20-198-221.ec2.internal
: 1500299942:0;kubectl get logs --namespace deis deis-monitor-telegraf-jsv67 --previous 
: 1500299952:0;kubectl --namespace deis logs deis-monitor-telegraf-jsv67 --previous
: 1500300017:0;kubectl get nodes -o wide | grep ip-172-20-150-72
: 1500300047:0;scp Documents/k8s-monitoring admin@ip-172-20-150-72.ec2.internal:/tmp/
: 1500300079:0;scp admin@ip-172-20-150-72.ec2.internal:/tmp/k8s-monitoring /etc/cron.hourly/k8s-monitoring
: 1500300093:0;ssh admin@ip-172-20-150-72.ec2.internal:/tmp/k8s-monitoring /etc/cron.hourly/k8s-monitoring
: 1500300103:0;ssh admin@ip-172-20-150-72.ec2.internal sudo cp /tmp/k8s-monitoring /etc/cron.hourly/k8s-monitoring
: 1500300114:0;ssh admin@ip-172-20-150-72.ec2.internal sudo cat /etc/cron.hourly/k8s-monitoring
: 1500301209:0;DEIS_PROFILE=production deis config:list -a dc-production | grep -i url
: 1500301511:0;DEIS_PROFILE=production deis config:list -a dc-production | grep -i PASSIVE_DATABASE_URL
: 1500302644:0;shipit ssh wellbeingid-production
: 1500315606:0;kubectl --namespace kube-system logs calico-node-1hsdj calico-node
: 1500315643:0;kubectl --namespace kube-system logs calico-node-1hsdj
: 1500315652:0;kubectl --namespace kube-system logs calico-node-1hsdj install-cni
: 1500315668:0;kubectl --namespace deis logs deis-monitor-telegraf-5b29c
: 1500315717:0;kubectl --namespace wbt-production logs wbt-production-jobs-1565386904-mkk84
: 1500316005:0;kubectl --namespace walkadoo-production logs walkadoo-production-web-3255782859-5s7c8
: 1500316028:0;kubectl --namespace walkadoo-production logs walkadoo-production-wbidaccountchangelistener-2408973673-2l6k1
: 1500316053:0;kubectl --namespace quitnet-production logs quitnet-production-houston-812562809-n1skh
: 1500316062:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-183-74.ec2.internal
: 1500316121:0;kubectl --namespace kube-system logs kube-proxy-ip-172-24-183-74.ec2.internal
: 1500316139:0;kubectl --namespace deis logs deis-logger-fluentd-4sg9x
: 1500316228:0;kubectl --namespace deis logs deis-logger-fluentd-d11g8
: 1500316234:0;kubectl get pods -o wide  --all-namespaces | grep fluentd
: 1500316246:0;history |grep kubectl | grep tail 
: 1500316411:0;kubectl get pods -o wide  --all-namespaces | grep fluentd | awk '${print $2}'
: 1500316425:0;kubectl get pods -o wide  --all-namespaces | grep fluentd | awk '{print $2}'
: 1500316479:0;for i in `kubectl get pods -o wide  --all-namespaces | grep fluentd | awk '{print $2}'`\
do \
kubectl --namespace deis logs $i --tail 20 > logs-tail20-prod-k8s-$i.log\
done
: 1500316587:0;more logs-tail20-prod-k8s-deis-log*.log
: 1500316636:0;cat logs-tail20-prod-k8s-deis-logger-fluentd-*.log | grep -v info
: 1500316652:0;more logs-tail20-prod-k8s-deis-logger-fluentd-168p8.log
: 1500316670:0;mv logs-tail20-prod-k8s-deis-logger-fluentd-168p8.log buffer-logs-tail20-prod-k8s-deis-logger-fluentd-168p8.log
: 1500316679:0;more logs-tail20-prod-k8s-deis-logger-fluentd-0f35j.log
: 1500316684:0;mv logs-tail20-prod-k8s-deis-logger-fluentd-0f35j.log normal-logs-tail20-prod-k8s-deis-logger-fluentd-0f35j.log
: 1500316692:0;more logs-tail20-prod-k8s-deis-logger-fluentd-215j6.log
: 1500316697:0;mv logs-tail20-prod-k8s-deis-logger-fluentd-215j6.log normal-logs-tail20-prod-k8s-deis-logger-fluentd-215j6.log
: 1500316703:0;more logs-tail20-prod-k8s-deis-logger-fluentd-3sjjn.log
: 1500316710:0;mv logs-tail20-prod-k8s-deis-logger-fluentd-3sjjn.log normal-logs-tail20-prod-k8s-deis-logger-fluentd-3sjjn.log
: 1500316733:0;kubectl get nodes -o wide | grep deis-logger-fluentd-4rqcp
: 1500316767:0;kubectl get pods -o wide  --all-namespaces > pod-list-all-for-deis-logger-fluentd
: 1500316790:0;kubectl --namespace deis logs deis-logger-fluentd-4rqcp --previous
: 1500316799:0;more logs-tail20-prod-k8s-deis-logger-fluentd-d4889.log
: 1500316804:0;mv logs-tail20-prod-k8s-deis-logger-fluentd-d4889.log normal-logs-tail20-prod-k8s-deis-logger-fluentd-d4889.log
: 1500316813:0;more logs-tail20-prod-k8s-deis-logger-fluentd-d11g8.log
: 1500316821:0;mv logs-tail20-prod-k8s-deis-logger-fluentd-d11g8.log normal-logs-tail20-prod-k8s-deis-logger-fluentd-d11g8.log
: 1500316828:0;more logs-tail20-prod-k8s-deis-logger-fluentd-4sg9x.log
: 1500316834:0;mv logs-tail20-prod-k8s-deis-logger-fluentd-4sg9x.log buffer-logs-tail20-prod-k8s-deis-logger-fluentd-4sg9x.log
: 1500316855:0;grep 4sg9x pod-list-all-for-deis-logger-fluentd
: 1500316890:0;mv buffer-logs-tail20-prod-k8s-deis-logger-fluentd-4sg9x.log buffer-logs-tail20-prod-k8s-deis-logger-fluentd-4sg9x-ip-172-24-183-74.ec2.internal.log
: 1500316906:0;grep 168p8 pod-list-all-for-deis-logger-fluentd
: 1500316920:0;mv buffer-logs-tail20-prod-k8s-deis-logger-fluentd-168p8.log buffer-logs-tail20-prod-k8s-deis-logger-fluentd-168p8-ip-172-24-221-193.ec2.internal.log
: 1500316927:0;more logs-tail20-prod-k8s-deis-logger-fluentd-ss2hb.log
: 1500316933:0;mv logs-tail20-prod-k8s-deis-logger-fluentd-ss2hb.log normal-logs-tail20-prod-k8s-deis-logger-fluentd-ss2hb.log
: 1500316940:0;more logs-tail20-prod-k8s-deis-logger-fluentd-qwfxp.log
: 1500316945:0;mv logs-tail20-prod-k8s-deis-logger-fluentd-qwfxp.log normal-logs-tail20-prod-k8s-deis-logger-fluentd-qwfxp.log
: 1500316951:0;more logs-tail20-prod-k8s-deis-logger-fluentd-l58t0.log
: 1500316957:0;mv logs-tail20-prod-k8s-deis-logger-fluentd-l58t0.log normal-logs-tail20-prod-k8s-deis-logger-fluentd-l58t0.log
: 1500316963:0;more logs-tail20-prod-k8s-deis-logger-fluentd-wj4r1.log
: 1500316967:0;mv logs-tail20-prod-k8s-deis-logger-fluentd-wj4r1.log normal-logs-tail20-prod-k8s-deis-logger-fluentd-wj4r1.log
: 1500316969:0;ls -tlra
: 1500316977:0;more logs-tail20-prod-k8s-deis-logger-fluentd-vznr4.log
: 1500316986:0;mv logs-tail20-prod-k8s-deis-logger-fluentd-vznr4.log buffer-logs-tail20-prod-k8s-deis-logger-fluentd-vznr4.log
: 1500316987:0;ls -lrta
: 1500316997:0;grep vznr4 pod-list-all-for-deis-logger-fluentd
: 1500317012:0;mv buffer-logs-tail20-prod-k8s-deis-logger-fluentd-vznr4.log buffer-logs-tail20-prod-k8s-deis-logger-fluentd-vznr4-ip-172-24-172-178.ec2.internal.log
: 1500317022:0;more logs-tail20-prod-k8s-deis-logger-fluentd-vj6h1.log
: 1500317043:0;mv logs-tail20-prod-k8s-deis-logger-fluentd-vj6h1.log dwh-emit-transaction-failed-logs-tail20-prod-k8s-deis-logger-fluentd-vj6h1.log
: 1500317050:0;more logs-tail20-prod-k8s-deis-logger-fluentd-wnhm0.log
: 1500317055:0;mv logs-tail20-prod-k8s-deis-logger-fluentd-wnhm0.log normal-logs-tail20-prod-k8s-deis-logger-fluentd-wnhm0.log
: 1500317127:0;grep vj6h1 pod-list-all-for-deis-logger-fluentd
: 1500317186:0;more logs-tail20-prod-k8s-deis-logger-fluentd-4rqcp.log
: 1500317225:0;grep 4rqcp pod-list-all-for-deis-logger-fluentd
: 1500317300:0;kubectl get nodes -o wide | grep p-172-24-183-74.ec2.internal
: 1500317310:0;kubectl get pods -o wide  --all-namespaces | grep p-172-24-183-74.ec2.internal
: 1500317322:0;ls *4sg9x*
: 1500317382:0;ls -l *logs-tail20-prod-k8s-deis-logger-fluentd*
: 1500317388:0;ls -l *logs-tail20-prod-k8s-deis-logger-fluentd* | wc -l 
: 1500317423:0;more dwh-emit-transaction-failed-logs-tail20-prod-k8s-deis-logger-fluentd-vj6h1.log
: 1500320024:0;ssh admin@ip-172-24-183-74.ec2.internal
: 1500320374:0;xit
: 1500320396:0;kubectl get nodes -o wide | grep ip-172-24-183-74
: 1500320505:0;kubectl get nodes -o wide | grep prom
: 1500320513:0;kubectl get pods -o wide  --all-namespaces | grep prom
: 1500320646:0;kubectl get nodes -o wide | grep ip-172-24-156-223
: 1500320706:0;kubectl get nodes -o wide | grep ip-172-24-156-223.ec2.internal
: 1500320716:0;kubectl describe node ip-172-24-156-223.ec2.internal
: 1500320807:0;kubectl delete pod wbt-production-web-705909626-f2d4r --namespace wbt-production
: 1500320866:0;kubectl get nodes -o wide | grep ip-172-24-218-141.ec2.internal
: 1500321123:0;ssh admin@ip-172-24-218-141.ec2.internal
: 1500472162:0;shipit deployment logs 5389
: 1500472182:0;shipit deployment logs 5390
: 1500472291:0;kubectl --namespace dc-production logs dc-production-jobs-402539773-0rfmr
: 1500561465:0;kubectl get events --all-namespaces | head
: 1500570028:0;kubectl get nodes -o wide | grep ip-172-20-198-39.ec2.internal
: 1500647080:0;brew search ember
: 1500655461:0;DEIS_PROFILE=production deis config:list -a dc-production | grep -i redis
: 1500655502:0;DEIS_PROFILE=production deis config:list -a dc-production | grep -i rails
: 1500656966:0;kubectl describe pod etcd-server-ip-172-20-162-53.ec2.internal --namespace kube-system
: 1500656975:0;kubectl get pods -o wide  --all-namespaces | grep dc-
: 1500657002:0;kubectl describe pod dc-staging-web-509282694-v2zz4 --namespace dc-staging
: 1500657092:0;kubectl get events --all-namespaces | grep -i live
: 1500657127:0;kubectl describe pod superset-web-3676454101-c0jm4 --namespace superset
: 1500657227:0;kubectl edit deployment superset-web-3676454101-c0jm4 --namespace superset
: 1500657238:0;kubectl edit deployment superset-web --namespace superset
: 1500657287:0;kubectl edit configmap superset-web --namespace superset
: 1500657297:0;kubectl edit config_map superset-web --namespace superset
: 1500657303:0;history | grep config
: 1500657318:0;kubectl get config-maps --namespace dc-production dc-production-web -o yaml
: 1500657329:0;kubectl get configmaps --namespace dc-production dc-production-web -o yaml
: 1500657336:0;kubectl get configmaps --namespace dc-production
: 1500657370:0;kubectl get help
: 1500657376:0;kubectl get --help
: 1500657446:0;kubectl get services --namespace dc-production 
: 1500657451:0;kubectl get services --namespace dc-production -o yaml
: 1500657502:0;deis healthchecks 
: 1500657511:0;deis healthchecks --help
: 1500657518:0;deis healthchecks:list
: 1500657531:0;deis healthchecks:list -a dc-production
: 1500657549:0;deis healthchecks:list --help
: 1500657619:0;DEIS_PROFILE=production deis apps:list
: 1500689554:0;ssh ubuntu!
: 1500690108:0;kubectl get events --all-namespaces | grep dc-production
: 1500690112:0;kubectl get events --all-namespaces | grep dc-prod
: 1500690155:0;DEIS_PROFILE=production deis releases -a dc-production  | head -10
: 1500690170:0;DEIS_PROFILE=production deis apps:list -a dc-production  
: 1500691398:0;kubectl scale rc dc-production-jobs --namespace dc-production --replicas=20
: 1500691514:0;deis scale --help
: 1500691612:0;kubectl gets pods -o wide  --all-namespaces | grep dc-production-jobs | wc -l
: 1500692404:0;DEIS_PROFILE=production deis scale jobs=0 -a dc-production
: 1500692413:0;kubectl get pods -o wide  --all-namespaces | grep dc-production-jobs | wc -l 
: 1500692785:0;aws rds describe-db-instances --db-instance-identifier dc-production3-read1
: 1500692866:0;while true ; do echo "$(date)" ; date >> dc-prod3-upgrade-aws-desc.log ; aws rds describe-db-instances --db-instance-identifier dc-production3  >> dc-prod3-upgrade-aws-desc.log ; sleep 30;  done
: 1500694476:0;DEIS_PROFILE=production deis scale jobs=5 -a dc-production
: 1500694643:0;DEIS_PROFILE=production deis scale jobs=10 -a dc-production
: 1500694718:0;DEIS_PROFILE=production deis scale jobs=15 -a dc-production
: 1500694775:0;DEIS_PROFILE=production deis scale jobs=20 -a dc-production
: 1500694843:0;DEIS_PROFILE=production deis scale jobs=25 -a dc-production
: 1500694891:0;DEIS_PROFILE=production deis scale jobs=27 -a dc-production
: 1500694972:0;tail -100 dc-prod3-upgrade-aws-desc.log
: 1500694978:0;kubectl get pods -o wide  --all-namespaces | grep dc-production-jobs
: 1500695069:0;kubectl get pods -o wide  --all-namespaces | grep dc-production 
: 1500695089:0;kubectl get pods -o wide  --all-namespaces | grep dc-production | awk '{print $7}'
: 1500695095:0;kubectl get pods -o wide  --all-namespaces | grep dc-production | awk '{print $8}'
: 1500695101:0;kubectl get pods -o wide  --all-namespaces | grep dc-production | awk '{print $8}' | uniq -c
: 1500695110:0;kubectl get pods -o wide  --all-namespaces | grep dc-production | awk '{print $8}' | sort | uniq -c
: 1500905161:0;DEIS_PROFILE=production deis healthchecks:list -a dc-production
: 1500905169:0;DEIS_PROFILE=production deis healthchecks:list -a dc-production -o yaml
: 1500905172:0;DEIS_PROFILE=production deis healthchecks:list -a dc-production 
: 1500907690:0;shipit deployment logs 5391 
: 1500907707:0;shipit ssh wellbeingid-staging 
: 1500907807:0;history | grep contin
: 1500907830:0;shipit env update wellbeingid-staging --no-continuous-deployment
: 1500908219:0;DEIS_PROFILE=production deis config:list -a reporting-production
: 1500908242:0;shipit env config:list reporting-production
: 1500908251:0;shipit env config_list reporting-production
: 1500908378:0;shipit env config_set reporting-production DATABASES_DC_URL "mysql2://reporting:vK4NA4X%236ygK%29rX8k%28jazwCa@dc-production3.cduvhvvpw6oo.us-east-1.rds.amazonaws.com:3306/meyouhealth_production"
: 1500908383:0;shipit env config_list reporting-production | grep -i dc
: 1500908395:0;shipit env show reporting-production
: 1500908412:0;shipit deployment create reporting-production 5aa4054
: 1500909219:0;history | grep elasticache
: 1500909305:0;aws elasticache create-replication-group --replication-group-id wellbeingid-staging --replication-group-description "WellbeingID Staging Redis Replication Group" --primary-cluster-id wellbeingid-staging
: 1500915752:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-201-130.ec2.internal
: 1500916849:0;aws s3 ls | grep -i rails
: 1500918319:0;DEIS_PROFILE=production deis config:list -a hello200-production
: 1500918327:0;DEIS_PROFILE=production deis config:list -a hello200-production | grep -i research
: 1500918345:0;DEIS_PROFILE=production deis config:list -a hello200-production | grep -i database
: 1500918367:0;shipit env config_list reporting-production | grep -i hello
: 1500918963:0;kubectl --namespace dc-production logs dc-production-web-1690421962-383km
: 1500918968:0;kubectl --namespace dc-production logs dc-production-web-1690421962-383km --prvious
: 1500918973:0;kubectl --namespace dc-production logs dc-production-web-1690421962-383km --previous
: 1500919078:0;kubectl --namespace deis logs deis-workflow-manager-2528409207-3dh72
: 1500919092:0;kubectl --namespace deis logs deis-builder-2994006370-z2h96
: 1500919105:0;kubectl get pods -o wide  --all-namespaces | grep deis
: 1500919122:0;kubectl --namespace deis logs deis-controller-2170126687-75z52
: 1500919168:0;kubectl --namespace deis logs deis-controller-2170126687-75z52 > deis-controller-dc-prod-mid-deploy-k8s-err-log.log
: 1500919430:0;DEIS_PROFILE=production deis ps:list -a dc-production  
: 1500919721:0;kubectl get pods -o wide  --all-namespaces | grep 100.97.181.207
: 1500919730:0;kubectl get pods -o wide  --all-namespaces | grep 207
: 1500921226:0;telnet shared-staging-postgres.cduvhvvpw6oo.us-east-1.rds.amazonaws.com 5432
: 1500921236:0;ssh admin@ip-172-20-130-151.ec2.internal
: 1500921455:0;ssh ip-172-20-209-155.ec2.internal
: 1500921461:0;ssh admin@ip-172-20-209-155.ec2.internal
: 1500999062:0;aws dms describe_endpoints
: 1500999068:0;aws dms describe-endpoints 
: 1500999103:0;aws dms describe-replication-tasks
: 1500999144:0;aws dms describe-replication-tasks | grep dms-wbid-migration
: 1500999150:0;aws dms describe-replication-tasks | grep -i dms-wbid-migration
: 1501002055:0;aws dms describe-replication-instances > Documents/aws-replication-instances.07252017.txt
: 1501002060:0;vi Documents/aws-replication-instances.07252017.txt
: 1501002639:0;aws rds help
: 1501002669:0;aws rds describe-db-instances help
: 1501002704:0;aws rds describe-db-instances > Documents/aws-rds-instances-all.07252107.txt
: 1501002714:0;grep -i Backup Documents/aws-rds-instances-all.07252107.txt
: 1501002792:0;vi Documents/aws-rds-instances-all.07252107.txt
: 1501003176:0;aws rds describe-db-instances > Documents/aws-rds-instances-all.07252107-after-bkup-chgs.txt
: 1501003189:0;diff -u Documents/aws-rds-instances-all.07252107.txt Documents/aws-rds-instances-all.07252107-after-bkup-chgs.txt
: 1501005595:0;kubectl get pods -o wide  --all-namespaces | grep 0a8ce0a79be940500
: 1501010692:0;aws rds help | grep snap
: 1501010710:0;aws rds describe-db-snapshots help
: 1501010739:0;aws rds describe-db-snapshots --db-instance-identifier concierge-production
: 1501026620:0;kubectl delete pod dc-production-jobs-2890217074-2s2np --namespace dc-production
: 1501026636:0;kubectl delete pod dc-production-jobs-2890217074-xj3jb --namespace dc-production
: 1501026653:0;kubectl delete pod dc-production-web-987875160-q35w3 --namespace dc-production
: 1501026727:0;kubectl delete pod quitnet-production-jobs-3675697924-24vlc --namespace quitnet-production
: 1501026850:0;ssh admin@ip-172-24-172-178.ec2.internal
: 1501027018:0;kubectl get pods -o wide  --all-namespaces | grep -i walkadoo
: 1501027832:0;kubectl delete pod quitnet-production-conciergelistener-1056426144-s02bp --namespace quitnet-production
: 1501027882:0;kubectl delete pod quitnet-production-jobs-3675697924-6xllw --namespace quitnet-production
: 1501028287:0;kubectl --namespace deis logs deis-monitor-telegraf-q8s29
: 1501077997:0;ssh admin@walkadoo-staging-conciergelistener-1401797898-n4q2z
: 1501078027:0;kubectl describe node ip-172-20-198-39.ec2.internal
: 1501078062:0;kubectl describe node ip-172-20-198-39.ec2.internal > kubectl-desc-node-ip-172-20-198-39.ec2.internal
: 1501078066:0;ssh admin@ip-172-20-198-39.ec2.internal
: 1501078251:0;kubectl delete pod walkadoo-staging-conciergelistener-1401797898-n4q2z --namespace walkadoo-staging
: 1501078283:0;kubectl delete pod quitnet-staging-clockwork-3386973062-s2k9r --namespace quitnet-staging
: 1501078298:0;kubectl describe node ip-172-20-198-39.ec2.internal > kubectl-desc-node-ip-172-20-198-39.ec2.internal-after-2-pods-deleted
: 1501080788:0;aws rds describe-reserved-db-instances-offerings
: 1501080808:0;aws rds describe-reserved-db-instances-offerings help
: 1501080884:0;aws rds describe-reserved-db-instances-offerings > aws-rds-describe-reserved-db-instances-offerings.072617.txt
: 1501083368:0;more aws-rds-instances-all.07252107.txt
: 1501083377:0;more aws-rds-instances-all.07252107-after-bkup-chgs.txt
: 1501083397:0;grep DBInstanceClass aws-rds-instances-all.07252107-after-bkup-chgs.txt
: 1501083401:0;grep DBInstanceClass aws-rds-instances-all.07252107-after-bkup-chgs.txt | sort 
: 1501083424:0;grep DBInstanceClass aws-rds-instances-all.07252107-after-bkup-chgs.txt | sort  | awk -F ":" '{print $2}'
: 1501083428:0;grep DBInstanceClass aws-rds-instances-all.07252107-after-bkup-chgs.txt | sort  | awk -F ":" '{print $2}' | uniq -c
: 1501083894:0;kubectl delete pod quitnet-staging-houston-469244042-7w9p3 --namespace quitnet-staging
: 1501087776:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-20-198-39.ec2.internal | wc -l 
: 1501090155:0;kubectl describe node ip-172-20-198-39.ec2.internal > kubectl-desc-node-ip-172-20-198-39.ec2.internal-after-all-3-pods-deleted
: 1501090167:0;diff -u kubectl-desc-node-ip-172-20-198-39.ec2.internal-after-all-3-pods-deleted kubectl-desc-node-ip-172-20-198-39.ec2.internal
: 1501090191:0;more kubectl-desc-node-ip-172-20-198-39.ec2.internal-after-all-3-pods-deleted
: 1501090195:0;tail -20 kubectl-desc-node-ip-172-20-198-39.ec2.internal-after-all-3-pods-deleted
: 1501090202:0;tail -25 kubectl-desc-node-ip-172-20-198-39.ec2.internal-after-all-3-pods-deleted
: 1501090207:0;tail -25 kubectl-desc-node-ip-172-20-198-39.ec2.internal-after-all-3-pods-deleted > 3
: 1501090220:0;tail -25 kubectl-desc-node-ip-172-20-198-39.ec2.internal > 1
: 1501090223:0;diff -u 1 3
: 1501090325:0;history | tail -10
: 1501090331:0;history | tail -100
: 1501090338:0;history | tail -100 | grep delete
: 1501090351:0;history | tail -100 | grep delete | grep staging
: 1501090528:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-20-198-39.ec2.internal
: 1501095062:0;cd Desktop
: 1501095295:0;ls -ltra aws-reserved-instance-calc-partial-1yr-*.png
: 1501095896:0;ls -ltra aws-reserved-instance-calc-partial-1yr-*.png | awk '{print $9}'
: 1501096420:0;bc
: 1501096608:0;more aws-rds-describe-reserved-db-instances-offerings.072617.txt
: 1501096723:0;history | grep describe-reserved
: 1501120895:0;aws-info i-13c42b8b
: 1501121791:0;watch -n 5 -d 'df -B 1' 
: 1501121797:0;df -Ph  -B 1
: 1501121804:0;df -B 1
: 1501121808:0;man df -Ph 
: 1501121812:0;man df 
: 1501162682:0;kubectl get pods -o wide  --all-namespaces | grep wellbeingid
: 1501162704:0;shipit env show ellbeingid-production
: 1501162712:0;shipit env show wellbeingid-production
: 1501162731:0;shipit env show wellbeingid-staging
: 1501162848:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-187-222.ec2.internal
: 1501162875:0;kubectl describe node ip-172-24-187-222.ec2.internal > kubectl-desc-node-ip-172-24-187-222.ec2.internal
: 1501162884:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-187-222.ec2.internal > pods-node-ip-172-24-187-222.ec2.internal
: 1501162925:0;ssh admin@ip-172-24-187-222.ec2.internal
: 1501163637:0;cd ../Documents
: 1501163645:0;kubectl get pods -o wide  --all-namespaces | grep daily
: 1501163748:0;kubectl get nodes -o wide  | grep ip-172-24-187-222.ec2.internal
: 1501163871:0;kubectl --namespace dc-production logs dc-production-web-987875160-019pg
: 1501163924:0;kubectl get pods -o wide  --all-namespaces | grep dc
: 1501163964:0;kubectl get pods -o wide  --all-namespaces | grep dc-production
: 1501163984:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-156-223.ec2.internal
: 1501164074:0;ssh admin@ip-172-24-156-223.ec2.internal
: 1501164568:0;kubectl get pods -o wide  --all-namespaces | grep Running
: 1501164595:0;for node in `kubectl get nodes -o wide | awk '! /master/ {print $1}' | tail -12`\
do\
echo ${node}; ssh admin@${node}  free -m\
done
: 1501164626:0;kubectl get nodes -o wide | wc -l 
: 1501164634:0;for node in `kubectl get nodes -o wide | awk '! /master/ {print $1}' | tail -15`\
do\
echo ${node}; ssh admin@${node}  free -m\
done
: 1501164707:0;kubectl get nodes -o wide | grep ip-172-24-172-178.ec2.internal
: 1501164720:0;kubectl get pods -o wide  --all-namespaces | ip-172-24-172-178.ec2.internal
: 1501165000:0;cd wellbeingid
: 1501165005:0;cd ..
: 1501165014:0;rbenv local 2.2.0 
: 1501165015:0;knife environment show monitoring-production -a default_attributes.sensu.dashboard.password
: 1501165104:0;knife environment show monitoring-production -a default_attributes.sensu.dashboard.username
: 1501165110:0;knife environment show monitoring-production -a default_attributes.sensu.dashboard.user
: 1501165393:0;knife environment show monitoring-production 
: 1501166521:0;kubectl delete pod walkadoo-staging-gcm-1264756338-m6s9b --namespace walkadoo-staging
: 1501166534:0;kubectl delete pod walkadoo-staging-twilio-622636022-xjqc2 --namespace walkadoo-staging
: 1501166552:0;kubectl get nodes -o wide | grep ip-172-20-221-8.ec2.internal
: 1501166568:0;kuectl describe node ip-172-20-221-8.ec2.internal
: 1501166574:0;kubectl describe node ip-172-20-221-8.ec2.internal
: 1501166694:0;kubectl delete pod walkadoo-staging-gcm-1264756338-vrr21 --namespace walkadoo-staging
: 1501166742:0;kubectl get pods -o wide  --all-namespaces | grep walkadoo-staging 
: 1501166790:0;ssh admin@ip-172-20-221-8.ec2.internal free -m
: 1501166855:0;kubectl delete pod walkadoo-staging-wbidaccountchangelistener-1411304210-0pfn2 --namespace walkadoo-staging
: 1501166858:0;kubectl get pods -o wide  --all-namespaces | grep walkadoo-staging
: 1501167271:0;ssh admin@ip-172-20-221-8.ec2.internal
: 1501167339:0;kubectl get pods -o wide  --all-namespaces | grep walkadoo
: 1501167367:0;kubectl delete pod walkadoo-staging-wbidaccountchangelistener-1411304210-z4mnp --namespace walkadoo-staging
: 1501167381:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-20-221-8.ec2.internal
: 1501167666:0;ls -l k8s-monitoring
: 1501167694:0;ckubectl get nodes -o wide | awk '! /master/ {print $1}' | tail -10
: 1501167696:0;kubectl get nodes -o wide | awk '! /master/ {print $1}' | tail -10
: 1501167886:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-172-178.ec2.internal
: 1501169662:0;for node in `kubectl get nodes -o wide | awk '! /master/ {print $1}' | tail -14`\
do\
ssh admin@${node} sudo cp /tmp/k8s-monitoring /etc/cron.hourly/k8s-monitoring\
done
: 1501175886:0;aws s3 help
: 1501179564:0;aws s3 ls |grep myh.insight
: 1501179672:0;aws s3 ls myh.insight.production
: 1501179682:0;aws s3 ls myh.insight.beta
: 1501256153:0;;gdir
: 1501256161:0;cd insight
: 1501257429:0;kubectl --namespace walkadoo-production logs walkadoo-production-jobs-2178931651-kllj0
: 1501257465:0;kubectl --namespace walkadoo-production exec -ti walkadoo-production-jobs-2178931651-kllj0 /runner/init bash
: 1501258453:0;kubectl get pods -o wide  --all-namespaces | grep walkadoo-production
: 1501258531:0;kubectl --namespace reporting-production logs reporting-production-conciergelistener-3323518104-vpkt7
: 1501258539:0;kubectl get pods -o wide  --all-namespaces | grep reporting
: 1501266327:0;history | grep aws | grep elast
: 1501266346:0;aws elasticache describe-cache-clusters > Documents/elasticache-clusters-all-07282017.txt
: 1501266358:0;aws elasticache describe-cache-clusters > ~Documents/elasticache-clusters-all-07282017.txt
: 1501266366:0;aws elasticache describe-cache-clusters > ~/Documents/elasticache-clusters-all-07282017.txt
: 1501266828:0;more ~/Documents/elasticache-clusters-all-07282017.txt
: 1501266849:0;grep CacheNodeType ~/Documents/elasticache-clusters-all-07282017.txt
: 1501266854:0;grep CacheNodeType ~/Documents/elasticache-clusters-all-07282017.txt | sort
: 1501266863:0;grep CacheNodeType ~/Documents/elasticache-clusters-all-07282017.txt | sort |  uniq -c
: 1501268233:0;aws elasticache describe-cache-clusters > ~/Documents/elasticache-clusters-all-07282017-after-removal-legacy-shipit-deploy-api.txt
: 1501268254:0;grep CacheNodeType ~/Documents/elasticache-clusters-all-07282017-after-removal-legacy-shipit-deploy-api.txt
: 1501268260:0;grep CacheNodeType ~/Documents/elasticache-clusters-all-07282017-after-removal-legacy-shipit-deploy-api.txt | sort | uniq -c
: 1501269759:0;more ~/Documents/elasticache-clusters-all-07282017-after-removal-legacy-shipit-deploy-api.txt
: 1501269804:0;grep EngineVersion ~/Documents/elasticache-clusters-all-07282017-after-removal-legacy-shipit-deploy-api.txt
: 1501269809:0;grep EngineVersion ~/Documents/elasticache-clusters-all-07282017-after-removal-legacy-shipit-deploy-api.txt | sort 
: 1501269814:0;grep EngineVersion ~/Documents/elasticache-clusters-all-07282017-after-removal-legacy-shipit-deploy-api.txt | sort  | uniq -c
: 1502114537:0;aws elasticache help
: 1502114567:0;more elasticache-clusters-all-07282017-after-removal-legacy-shipit-deploy-api.txt
: 1502114642:0;head elasticache-clusters-all-07282017-after-removal-legacy-shipit-deploy-api.txt
: 1502114651:0;head -100 elasticache-clusters-all-07282017-after-removal-legacy-shipit-deploy-api.txt
: 1502114835:0;grep SecurityGroupId elasticache-clusters-all-07282017-after-removal-legacy-shipit-deploy-api.txt
: 1502114842:0;grep SecurityGroupId elasticache-clusters-all-07282017-after-removal-legacy-shipit-deploy-api.txt | sort 
: 1502114845:0;grep SecurityGroupId elasticache-clusters-all-07282017-after-removal-legacy-shipit-deploy-api.txt | sort  | uniq -c
: 1502114854:0;vi elasticache-clusters-all-07282017-after-removal-legacy-shipit-deploy-api.txt
: 1502114998:0;kubectl get pods -o wide  --all-namespaces | grep dc-prod | grep -v Running
: 1502115011:0;kubectl get nodes -o wide | grep ip-172-24-139-127.ec2.internal
: 1502115105:0;ssh admin@ip-172-24-139-127.ec2.internal
: 1502118243:0;grep "-memcache" elasticache-clusters-all-07282017-after-removal-legacy-shipit-deploy-api.txt
: 1502118252:0;grep "\-memcache" elasticache-clusters-all-07282017-after-removal-legacy-shipit-deploy-api.txt
: 1502118261:0;grep -B 50 "\-memcache" elasticache-clusters-all-07282017-after-removal-legacy-shipit-deploy-api.txt
: 1502118331:0;cp elasticache-clusters-all-07282017-after-removal-legacy-shipit-deploy-api.txt memcache-list-all-07282017.txt
: 1502118347:0;mv memcache-list-all-07282017.txt elasticache-memcache-list-all-07282017.txt
: 1502118352:0;vi elasticache-memcache-list-all-07282017.txt
: 1502118557:0;grep CacheClusterId elasticache-memcache-list-all-07282017.txt
: 1502118776:0;grep Security elasticache-memcache-list-all-07282017.txt
: 1502118969:0;aws vpc help
: 1502119304:0;grep SecurityGroupId elasticache-memcache-list-all-07282017.txt
: 1502120245:0;aws sg help
: 1502120389:0;aws ec2 describe-stale-security-groups
: 1502120447:0;aws ec2 describe-security-groups
: 1502120494:0;aws ec2 describe-security-groups > aws-ec2-describe-security-groups.08062017.txt
: 1502120505:0;grep VpcId aws-ec2-describe-security-groups.08062017.txt
: 1502120547:0;grep VpcId aws-ec2-describe-security-groups.08062017.txt | awk -F ":" '{print $2}'
: 1502120553:0;grep VpcId aws-ec2-describe-security-groups.08062017.txt | awk -F ":" '{print $2}' | sort 
: 1502120559:0;grep VpcId aws-ec2-describe-security-groups.08062017.txt | awk -F ":" '{print $2}' | sort  | uniq -c
: 1502120657:0;grep VpcId aws-ec2-describe-security-groups.08062017.txt | awk -F ":" '{print $2}' | sort  | uniq -c > vpc-id-list
: 1502120682:0;cp vpc-id-list vpc-ids-to-desc
: 1502120689:0;vi vpc-ids-to-desc
: 1502120717:0;cat vpc-ids-to-desc
: 1502120729:0;aws ec2 describe-stale-security-groups help
: 1502120743:0;aws ec2 describe-stale-security-groups --vpc-id vpc-13956976
: 1502120824:0;for in in `cat vpc-ids-to-desc`\
do\
aws ec2 describe-stale-security-groups --vpc-id $i > aws-ec2-desc-stale-security-group-output-$i.txt\
done 
: 1502120861:0;aws ec2 describe-stale-security-groups --vpc-id vpc-13956976 > aws-ec2-desc-stale-security-group-output-vpc-13956976.txt
: 1502120884:0;aws ec2 describe-stale-security-groups --vpc-id vpc-2c421b4b > aws-ec2-desc-stale-security-group-output-vpc-2c421b4b.txt
: 1502120905:0;aws ec2 describe-stale-security-groups --vpc-id vpc-cbb293ac > aws-ec2-desc-stale-security-group-output-vpc-cbb293ac.txt
: 1502120928:0;aws ec2 describe-stale-security-groups --vpc-id vpc-daeeefbd > aws-ec2-desc-stale-security-group-output-vpc-daeeefbd.txt
: 1502120955:0;aws ec2 describe-stale-security-groups --vpc-id vpc-eb706589 > aws-ec2-desc-stale-security-group-output-vpc-eb706589.txt
: 1502120967:0;more aws-ec2-desc-stale-security-group-output-vpc-13956976.txt
: 1502120973:0;more aws-ec2-desc-stale-security-group-output-vpc-2c421b4b.txt
: 1502120979:0;more aws-ec2-desc-stale-security-group-output-vpc-cbb293ac.txt
: 1502120985:0;more aws-ec2-desc-stale-security-group-output-vpc-daeeefbd.txt
: 1502120990:0;more aws-ec2-desc-stale-security-group-output-vpc-eb706589.txt
: 1502289327:0;ssh admin@ip-172-24-202-236.ec2.internal
: 1502289856:0;kubectl delete pod dc-production-jobs-11483116-b0h8z --namespace dc-production
: 1502289889:0;kubectl --namespace dc-production logs dc-production-jobs-11483116-sccq9
: 1502289912:0;kubectl --namespace dc-production logs dc-production-jobs-11483116-sccq9 --previous
: 1502289937:0;kubectl delete pod dc-production-jobs-11483116-sccq9 --namespace dc-production
: 1502289966:0;kubectl delete pod dc-production-web-73910482-z55fq --namespace dc-production
: 1502289971:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-202-236.ec2.internal
: 1502290041:0;kubectl delete pod wellbeingid-production-web-2792736409-84c9n --namespace wellbeingid-production
: 1502290064:0;kubectl delete pod insight-production-jobs-1780240644-c671s --namespace insight-production
: 1502290088:0;kubectl delete pod deis-logger-fluentd-rbkpz --namespace deis
: 1502290124:0;kubectl delete pod hello200-production-web-3761757272-6txz4 --namespace hello200-production
: 1502290147:0;kubectl delete pod deis-monitor-telegraf-vz4p1 --namespace deis
: 1502290171:0;kubectl delete pod deis-nsqd-3597503299-bndg4 --namespace deis
: 1502290205:0;kubectl delete pod kube-dns-782804071-sk79b --namespace kube-system
: 1502290228:0;kubectl delete pod hurby-server-production-web-3953324215-4d0ct --namespace hurby-server-production
: 1502290247:0;kubectl delete pod hurby-server-production-gcm-3954466293-4282c --namespace hurby-server-production
: 1502305446:0;vi reserved-instance-list
: 1502305460:0;cat reserved-instance-list
: 1502305558:0;ssh ubuntu@ip-172-16-1-117.ec2.internal
: 1502371824:0;aws cloudwatch enable-alarm-actions
: 1502371829:0;aws cloudwatch enable-alarm-actions help
: 1502371859:0;aws cloudwatch help
: 1502372103:0;kubectl delete pod dc-production-jobs-456145380-h0z5l --namespace dc-production
: 1502373382:0;history | grep deis
: 1502374258:0;kubectl --namespace deis exec deis-router-2126433040-7k78v -- bash
: 1502374287:0;kubectl --namespace deis exec deis-router-2126433040-7k78v -- cat /opt/router/conf/nginx.conf
: 1502374362:0;kubectl --namespace deis exec deis-router-2126433040-7k78v -- grep timeout /opt/router/conf/nginx.conf 
: 1502378226:0;mkdir logs/logs-ip-172-24-200-178.ec2.internal
: 1502378246:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-200-178.ec2.internal > logs/logs-ip-172-24-200-178.ec2.internal/pods-ip-172-24-200-178.ec2.internal
: 1502378268:0;kubectl get pods -o wide  --all-namespaces > logs/logs-ip-172-24-200-178.ec2.internal/all-pods.log
: 1502378282:0;kubectl get nodes -o wide > logs/logs-ip-172-24-200-178.ec2.internal/all-nodes.log
: 1502378326:0;kubectl delete pod dc-production-jobs-456145380-4jxk4 --namespace dc-production
: 1502379227:0;kubectl delte pod iris-production-jobs-1926774241-k5mkp --namespace iris-production
: 1502379236:0;kubectl delete pod iris-production-jobs-1926774241-k5mkp --namespace iris-production
: 1502379318:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-200-178.ec2.internal
: 1502379364:0;ssh admin@ip-172-24-200-178.ec2.internal
: 1502379480:0;vi logs/logs-ip-172-24-200-178.ec2.internal/systemd-journald.log
: 1502379509:0;cat logs/logs-ip-172-24-200-178.ec2.internal/systemd-journald.log
: 1502379641:0;cd logs
: 1502379648:0;cd logs-ip-172-24-200-178.ec2.internal
: 1502379654:0;more pods-ip-172-24-200-178.ec2.internal
: 1502379906:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-200-178
: 1502380129:0;delete pod deis-logger-fluentd-qjsrx --namespace deis
: 1502380139:0;kubectl delete pod deis-logger-fluentd-qjsrx --namespace deis
: 1502380153:0;vi logs/logs-ip-172-24-200-178.ec2.internal/deleted-pods.log
: 1502380192:0;cat >> logs/logs-ip-172-24-200-178.ec2.internal/deleted-pods.log
: 1502383343:0;kubectl get pods -o wide  --all-namespaces >> ~/Documents/logs/logs-ip-172-24-200-178.ec2.internal/all-pods-node-lost.log
: 1502383413:0;kubectl get events >> ~/Documents/logs/logs-ip-172-24-200-178.ec2.internal/events-node-lost.log
: 1502383427:0;kubectl get events --all-namespaces >> ~/Documents/logs/logs-ip-172-24-200-178.ec2.internal/events-node-lost.log
: 1502383466:0;kubectl get nodes -o wide  | grep ip-172-24-200-178.ec2.internal
: 1502383615:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-20-222-249
: 1502383869:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-134-157.ec2.internal
: 1502459656:0;aws help
: 1502459664:0;aws ec2 help
: 1502459690:0;aws ec2 help | grep auto
: 1502459758:0;aws autoscaling  describe-auto-scaling-groups
: 1502459788:0;aws autoscaling  describe-auto-scaling-groups > aws-ec2-autoscaling-groups-08112017.txt
: 1502461911:0;kubectl config get-contexts
: 1502463634:0;aws autoscaling  describe-auto-scaling-groups > aws-ec2-autoscaling-groups-08112017-after-cleanup.txt
: 1502463650:0;more aws-ec2-autoscaling-groups-08112017-after-cleanup.txt
: 1502463813:0;kubectl get namespaces
: 1502464708:0;kubectl delete pod wbt-staging-jobs-1484275376-n7svk --namespace wbt-staging
: 1502464724:0;kubectl delete pod wbt-staging-web-3384725398-8wp94 --namespace wbt-staging
: 1502472273:0;aws cloudwatch describe-alarm-history > aws-cloudwatch-desc-alarm-history.txt
: 1502472327:0;aws cloudwatch describe-alarms --alarm-name-prefix nodes
: 1502472387:0;aws cloudwatch describe-alarms > aws-cloudwatch-describe-alarms-generate-cli-skeleton.08112017.txt
: 1502472814:0;aws cloudwatch describe-alarms-for-metric    --region us-east-1
: 1502472828:0;aws cloudwatch describe-alarms-for-metric    --region us-east-1 --metric-name ConfigEventCount
: 1502472838:0;aws cloudwatch describe-alarms-for-metric    --region us-east-1 --metric-name ConfigEventCount --namespace AWS/CloudTrailMetrics
: 1502472854:0;aws cloudwatch describe-alarms-for-metric    --region us-east-1 --metric-name ConfigEventCount                                                                                                                  127 âƒ¦µ
: 1502472854:0;usage: aws [options] <command> <subcommand> [<subcommand> ...] [parameters]
: 1502472861:0;aws cloudwatch describe-alarms-for-metric    --region us-east-1 --metric-name ConfigEventCount                                                                                                                  127 âƒ¦µ help
: 1502472898:0;aws cloudwatch describe-alarms-for-metric  --region us-east-1 --metric-name BurstBalance                                                                                                     
: 1502472909:0;aws cloudwatch describe-alarms-for-metric  --region us-east-1 --metric-name BurstBalance                                                                                                      --namespace AWS/CloudTrailMetrics
: 1502472932:0;aws cloudwatch describe-alarms-for-metric --region us-east-1 --metric-name BurstBalance
: 1502473013:0;aws cloudwatch describe-alarms-for-metric  --metric-name BurstBalance --namespace AWS/EBS
: 1502473023:0;aws cloudwatch describe-alarms-for-metric  --metric-name BurstBalance 
: 1502473053:0;aws cloudwatch describe-alarms-for-metric 
: 1502473057:0;aws cloudwatch describe-alarms-for-metric help
: 1502473069:0;aws cloudwatch describe-alarms-for-metric --namespace "AWS/EBS"
: 1502473080:0;aws cloudwatch describe-alarms-for-metric --namespace AWS/EBS
: 1502473099:0;more aws-cloudwatch-desc-alarm-history.txt
: 1502473132:0;more aws-cloudwatch-describe-alarms-generate-cli-skeleton.08112017.txt
: 1502474171:0;aws cloudwatch describe-alarms help
: 1502474240:0;aws cloudwatch describe-alarms --generate-cli-skeleton
: 1502474618:0;	aws cloudwatch describe-alarms-for-metric  --region us-east-1  --metric-name  BurstBalance  --namespace AWS/EBS
: 1502676684:0;history | grep deployment-api
: 1502676687:0;history | grep deployment
: 1502676698:0;kubectl get deployment --namespace kube-system -o yaml
: 1502676741:0;kubectl get deployment --namespace wilder-staging-web -o yaml
: 1502676760:0;kubectl get deployment --namespace walkadoo-staging-web -o yaml
: 1502676785:0;kubectl get config_map --namespace walkadoo-staging -o yaml
: 1502676793:0;kubectl get configmap --namespace walkadoo-staging -o yaml
: 1502676800:0;history | grep configmap
: 1502676841:0;kubectl edit configmap iris-staging-web --namespace iris-staging
: 1502676872:0;kubectl edit configmap quitnet-staging-web --namespace quitnet-staging
: 1502676880:0;kubectl edit configmap quitnet-staging --namespace quitnet-staging
: 1502676895:0;history | grep configmap | grep stagingb
: 1502676897:0;history | grep configmap | grep staging
: 1502676993:0;kubectl get deployment --namespace walkadoo-staging -o yaml
: 1502676998:0;kubectl get deployment --namespace walkadoo-staging -o yaml | more
: 1502677378:0;kubectl get configmap --namespace prometheus -o yaml | more
: 1502677441:0;kubectl get deployment --namespace prometheus -o yaml | more
: 1502677561:0;kubectl get services --namespace prometheus -o yaml | more
: 1502699197:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-148-226.ec2.internal
: 1502699267:0;ssh admin@ip-172-24-148-226.ec2.internal
: 1502699548:0;ssh admin@ip-172-24-219-242.ec2.internal
: 1502709715:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-219-242.ec2.internal
: 1502709723:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-188-56.ec2.internal
: 1502718197:0;ls -l *cloud*
: 1502718207:0;more cloudwatch-describe-alarms-all.052017.txt
: 1502718269:0;history | grep cloud
: 1502718279:0;aws cloudwatch describe-alarms 
: 1502718301:0;aws cloudwatch describe-alarms  > cloudwatch-describe-alarms-all.08142017.txt
: 1502718311:0;grep awsebs cloudwatch-describe-alarms-all.08142017.txt
: 1502718322:0;grep AlarmName cloudwatch-describe-alarms-all.08142017.txt
: 1502718332:0;grep AlarmName cloudwatch-describe-alarms-all.08142017.txt > alarm-names-list
: 1502718526:0;brew update terraform
: 1502718595:0;cd myh-terraform
: 1502718602:0;cd environments
: 1502718755:0;cd hello200
: 1502718757:0;terraform get
: 1502718793:0;cd staging/hello200
: 1502718798:0;more terraform.tfvars
: 1502718823:0;vi aws-cloudwatch-alarm.tf
: 1502718881:0;more cloudwatch-describe-alarms-all.08142017.txt
: 1502718888:0;ls -lotra
: 1502718891:0;more alarm-names-list
: 1502718953:0;cp terraform.tfstate terraform.tfstate.h200
: 1502718977:0;more aws-cloudwatch-alarm.tf
: 1502719048:0;terraform import awsebs-vol-00b0243d7721f67e9-High-Burst-Balance i-04ba0822ec4492869
: 1502719094:0;terraform import awsebs-vol-00b0243d7721f67e9-High-Burst-Balance
: 1502719167:0;cat aws-cloudwatch-alarm.tf
: 1502719924:0;aws autoscaling help
: 1502719969:0;aws autoscaling describe-lifecycle-hook-types help
: 1502720072:0;aws cloudwatch list-metrics --namespace "AWS/AutoScaling"
: 1502720078:0;aws cloudwatch list-metrics
: 1502720092:0;aws cloudwatch list-metrics |\
 grep -i burst
: 1502720131:0;aws cloudwatch list-metrics --metricname "BurstBalance"
: 1502723174:0;aws autoscaling enable-metrics-collection help
: 1502724288:0;aws cloud formation get-template
: 1502724331:0;pip install --upgrade awscli
: 1502724348:0;aws cloudformation get-template
: 1502724355:0;aws cloudformation get-template help
: 1502724452:0;aws cloudformation get-template --stack-name wellbeingid-production
: 1502724793:0;kubectl --namespace=deis describe svc deis-router
: 1502726159:0;kubectl get deployments --all-namespaces
: 1502726213:0;kubectl get configmaps --all-namespaces
: 1502726286:0;kubectl get deployments --namespace wbt-production -o yaml 
: 1502726294:0;kubectl get deployments --namespace wbt-production -o yaml  | more
: 1502726341:0;kubectl get deployments --namespace wbt-production 
: 1502726370:0;kubectl get deployments wbt-production-web --namespace wbt-production -o yaml 
: 1502726465:0;kubectl describe deployments --namespace wbt-production
: 1502726555:0;kubectl describe deployments --namespace dc-production
: 1502726591:0;kubectl get rs --all-namespaces
: 1502726658:0;kubectl rollout history deployment/dc-production 
: 1502726690:0;kubectl describe deployment
: 1502726697:0;kubectl describe deployment --namespace dc-production
: 1502726736:0;kubectl rollout history deployment/dc-production-web --namespace dc-production
: 1502726876:0;kubectl autoscale --help
: 1502730386:0;kops get instancegroup nodes
: 1502737947:0;kubectl get pods -o wide  --all-namespaces | grep  ip-172-20-222-249.ec2.internal
: 1502737967:0;kubectl get pods -o wide  --all-namespaces | grep  ip-172-20-222-249.ec2.internal | wc -l 
: 1502737984:0;kubectl delete pod wbt-staging-clockwork-1284850969-khtkh --namespace wbt-staging
: 1502738001:0;kubectl delete pod wbt-staging-web-2834156923-dzp96 --namespace wbt-staging
: 1502738010:0;kubectl get nodes -o wide | grep ip-172-20-222-249.ec2.internal
: 1502738192:0;ssh admin@ip-172-20-222-249.ec2.internal
: 1502804889:0;;kci
: 1502805055:0;history | grep kops
: 1502805064:0;kops update cluster
: 1502805562:0;kops get instancegroup nodes -o yaml
: 1502814090:0;kubectl --namespace quitnet-staging logs quitnet-staging-rabbitmq-1867388880-1tqb2
: 1502814094:0;kubectl --namespace quitnet-staging logs quitnet-staging-rabbitmq-1867388880-1tqb2 --previous
: 1502816993:0;kops edit instancegroup nodes 
: 1502817014:0;kops update cluster 
: 1502817077:0;kops update cluster --yes
: 1502820307:0;kubectl get nodes -o wide | grep ip-172-24-196-37.ec2.internal
: 1502820329:0;kubectl describe node ip-172-24-221-201.ec2.internal
: 1502820360:0;ssh admin@ip-172-24-221-201.ec2.internal
: 1502821980:0;cd iris
: 1502822000:0;ls -lrtra vendor/cache
: 1502822018:0;ls -l *new*
: 1502822020:0;ls
: 1502822026:0;ls -lrtra vendor/cache/newrelic_rpm-4.3.0.335.gem
: 1502822031:0;more vendor/cache/newrelic_rpm-4.3.0.335.gem
: 1502822039:0;file vendor/cache/newrelic_rpm-4.3.0.335.gem
: 1502822050:0;strings vendor/cache/newrelic_rpm-4.3.0.335.gem
: 1502822060:0;tar -tvf vendor/cache/newrelic_rpm-4.3.0.335.gem
: 1502822097:0;tar -tvf vendor/cache/sidekiq-*
: 1502822107:0;file vendor/cache/sidekiq-*
: 1502822153:0;kubectl get pods -o wide  --all-namespaces 
: 1502822366:0;kubectl get nodes -o wide | grep d
: 1502822430:0;kubectl get events | more
: 1502822451:0;ssh admin@ip-172-24-179-142.ec2.internal
: 1502822456:0;kubectl get nodes -o wide | grep ip-172-24-179-142.ec2.internal
: 1502823362:0;cd vendor
: 1502823365:0;cd cache
: 1502823367:0;file *
: 1502823781:0;kubectl get nodes -o wide | grep -v master
: 1502823828:0;kubectl get events
: 1502823920:0;kubectl describe node ip-172-24-162-176.ec2.internal
: 1502824206:0;ssh admin@ip-172-24-170-138.ec2.internal
: 1502824296:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-170-138.ec2.internal
: 1502824339:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-162-238.ec2.internal
: 1502824421:0;kubectl describe node ip-172-24-162-238.ec2.internal
: 1502824714:0;history | grep drain
: 1502824731:0;kubectl drain ip-172-24-162-238.ec2.internal 
: 1502824770:0;kubectl drain ip-172-24-162-238.ec2.internal --force 
: 1502825583:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-183-127.ec2.internal
: 1502826105:0;kubectl get pods -o wide  --all-namespaces | grep dc-prod | awk '{print $7}'
: 1502826110:0;kubectl get pods -o wide  --all-namespaces | grep dc-prod | awk '{print $8}'
: 1502826191:0;kubectl get pods -o wide  --all-namespaces | grep dc-prod | awk '{print $8}' | sort | uniq -c | grep ip-172-24-183-127
: 1502826238:0;kubectl get pods -o wide  --all-namespaces | grep dc-prod | awk '{print $8}' | sort | uniq -c
: 1502890766:0;history | grep prome
: 1502891207:0;history | grep nslookup
: 1502891355:0;kubectl get pods -o wide  --all-namespaces | grep dns
: 1502891434:0;kubectl exec -ti walkadoo-production-web-1595373713-xtrzx --namespace walkadoo-production nslookup prometheus-pushgateway.prometheus.svc.cluster.local
: 1502891555:0;kubectl edit configmap prometheus-config --namespace prometheus
: 1502891616:0;kubectl get configmap prometheus-config --namespace prometheus -o yaml
: 1502892036:0;kubectl describe node ip-172-24-137-213.ec2.internal
: 1502892049:0;ssh admin@ip-172-24-137-213.ec2.internal
: 1502892735:0;DEIS_PROFILE=production deis releases -a dc-production  
: 1502892770:0;kubectl drain ip-172-24-137-213.ec2.internal
: 1502893871:0;kubectl delete pod dc-production-jobs-464402916-hfwqz --namespace dc-production
: 1502893885:0;kubectl delete pod dc-production-jobs-464402916-z25t5 --namespace dc-production
: 1502893898:0;kubectl delete pod dc-production-web-1715194570-gj82m --namespace dc-production
: 1502894313:0;kubectl delete pod quitnet-production-clockwork-1093647556-xsr57 --namespace quitnet-production
: 1502894383:0;kubectl delete pod quitnet-production-houston-2325789128-1zh6c --namespace quitnet-production
: 1502894403:0;kubectl delete pod quitnet-production-web-2334016821-fj34k --namespace quitnet-production
: 1502894426:0;kubectl delete pod wellbeingid-production-web-2761741987-xs96n --namespace wellbeingid-production
: 1502894553:0;kubectl delete pod iris-production-clockwork-4077864617-tqbn8 --namespace iris-production
: 1502894823:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-137-213.ec2.internal
: 1502895863:0;kubectl drain ip-172-24-183-16.ec2.internal 
: 1502896015:0;kubectl delete pod dc-production-jobs-464402916-1cj0w --namespace dc-production
: 1502896033:0;kubectl delete pod dc-production-web-1715194570-mwv4q --namespace dc-production
: 1502896054:0;kubectl delete pod dcmobile-production-web-2155079255-pprwz --namespace dcmobile-production
: 1502896081:0;kubectl delete pod hello200-production-web-3761757272-87dsc --namespace hello200-production
: 1502896098:0;kubectl delete pod insight-production-clockwork-1412258274-qds08 --namespace insight-production
: 1502896114:0;kubectl delete pod insight-production-jobs-2385203550-9pdf1 --namespace insight-production
: 1502896128:0;kubectl delete pod insight-production-web-3742229060-zkt88 --namespace insight-production
: 1502896150:0;kubectl delete pod quitnet-production-rabbitmq-733723344-hv2px --namespace quitnet-production
: 1502896169:0;kubectl delete pod superset-web-3676454101-jc1lm --namespace superset
: 1502896184:0;kubectl delete pod wilder-production-clockwork-1942846971-5k5sn --namespace wilder-production
: 1502896221:0;kubectl delete pod wilder-production-web-832505452-m84xx --namespace wilder-production
: 1502896252:0;kubectl delete pod elasticsearch-production-cmd-1853344589-z81zc --namespace elasticsearch-production
: 1502896270:0;kubectl get pods -o wide  --all-namespaces | grep -v Running 
: 1502896310:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-183-16.ec2.internal
: 1502896434:0;kubectl get nodes -o wide  | grep ip-172-24-181-10.ec2.internal
: 1502896447:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-181-10.ec2.internal
: 1502896864:0;kubectl describe node ip-172-24-196-85.ec2.internal
: 1502896885:0;ssh admin@ip-172-24-196-85.ec2.internal
: 1502896895:0;kubectl drain ip-172-24-196-85.ec2.internal
: 1502896924:0;kubectl delete pod dc-production-assignment-4119086096-cwz4d --namespace dc-production
: 1502896938:0;kubectl delete pod dc-production-jobs-464402916-2sjjz --namespace dc-production
: 1502896951:0;kubectl delete pod dc-production-web-1715194570-bp1t5 --namespace dc-production
: 1502896966:0;kubectl delete pod dc-production-web-1715194570-rn8tf --namespace dc-production
: 1502896982:0;kubectl delete pod dcmobile-production-web-2155079255-gbnmg --namespace dcmobile-production
: 1502897020:0;kubectl delete pod hello200-production-wbidaccountchangelistener-1531352054-k85hg --namespace hello200-production
: 1502897039:0;kubectl delete pod iris-production-clockwork-4077864617-tk1qt --namespace iris-production
: 1502897055:0;kubectl delete pod walkadoo-production-conciergelistener-2697588748-95dkp --namespace walkadoo-production
: 1502897175:0;kubectl delete pod wilder-production-web-832505452-3sj7r --namespace wilder-production
: 1502897197:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-196-85.ec2.internal
: 1502897845:0;kubectl drain ip-172-24-150-211.ec2.internal
: 1502897884:0;kubectl delete pod dc-production-jobs-464402916-bv1z7 --namespace dc-production
: 1502897901:0;kubectl delete pod dc-production-jobs-464402916-jrwbr --namespace dc-production
: 1502897918:0;kubectl delete pod dc-production-web-1715194570-vvpwz --namespace dc-production
: 1502897935:0;kubectl delete pod dc-production-web-1715194570-z5fqj --namespace dc-production
: 1502897962:0;kubectl delete pod dwh-production-conciergelistener-2904721338-4h4r0 --namespace dwh-production
: 1502897993:0;kubectl delete pod hurby-server-production-web-3953324215-zw9rc --namespace hurby-server-production
: 1502898012:0;kubectl delete pod iris-production-jobs-883179045-2dpw7 --namespace iris-production
: 1502898026:0;kubectl delete pod iris-production-web-3060649739-wf1z7 --namespace iris-production
: 1502898044:0;kubectl delete pod prometheus-1575380483-0thq9 --namespace prometheus
: 1502898061:0;kubectl delete pod reporting-production-clockwork-1347577415-g49ls --namespace reporting-production
: 1502898076:0;kubectl delete pod reporting-production-conciergelistener-189100895-rp153 --namespace reporting-production
: 1502898094:0;kubectl delete pod reporting-production-web-3115839146-zl14k --namespace reporting-production
: 1502898115:0;kubectl delete pod wellbeingid-production-web-3751513322-07t84 --namespace wellbeingid-production
: 1502898655:0;ssh admin@ip-172-24-150-211.ec2.internal
: 1502898696:0;more k8s-monitoring
: 1502898969:0;kubectl get nodes -o wide | grep ip-172-24-150-211.ec2.internal
: 1502898981:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-150-211.ec2.internal
: 1502904144:0;kubectl drain ip-172-24-145-137.ec2.internal
: 1502912354:0;kubectl delete pod dc-production-jobs-464402916-3c4kc --namespace dc-production
: 1502912372:0;kubectl delete pod dc-production-jobs-464402916-ghf0p --namespace dc-production
: 1502912412:0;kubectl delete pod dc-production-web-1715194570-5lwbj --namespace dc-production
: 1502912432:0;kubectl delete pod dc-production-web-1715194570-nwlch --namespace dc-production
: 1502912449:0;kubectl delete pod hello200-production-clockwork-2425911734-8n2kh --namespace hello200-production
: 1502912468:0;kubectl delete pod quitnet-production-web-1753105796-8k5nh --namespace quitnet-production
: 1502913068:0;kubectl delete pod walkadoo-production-houston-252960760-g037s --namespace walkadoo-production
: 1502913088:0;kubectl delete pod walkadoo-production-twilio-362339864-38bcl --namespace walkadoo-production
: 1502913104:0;kubectl delete pod walkadoo-production-wbidaccountchangelistener-2589982004-tc722 --namespace walkadoo-production
: 1502913123:0;kubectl delete pod walkadoo-production-web-2891017061-pcd17 --namespace walkadoo-production
: 1502913139:0;kubectl delete pod wilder-production-jobs-750482807-khjvl --namespace wilder-production
: 1502913323:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-145-137.ec2.internal
: 1502913405:0;ssh admin@ip-172-24-183-127.ec2.internal
: 1502914838:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-146-120.ec2.internal
: 1502914925:0;kubectl drain ip-172-24-144-87.ec2.internal 
: 1502914956:0;kubectl delete pod dc-production-jobs-464402916-dkzhd dc-production
: 1502914966:0;kubectl delete pod dc-production-jobs-464402916-dkzhd --namespace dc-production
: 1502914981:0;kubectl delete pod dc-production-jobs-464402916-s575n --namespace dc-production
: 1502914994:0;kubectl delete pod dc-production-web-1715194570-23tnf --namespace dc-production
: 1502915009:0;kubectl delete pod dc-production-web-1715194570-820zq --namespace dc-production
: 1502915030:0;kubectl delete pod dwh-production-jobs-3927274014-3db43 --namespace dwh-production
: 1502915064:0;kubectl delete pod hello200-production-web-3761757272-rg3n2 --namespace hello200-production
: 1502915086:0;kubectl delete pod hurby-server-production-houston-1288594777-lfcmh --namespace hurby-server-production
: 1502915101:0;kubectl delete pod prometheus-pushgateway-2921373215-dls4r --namespace prometheus
: 1502915118:0;kubectl delete pod reporting-production-jobs-2704956867-cg1th --namespace reporting-production
: 1502915142:0;kubectl delete pod walkadoo-production-gcm-3044923540-th9ms --namespace walkadoo-production
: 1502915157:0;kubectl delete pod wellbeingid-production-web-4100754786-hnnq8 --namespace wellbeingid-production
: 1502915164:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-144-87.ec2.internal
: 1502976132:0;for node in `kubectl get nodes -o wide | awk '! /master/ {print $1}' | tail -15`\
do\
scp k8s-monitoring admin@${node}:/tmp/k8s-monitoring\
done
: 1502976164:0;kubectl get nodes -o wide | awk '! /master/ {print $1}' | tail -15
: 1502976168:0;kubectl get nodes -o wide | awk '! /master/ {print $1}' | tail -14
: 1502976172:0;kubectl get nodes -o wide | awk '! /master/ {print $1}' | tail -13
: 1502976198:0;for node in `kubectl get nodes -o wide | awk '! /master/ {print $1}' | tail -12`\
do\
ssh admin@${node} sudo cp /tmp/k8s-monitoring /etc/cron.hourly/k8s-monitoring\
done
: 1502977311:0;kubectl get nodes -o wide | awk '! /master/ {print $1}' | tail -12
: 1502977319:0;kubectl get nodes -o wide | awk '! /master/ {print $1}' | tail -9
: 1502977323:0;kubectl get nodes -o wide | awk '! /master/ {print $1}' | tail -8
: 1502977327:0;kubectl get nodes -o wide | awk '! /master/ {print $1}' | tail -7
: 1502977341:0;for node in `kubectl get nodes -o wide | awk '! /master/ {print $1}' | tail -7`\
do\
scp k8s-monitoring admin@${node}:/tmp/k8s-monitoring\
done
: 1502977378:0;for node in `kubectl get nodes -o wide | awk '! /master/ {print $1}' | tail -7`\
do\
ssh admin@${node} sudo cp /tmp/k8s-monitoring /etc/cron.hourly/k8s-monitoring\
done
: 1502979848:0;more .aws/credentials
: 1502979890:0;cd .aws
: 1502979909:0;cp credentials creds-orig-081717.bkup 
: 1502980152:0;git clone git@github.com:meyouhealth/dailychallenge.git
: 1503065738:0;kubectl --namespace quitnet-staging logs quitnet-staging-web-3010372284-6qfrz
: 1503065754:0;kubectl --namespace quitnet-staging logs quitnet-staging-web-3010372284-6qfrz --show-timestamps
: 1503065772:0;kubectl --namespace quitnet-staging logs quitnet-staging-web-3010372284-6qfrz --timestamps
: 1503065859:0;kubectl delete pod quitnet-staging-web-3010372284-6qfrz --namespace quitnet-staging
: 1503065863:0;kubectl delete quitnet-staging-web-3010372284-6qfrz --namespace quitnet-staging
: 1503065866:0;kubectl get pods -o wide  --all-namespaces | grep quitnet 
: 1503069246:0;DEIS_PROFILE=production deis config:list -a walkadoo-production | grep -i database
: 1503069250:0;DEIS_PROFILE=production deis config:list -a walkadoo-production | grep -i mem
: 1503069270:0;DEIS_PROFILE=production deis config:list -a walkadoo-production | grep -i red
: 1503073094:0;DEIS_PROFILE=production deis config:list -a insight-production | grep -i red
: 1503073125:0;kubectl delete pod insight-production-jobs-2385203550-t6l44 --namespace insight-production
: 1503073165:0;kubectl delete pod insight-production-conciergelistener-2892457722-tdwg7 --namespace insight-production
: 1503073201:0;kubectl delete pod insight-production-clockwork-1412258274-5k4w1 --namespace insight-production
: 1503073253:0;kubectl delete pod insight-production-web-3742229060-29nt7 --namespace insight-production
: 1503073275:0;kubectl delete pod insight-production-web-3742229060-qs0lr --namespace insight-production
: 1503073281:0;kubectl get pods -o wide  --all-namespaces | grep insight
: 1503078480:0;kubectl get deployments etcd-server-events --namespace kube-system -o yaml 
: 1503078488:0;kubectl get deployments etcd-server --namespace kube-system -o yaml 
: 1503078499:0;kubectl get deployments  --namespace kube-system 
: 1503078534:0;kubectl get deployments tiller-deploy  --namespace kube-system -o yaml
: 1503079248:0;kubectl get pods -o wide  --all-namespaces | grep etcd
: 1503079342:0;bg
: 1503324828:0;kubectl get pods -o wide  --all-namespaces | grep etc
: 1503324853:0;kubectl --namespace kube-system logs etcd-server-events-ip-172-24-211-38.ec2.internal
: 1503324871:0;kubectl --namespace kube-system logs etcd-server-events-ip-172-24-143-203.ec2.internal
: 1503324885:0;kubectl --namespace kube-system logs etcd-server-ip-172-24-143-203.ec2.internal
: 1503324907:0;kubectl get services --all-namespaces
: 1503326727:0;more ~/.aws/credentials
: 1503326944:0;env
: 1503326962:0;env | grep -i AWS
: 1503326976:0;cd 
: 1503327007:0;more .zshrc
: 1503329334:0;cd .aws/
: 1503329379:0;cd Download
: 1503329381:0;cd Downloads
: 1503329401:0;ls -ltrah
: 1503329424:0;mv "accessKeys(1).csv" dc-stg-access-keys.csv
: 1503329430:0;more dc-stg-access-keys.csv
: 1503329944:0;grep AKIAIEL4OQX3GHWLGGKA credentials
: 1503330013:0;ls -l accessKeys*
: 1503330027:0;rm accessKeys*1*.csv
: 1503330031:0;rm accessKeys.csv
: 1503330082:0;vi credentials
: 1503330606:0;kubectl get events --all-namespaces | grep EvictionThresholdMet
: 1503330684:0;kubectl --namespace deis logs deis-logger-fluentd-q03lg --previous
: 1503336655:0;which terraform
: 1503336660:0;terraform --version
: 1503336670:0;brew upgrade terraform
: 1503338007:0;cat redis.tf
: 1503338306:0;ssh admin@ip-172-20-141-204.ec2.internal
: 1503338481:0;vi check-stg-hosts-k8s-issue
: 1503338492:0;cat check-stg-hosts-k8s-issue
: 1503338508:0;cat check-stg-hosts-k8s-issue | grep ip-172-20-141-204.ec2.internal
: 1503338603:0;ssh admin@ip-172-20-152-59.ec2.internal
: 1503338733:0;kubectl get events | grep ip-172-20-141-204
: 1503338798:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-20-141-204.ec2.internal
: 1503342369:0;cd staging
: 1503342388:0;cat variables.tf
: 1503342458:0;more README.md
: 1503343140:0;cp README.md /tmp
: 1503343145:0;ls -l ./tmp/README.md
: 1503343149:0;more /tmp/README.md
: 1503343176:0;history | grep git 
: 1503343191:0;history > hist
: 1503343193:0;vi hist
: 1503343354:0;git -branch documentation-updates
: 1503343364:0;git checkout -branch documentation-updates
: 1503343371:0;git checkout -b documentation-updates
: 1503343385:0;cp /tmp/README.md .
: 1503343390:0;git add README.md
: 1503343422:0;git diffed
: 1503343424:0;git diff
: 1503343500:0;git push origin documentation-updates
: 1503343748:0;git checkout origin
: 1503357998:0;kubectl delete pod dashiell-web-691912494-2lf4d --namespace dashiell
: 1503358040:0;rbenv list
: 1503358044:0;rbenv show
: 1503358048:0;rbenv --help
: 1503358063:0;rbenv versions
: 1503358221:0;kubectl --namespace dashiell logs dashiell-web-691912494-hvd09 
: 1503358237:0;~ â€¹2.1.5â€º  $ kubectl get pods -o wide  --all-namespaces | grep -v Running
: 1503358237:0;NAMESPACE               NAME                                                          READY     STATUS             RESTARTS   AGE       IP                NODE
: 1503358237:0;dashiell                dashiell-web-691912494-hvd09                                  0/1       CrashLoopBackOff   4          2m        100.105.0.8       ip-172-20-217-78.ec2.internal
: 1503358237:0;~ â€¹2.1.5â€º  $ kubectl --namespace dashiell logs dashiell-web-691912494-hvd09
: 1503358237:0;bundler: failed to load command: thin (/app/vendor/bundle/ruby/2.3.0/bin/thin)
: 1503358237:0;NameError: uninitialized constant ActiveSupport::Autoload
: 1503358262:0;kubectl --namespace dashiell logs dashiell-web-691912494-hvd09 > Documents/dashiell-stg-err-codes
: 1503358578:0;kubectl exec -ti dashiell-web-691912494-hvd09 --namespace dashiell /bin/sh 
: 1503358601:0;kubectl exec -ti dashiell-web-691912494-hvd09 --namespace dashiell bash
: 1503358671:0;kubectl exec -ti dashiell-web-691912494-hvd09 --namespace dashiell bundle exec 
: 1503358766:0;more Documents/dashiell-stg-err-codes
: 1503359361:0;history | grep kubectl | grep bash
: 1503359377:0;kubectl get pods -o wide  --all-namespaces |grep -v Running
: 1503359418:0;kubectk exec dashiell-web-691912494-hvd09 -i -t bash --namespace dashiell
: 1503359424:0;kubectl exec dashiell-web-691912494-hvd09 -i -t bash --namespace dashiell
: 1503359444:0;kubectl exec dashiell-web-691912494-hvd09 -i -t bash -il --namespace dashiell
: 1503359475:0;kubectl exec dashiell-web-691912494-hvd09 date
: 1503409599:0;kubectl exec dashiell-web-691912494-hvd09 date --namespace dashiell
: 1503409616:0;kubectl exec dashiell-web-4220770733-l0jkj date --namespace dashiell
: 1503409629:0;kubectl get pods -o wide  --all-namespaces | grep dash
: 1503409656:0;kubectl exec kubernetes-dashboard-3203831700-6kscs -i -t bash -il --namespace dashiell
: 1503409662:0;kubectl exec kubernetes-dashboard-3203831700-6kscs -i -t bash  --namespace dashiell
: 1503409740:0;kubectl exec -it kubernetes-dashboard-3203831700-6kscs -- /bin/bash --namespace dashiell
: 1503409771:0;kubectl --namespace kubernetes-dashboard-3203831700-6kscs logs kubernetes-dashboard-3203831700-6kscs
: 1503409832:0;kubectl exec dashiell-web-4220770733-l0jkj -i -t bash  --namespace dashiell
: 1503409893:0;kubectl exec -it dashiell-web-4220770733-l0jkj -- /bin/sh --namespace dashiell
: 1503409970:0;kubectl exec -it dashiell-web-4220770733-l0jkj -- /bin/bash
: 1503409994:0;kubectl exec -it dashiell-web-4220770733-l0jkj -- /bin/bash --namespace dashiell
: 1503410006:0;history | grep walkadoo | grep exec
: 1503410045:0;kubectl --namespace dashiell exec -ti dashiell-web-4220770733-l0jkj /runner/init bash 
: 1503410068:0;kubectl --namespace dashiell exec -ti dashiell-web-4220770733-l0jkj nslookup prometheus-pushgateway.prometheus.svc.cluster.local
: 1503410090:0;kubectl get pods -o wide  --all-namespaces | head -1 
: 1503410121:0;kubectl get pods -o wide  --all-namespaces | grep walkadoo-production-jobs
: 1503410145:0;kubectl --namespace walkadoo-production exec -ti walkadoo-production-jobs-2193742787-frxdc /runner/init bash
: 1503410160:0;kubectl --namespace walkadoo-production exec -ti walkadoo-production-jobs-2193742787-frxdc --  bash
: 1503410201:0;kubectl --namespace dashiell exec -ti dashiell-web-4220770733-l0jkj --  bash
: 1503410214:0;kubectl --namespace dashiell logs dashiell-web-4220770733-l0jkj
: 1503410241:0;DEIS_PROFILE=staging deis ps:list -a dashiell
: 1503410254:0;DEIS_PROFILE=staging deis config:list -a dashiell
: 1503410327:0;history | grep MAINT
: 1503410346:0;DEIS_PROFILE=staging deis config:set dashiell MAINTENANCE_MODE true
: 1503410381:0;DEIS_PROFILE=staging deis config:set -a dashiell MAINTENANCE_MODE=trie
: 1503410572:0;DEIS_PROFILE=staging deis config:set -a dashiell MAINTENANCE_MODE=true
: 1503410596:0;kubectl describe pod dashiell-web-1830249032-7cgsc
: 1503410608:0;kubectl describe pod dashiell-web-1830249032-7cgsc --namespace dashiell
: 1503410637:0;kubectl get pods -o wide  --namespace dashiell
: 1503410661:0;kubectl --namespace deis dashiell dashiell-web-1830249032-7cgsc
: 1503410671:0;kubectl --namespace dashiell dashiell-web-1830249032-7cgsc
: 1503410680:0;kubectl --namespace dashiell logs dashiell-web-1830249032-7cgsc
: 1503410852:0;kubectl exec dashiell-web-3542180381-mz6xh -i -t bash --namespace dashiell
: 1503410866:0;kubectl get pods -o wide  --all-namespaces | grep walkadoo-staging-jobs
: 1503410886:0;kubectl exec walkadoo-staging-jobs-417114302-gg2pg -i -t bash --namespace walkadoo-staging
: 1503411045:0;DEIS_PROFILE=staging deis config:unset -a dashiell MAINTENANCE_MODE
: 1503411118:0;helm -l 
: 1503411122:0;helm list
: 1503411290:0;cd git
: 1503411312:0;git clone git@github.com:meyouhealth/dashiell.git
: 1503411371:0;cd dashiell
: 1503411373:0;ls -ltr
: 1503411910:0;cat README.md
: 1503411950:0;history | grep deis | grep push
: 1503411968:0;pwd
: 1503412004:0;deis git:remote --help
: 1503412028:0;git remote 
: 1503412064:0;deis --help
: 1503412071:0;deis keys --help
: 1503412082:0;deis keys:add --help
: 1503412197:0;kubectl --namespace deis logs deis-builder-848547108-1s98s
: 1503412286:0;ls -l ~/.ssh/
: 1503412315:0;cat .git/config
: 1503412456:0;DEIS_PROFILE=staging
: 1503412506:0;export DEIS_PROFILE=staging
: 1503412513:0;echo $DEIS_PROFILE
: 1503412517:0;deis apps
: 1503412530:0;git remote --help
: 1503412540:0;git remote remove staging
: 1503412554:0;deis git:remote -a dashiell -r staging 
: 1503412571:0;deis keys:add
: 1503412589:0;git remote
: 1503412621:0;git push staging 
: 1503412634:0;git push staging --force
: 1503412724:0;DEIS_PROFILE=staging deis releases -a dashiell
: 1503412792:0;kubectl --namespace dashiell logs dashiell-web-2648758911-qq4mp
: 1503412922:0;kubectl get pods -o wide  --all-namespaces | grep builder
: 1503412934:0;kubectl --namespace deis logs deis-builder-848547108-1s98s --timestamp
: 1503412938:0;kubectl --namespace deis logs deis-builder-848547108-1s98s --timestamps
: 1503413143:0;more Gemfile.lock | grep activesuppo
: 1503413466:0;more Gemfile
: 1503413782:0;kubectl exec dashiell-web-2648758911-qq4mp -i -t bash --namespace dashiell
: 1503413790:0;kubectl get pods -o wide  --all-namespaces | grep dashiell
: 1503413805:0;kubectl exec dashiell-web-4051950249-pd7wv -i -t bash --namespace dashiell
: 1503413856:0;kubectl --namespace dashiell logs dashiell-web-4051950249-pd7wv
: 1503414048:0;git checkout README.md
: 1503414050:0;git diff 
: 1503414897:0;homebrew list kubernetes
: 1503414903:0;homebrew list kops
: 1503414909:0;homebrew list | grep k
: 1503414916:0;homebrew list
: 1503414926:0;brew list kubernetes
: 1503414941:0;brew list kops
: 1503414950:0;brew upgrade kops
: 1503414984:0;kubectl version
: 1503414989:0;helm version
: 1503415145:0;kops version
: 1503416856:0;ssh ubuntu@172.16.1.117
: 1503416968:0;kubectl get pods -o wide  --all-namespaces | rgep wellbeingtracker
: 1503416984:0;nslookup wellbeingtracker.myhstg.com/
: 1503416986:0;nslookup wellbeingtracker.myhstg.com
: 1503417377:0;kubectl delete pod dc-production-jobs-27998188-832p6 --namespace dc-production
: 1503417876:0;kubectl exec wbt-staging-web-171931494-w57hb -i -t nslookup wellbeingtracker.myhstg.com --namespace wbt-staging
: 1503417886:0;DEIS_PROFILE=staging deis releases -a wbt-staging
: 1503418008:0;kubectl --namespace wbt-staging logs wbt-staging-web-171931494-d236b
: 1503418017:0;DEIS_PROFILE=staging deis ps:list -a wbt-staging
: 1503418031:0;kubectl --namespace wbt-staging logs wbt-staging-web-171931494-w57hb
: 1503420821:0;kubectl --namespace wbt-staging logs wbt-staging-web-171931494-w57hb --timestamp
: 1503420823:0;kubectl --namespace wbt-staging logs wbt-staging-web-171931494-w57hb --timestamps
: 1503421271:0;cd skylight
: 1503421288:0;grep -i haml Gemfile
: 1503421348:0;more Gemfile.lock
: 1503421356:0;vi Gemfile.lock
: 1503421372:0;more config.ru
: 1503421383:0;cd config/environments
: 1503421455:0;more staging.rb
: 1503421888:0;kubectl exec wbt-staging-web-171931494-w57hb  -i -t bash  --namespace wbt-staging
: 1503423022:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-208-72.ec2.internal
: 1503423064:0;ssh admin@ip-172-24-208-72.ec2.internal
: 1503423158:0;kubectl delete pod dc-production-web-90425554-v3m8p --namespace dc-production
: 1503423991:0;kubectl get pods -o wide  --all-namespacess
: 1503424004:0;kops get instancegroups nodes
: 1503424008:0;kops get instancegroups
: 1503424100:0;kops get cluster neywk.k8s.meyouhealth.com
: 1503424105:0;kops get cluster neywk.k8s.meyouhealth.com -o yaml
: 1503424127:0;kops get cluster neywk.k8s.meyouhealth.com -o yaml > Documents/cluster-neywk.k8s.meyouhealth.com.b4.upgrade.yaml
: 1503424138:0;kops edit cluster neywk.k8s.meyouhealth.com
: 1503424192:0;kops update cluster neywk.k8s.meyouhealth.com
: 1503424426:0;kops update cluster neywk.k8s.meyouhealth.com --yes
: 1503424465:0;kops rolling-update cluster
: 1503424517:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-143-203.ec2.internal
: 1503424709:0;ssh admin@ip-172-24-157-126.ec2.internal
: 1503424935:0;kubectl describe node ip-172-24-157-126.ec2.internal
: 1503424950:0;kubectl describe node ip-172-24-144-234.ec2.internal
: 1503424983:0;ssh admin@ip-172-24-144-234.ec2.internal
: 1503425076:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-144-234
: 1503425196:0;kubectl exec dc-production-web-90425554-jkx6l  -i -t bash  --namespace dc-production
: 1503425230:0;kubectl describe node ip-172-24-151-149.ec2.internal
: 1503425440:0;ssh admin@ip-172-24-146-120.ec2.internal
: 1503425471:0;cd
: 1503425503:0;ssh admin@ip-172-24-151-149.ec2.internal sudo systemctl restart kubelet.service
: 1503425523:0;ssh admin@ip-172-24-151-149.ec2.internal sudo systemctl status kubelet.service
: 1503425646:0;kubectl delete pod prometheus-1575380483-h29dz --namespace prometheus
: 1503425685:0;kubectl delete pod kube-dns-823478599-vk1dx --namespace kube-system
: 1503425737:0;kubectl --namespace prometheus logs prometheus-1575380483-v6m5n
: 1503425780:0;kubectl --namespace kube-system logs kube-dns-823478599-5zfj1
: 1503425789:0;kubectl --namespace kube-system logs kube-dns-823478599-5zfj1 kubedns
: 1503425796:0;kubectl --namespace kube-system logs kube-dns-823478599-5zfj1 dnsmasq
: 1503425802:0;kubectl --namespace kube-system logs kube-dns-823478599-5zfj1 sidecar
: 1503425819:0;kubectl describe pod kube-dns-823478599-5zfj1
: 1503425878:0;kubectl get nodes -o wide | grep ip-172-24-214-100.ec2.internal
: 1503425946:0;watch "kubectl get pods -o wide  --all-namespaces | grep -v Running"
: 1503425972:0;kubectl describe pod kube-dns-823478599-5zfj1 --namespace kube-system
: 1503426046:0;ssh admin@ip-172-24-214-100.ec2.internal
: 1503426476:0;kubectl get nodes -o wide | grep ip-172-24-171-84.ec2.internal
: 1503426609:0;kops rolling-update --help
: 1503426711:0;ssh admin@ip-172-24-184-15.ec2.internal
: 1503426756:0;kubectl get nodes -o wide | grep ip-172-24-184-15
: 1503426771:0;kubectl get nodes -o wide | grep NotReady
: 1503426795:0;kubectl get nodes -o wide | grep NotReady | tr -s " "
: 1503426808:0;kubectl get nodes -o wide | grep NotReady | tr -s " " | cut -f 2 -d " "
: 1503426872:0;kubectl get nodes -o wide | grep NotReady | tr -s " " | cut -f 1 -d " " | xargs -n 1 -I {} ssh admin@{} sudo systemctl restart kubelet.service 
: 1503426890:0;kubectl get nodes -o wide | grep NotReady | tr -s " " | cut -f 1 -d " "
: 1503426989:0;kubectl get nodes -o wide | grep p-172-24-211-38.ec2.internal
: 1503427105:0;ssh admin@ip-172-24-208-56.ec2.internal
: 1503427267:0;date
: 1503427397:0;kubectl get nodes -o wide | grep NotReady | tr -s " " | cut -f 1 -d " " | xargs -n 1 -I {} ssh admin@{} sudo systemctl restart kubelet.service
: 1503427502:0;kubectl describe node ip-172-24-208-56.ec2.internal
: 1503427598:0;kubectl get pods -o wide  --all-namespaces | grep dc-production-ass
: 1503427678:0;kubectl get pods -o wide  --all-namespaces | grep promethe
: 1503428350:0;kubectl delete pod concierge-production-web-1178603087-6k09r --namespace concierge-production
: 1503428354:0;kubectl --namespace concierge-production logs concierge-production-web-1178603087-6k09r
: 1503428379:0;kubectl --namespace concierge-production logs concierge-production-web-1178603087-rz180
: 1503428702:0;ssh admin@ip-172-24-223-31.ec2.internal 
: 1503428951:0;kubectl get nodes -o wide | grep ip-172-24-208-56.ec2.internal
: 1503429405:0;kubectl get pods -o wide  --all-namespaces | grep concierge
: 1503429422:0;kubectl get pods -o wide  --namespace concierge-production
: 1503429858:0;kubectl describe node ip-172-24-223-31.ec2.internal
: 1503430158:0;kubectl get nodes -o wide
: 1503430485:0;kubectl --namespace kube-system logs kube-controller-manager-ip-172-24-202-82.ec2.internal
: 1503430504:0;kubectl --namespace kube-system logs kube-controller-manager-ip-172-24-157-126.ec2.internal
: 1503430519:0;kubectl --namespace deis logs deis-controller-2170126687-245ps
: 1503430540:0;kubectl get pods -o wide  --all-namespaces | grep controller
: 1503430572:0;kubectl --namespace kube-system log calico-policy-controller-3799769598-3d1r8
: 1503582336:0;kubectl config use-context lrywh.k8s.myhstg.com
: 1503582513:0;kubectl --namespace deis logs deis-monitor-telegraf-68t33
: 1503582532:0;kubectl delete pod deis-monitor-telegraf-68t33 --namespace deis
: 1503582675:0;nslookup account.myhstg.com
: 1503582739:0;kubectl get pods -o wide  --all-namespaces | grep quitnet
: 1503582762:0;kubectl exec quitnet-staging-web-2568014492-lxm8f -i -t bash --namespace quitnet-staging
: 1503582927:0;kubectl get pods -o wide  --all-namespaces | grep wbt
: 1503583465:0;kubectl get pods -o wide  --all-namespaces
: 1503583548:0;kubectl get pods -o wide  --all-namespaces | grep kube-dns
: 1503583568:0;kubectl -n kube-system describe service kube-dns
: 1503583601:0;kubectl exec wbt-staging-web-1959816823-mkcmw -i -t bash --namespace wbt-staging
: 1503583711:0;kubectl get pods -o wide  --all-namespaces | grep 100.64.0.10
: 1503584549:0;more Documents/cluster-neywk.k8s.meyouhealth.com.b4.upgrade.yaml
: 1503596471:0;kops get cluster neywk.k8s.meyouhealth.com -o json --full | jq .spec.nonMasqueradeCIDR --raw-output
: 1503596503:0;kops get cluster neywk.k8s.meyouhealth.com -o json --full | jq .spec.kubeControllerManager.clusterCIDR --raw-output
: 1503596511:0;kubectl cluster-info  
: 1503596529:0;kops get cluster lrywh.k8s.meyouhealth.com -o json --full | jq .spec.nonMasqueradeCIDR --raw-output
: 1503596551:0;kops get cluster lrywh.k8s.myhstg.com -o json --full | jq .spec.nonMasqueradeCIDR --raw-output
: 1503596575:0;kops get cluster lrywh.k8s.myhstg.com -o json --full | jq .spec.kubeControllerManager.clusterCIDR --raw-output
: 1503596609:0;kops get cluster lrywh.k8s.myhstg.com -o json --full | jq .spec.networking.calico.crossSubnet --raw-output
: 1503596628:0;kops get cluster neywk.k8s.meyouhealth.com -o json --full | jq .spec.networking.calico.crossSubnet --raw-output
: 1503596775:0;kops get cluster neywk.k8s.meyouhealth.com -o json --full
: 1503596868:0;kops get cluster lrywh.k8s.myhstg.com -o json --full 
: 1503597000:0;kops --help
: 1503597397:0;kubectl get nodes -o wide  | grep ip-172-20-162-162
: 1503597403:0;kubectl get nodes -o wide  
: 1503597429:0;kubectl describe node ip-172-20-131-3.ec2.internal
: 1503597467:0;kubectl get nodes -o wide  --show-labels
: 1503597477:0;kubectl get nodes -o wide  --show-labels | grep master
: 1503597488:0;ssh admin@ip-172-20-131-3.ec2.internal
: 1503597916:0;ssh admin@ip-172-20-158-241.ec2.internal
: 1503597978:0;kubectl exec calicoctl  -i -t bash --namespace kube-system
: 1503598043:0;kubectl exec calico-node-w43c4 -i -t bash --namespace kube-system 
: 1503598065:0;kubectl describe pod/calico-node-w43c4 --namespace kube-system
: 1503598404:0;kubectl exec calico-node-w43c4 -i -t /bin/sh --namespace kube-system 
: 1503598553:0;kubectl exec calicoctl -i -t /bin/sh --namespace kube-system 
: 1503600026:0;export NAME=lrywh.k8s.myhstg.com
: 1503600028:0;cd Documents
: 1503600045:0;wget https://raw.githubusercontent.com/kubernetes/kops/master/docs/calico_cidr_migration/create_migration_manifest.sh -O create_migration_manifest.sh
: 1503600051:0;more create_migration_manifest.sh
: 1503600077:0;wget https://raw.githubusercontent.com/kubernetes/kops/master/docs/calico_cidr_migration/jobs.yaml.template -O jobs.yaml.template
: 1503600092:0;vi jobs.yaml.template
: 1503600146:0;kubectl proxy 
: 1503604979:0;history | grep aws | grep s3
: 1503605035:0;history | grep pip | grep aws
: 1503665442:0;kubectl config use-context neywk.k8s.meyouhealth.com
: 1503665452:0;kubectl get pods -o wide  --all-namespaces | grep -V Running
: 1503665826:0;kubectl get logs --namespace dc-production dc-production-web-2068920915-q55sb
: 1503665849:0;kubectl --namespace dc-production logs dc-production-web-2068920915-q55sb
: 1503665986:0;kubectl delete pod dc-production-web-2068920915-q55sb
: 1503665994:0;kubectl delete pod dc-production-web-2068920915-q55sb --namespace dc-production
: 1503666078:0;kubectl --namespace dc-production logs dc-production-jobs-3564743888-r0nx7
: 1503666096:0;kubectl delete pod dc-production-jobs-3564743888-r0nx7 --namespace dc-production
: 1503666129:0;kubectl delete pod dc-production-jobs-3564743888-f0lvp --namespace dc-production
: 1503666164:0;kubectl --namespace dc-production logs dc-production-web-2068920915-9nswt
: 1503666173:0;kubectl --namespace dc-production logs dc-production-web-2068920915-9nswt | more
: 1503666383:0;kubectl get pods -o wide  --all-namespaces | grep -v Running
: 1503666390:0;kubectl get pods -o wide  --all-namespaces | grep dc-prod
: 1503666413:0;kubectl get events --all-namespaces
: 1503666488:0;kubectl get events  
: 1503666644:0;kubectl get nodes -o wide | grep ip-172-24-184-15.ec2.internal
: 1503666661:0;kubectl get pods -o wide  --all-namespaces | grep ip-172-24-184-15.ec2.internal
: 1503666712:0;kubectl get events --all-namespaces | grep dc-production-web-2068920915-q55sb
: 1503666812:0;kubectl get pods -o wide  --all-namespaces | grep dc-production-web
: 1503666831:0;kubectl get pods -o wide  --all-namespaces | grep dc-production-web | wc -l 
: 1503666925:0;kubectl get pods -o wide  --all-namespaces | grep dc-production-web | awk '{print $7}'
: 1503666931:0;kubectl get pods -o wide  --all-namespaces | grep dc-production-web | awk '{print $8}'
: 1503666937:0;kubectl get pods -o wide  --all-namespaces | grep dc-production-web | awk '{print $8}' | sort 
: 1503667940:0;more jobs.yaml.template
: 1503668738:0;kubectl get pods -o wide  --all-namespaces | grep calico
: 1503668919:0;DEIS_PROFILE=production kube-system ps:list
: 1503668933:0;DEIS_PROFILE=production deis apps
: 1503671146:0;aws dms help
: 1503671200:0;aws dms  dailychallenge-ods-all
: 1503671220:0;aws dms describe-replication-tasks help 
: 1503671234:0;aws dms describe-replication-tasks dailychallenge-ods-all
: 1503671239:0;aws dms describe-replication-tasks 
: 1503671268:0;aws dms describe-replication-tasks > aws-dms-describe-replication-tasks-all.08252017.txt
: 1503671275:0;vi aws-dms-describe-replication-tasks-all.08252017.txt
: 1503671327:0;aws dms help | grep describe
: 1503671339:0;aws dms describe-replication-instances
: 1503671360:0;aws dms describe-replication-instances > aws-describe-replication-instances.08252017.txt
: 1503671370:0;more aws-describe-replication-instances.08252017.txt
: 1503671563:0;vi aws-describe-replication-instances.08252017.txt
: 1503671816:0;la -lra
: 1503671841:0;cp aws-dms-describe-replication-tasks-all.08252017.txt dc-ods-task-cp.08252017.txt
: 1503671846:0;vi dc-ods-task-cp.08252017.txt
: 1503674817:0;more dc-ods-task-cp.08252017.txt
: 1503674922:0;vi dc-ods-test-mapping.json
: 1503674997:0;mv dc-ods-test-mapping.json dc-ods-all-mapping.json
: 1503675006:0;cat dc-ods-all-mapping.json
: 1503691452:0;brew install letsencrypt
: 1504031348:0;git clone git@github.com:meyouhealth/dwh.git
: 1504031365:0;cd dwh
: 1504034430:0;mkdir infra
: 1504034433:0;cd infra 
: 1504034437:0;mkdir production
: 1504034464:0;cp ../dailychallenge/infra/README.md .
: 1504034468:0;vi README.md
: 1504034585:0;mv README.md infra/
: 1504034601:0;git add infra
: 1504034640:0;git diff --staged
: 1504034653:0;git add production
: 1504034815:0;terraform init 
: 1504034824:0;ls -ltar
: 1504034870:0;cd ~/Documents/git
: 1504034872:0;cd dailychallenge
: 1504034892:0;git pull origin master
: 1504034906:0;git branch
: 1504034922:0;git branch master
: 1504034929:0;git checkout master
: 1504034945:0;git pulll
: 1504034947:0;git pull
: 1504034952:0;cd infra
: 1504034952:0;ls 
: 1504034962:0;more mysql.tf
: 1504035048:0;ls -ltra *var*
: 1504035055:0;ls -ltra .??*
: 1504035079:0;more providers.tf
: 1504035110:0;cp ~/Documents/git/dailychallenge/infra/production/providers.tf providers.tf
: 1504035115:0;vi providers.tf
: 1504035166:0;env | grep AWS
: 1504035203:0;terraform init
: 1504035218:0;terraform plan -out=test.plan
: 1504035226:0;more test.plan
: 1504035239:0;more dns.tf
: 1504035260:0;cd ../
: 1504035270:0;git add production/providers.tf
: 1504035291:0;more production/.terraform
: 1504035302:0;ls -oltra production/.terraform
: 1504035306:0;ls -oltra production/.terraform/plugins
: 1504035311:0;ls -oltra production/.terraform/plugins/darwin_amd64
: 1504035328:0;git status
: 1504035332:0;git commit
: 1504035417:0;git push origin master
: 1504035425:0;cd production
: 1504035427:0;ls -ltra
: 1504060243:0;vi endpoints.tf
: 1504060578:0;mv endpoints.tf analysis-datawarehouse-dms-endpoints.tf
: 1504060583:0;cat analysis-datawarehouse-dms-endpoints.tf
: 1504060619:0;mv analysis-datawarehouse-dms-endpoints.tf analysis-warehouse-production-dms-endpoint.tf
: 1504060628:0;vi analysis-warehouse-production-dms-endpoint.tf
: 1504060675:0;terraform import aws_dms_endpoint.analysis-warehouse-production analysis-warehouse-production-dms-endpoint.tf
: 1504060722:0;vi analysis-dw-prod.tf
: 1504060739:0;terraform import aws_dms_endpoint.analysis-warehouse-production analysis-dw-prod.tf
: 1504060772:0;aws dms describe-endpoints
: 1504060807:0;ls -lra
: 1504060813:0;rm analysis-dw-prod.tf
: 1504060832:0;mv analysis-warehouse-production-dms-endpoint.tf analysis-dw-prod-endpoint.tf
: 1504061181:0;terraform import aws_dms_endpoint.analysis-warehouse-production analysis-dw-prod-endpoint.tf
: 1504061193:0;vi analysis-dw-prod-endpoint.tf
: 1504061253:0;cat analysis-dw-prod-endpoint.tf
: 1504061260:0;mv analysis-dw-prod-endpoint.tf analysis_warehouse_production_endpoint.tf
: 1504061268:0;terraform import aws_dms_endpoint.analysis-warehouse-production analysis_warehouse_production_endpoint.tf
: 1504061297:0;vi analysis_warehouse_production_endpoint.tf
: 1504061358:0;mv analysis_warehouse_production_endpoint.tf analysis-warehouse-production-endpoint.tf
: 1504061378:0;terraform import aws_dms_endpoint.analysis_warehouse_production analysis-warehouse-production-endpoint.tf
: 1504061393:0;mv analysis-warehouse-production-endpoint.tf endpoint.tf
: 1504061433:0;terraform import aws_dms_endpoint.analysis_warehouse_production endpoint.tf
: 1504061447:0;vi endpoint.tf
: 1504061491:0;cat endpoint.tf
: 1504061498:0;mv endpoint.tf anal-dw-prod-endpoint.tf
: 1504061510:0;terraform import aws_dms_endpoint.datawarehouse anal-dw-prod-endpoint.tf
: 1504061534:0;vi anal-dw-prod-endpoint.tf
: 1504061560:0;cat anal-dw-prod-endpoint.tf
: 1504061570:0;terraform import aws_dms_endpoint.datawarehouse anal-dw-prod-endpoint.tf
: 1504061582:0;vi anal-dw-prod-endpoint.tf
: 1504061589:0;terraform import aws_dms_endpoint.datawarehouse anal-dw-prod-endpoint.tf
: 1504061649:0;mv anal-dw-prod-endpoint.tf AnalysisWarehouseProductionEndpoint.tf
: 1504061660:0;terraform import AnalysisWarehouseProductionEndpoint.tf
: 1504061680:0;terraform import aws_dms_endpoint.datawarehouse AnalysisWarehouseProductionEndpoint.tf
: 1504061721:0;mv AnalysisWarehouseProductionEndpoint.tf datawarehouse.tf
: 1504061747:0;mv datawarehouse.tf datawarehouse-endpoint.tf
: 1504061792:0;terraform import aws_dms_endpoint.datawarehouse datawarehouse-endpoint
: 1504061844:0;vi datawarehouse-endpoint.tf
: 1504061893:0;terraform import aws_dms_endpoint.datawarehouse analysis-warehouse-production
: 1504061929:0;kubectl config use-context neywk.k8s.meyouhealth.com
: 1504061935:0;kubectl get pods -o wide  --all-namespaces
: 1504061957:0;kubectl get nodes -o wide 
: 1504061976:0;kubectl --namespace dc-production logs dc-production-conciergelistener-1079886444-d8vjn
: 1504061982:0;kubectl --namespace dc-production logs dc-production-conciergelistener-1079886444-d8vjn --previous
: 1504062013:0;kubectl --namespace dc-production logs dc-production-clockwork-1374139732-dqnsm
: 1504062023:0;kubectl --namespace dc-production logs dc-production-conciergelistener-1079886444-d8vjn
: 1504062033:0;kubectl get nodes -o wide 
: 1504062047:0;kops get cluster neywk.k8s.meyouhealth.com -o json --full
: 1504062070:0;kops edit cluster neywk.k8s.meyouhealth.com
: 1504062090:0;history | grep kops
: 1504062131:0;kops get cluster neywk.k8s.meyouhealth.com -o yaml > Documents/cluster-neywk.k8s.meyouhealth.com.1.6.2.after.upgrade.yaml
: 1504062144:0;diff Documents/cluster-neywk.k8s.meyouhealth.com.1.6.2.after.upgrade.yaml Documents/cluster-neywk.k8s.meyouhealth.com.b4.upgrade.yaml
: 1504062210:0;kops get cluster lrywh.k8s.myhstg.com -o yaml > Documents/cluster-lrywh.k8s.myhstg.com.after.upgrade.1.7.yaml
: 1504062222:0;diff Documents/cluster-lrywh.k8s.myhstg.com.after.upgrade.1.7.yaml Documents/cluster-neywk.k8s.meyouhealth.com.1.6.2.after.upgrade.yaml
: 1504063331:0;cd Documents
: 1504063363:0;kubectl get pods -o wide  --all-namespaces > pods-b4-upgrade-k8s-1.7.4-all.log
: 1504063369:0;kubectl cluster-info  
: 1504063384:0;kubectl get nodes -o wide > nodes-b4-upgrade-k8s-1.7.4-all.log
: 1504063388:0;ls -ltra
: 1504066020:0;kubectl get pods -o wide  --all-namespaces | grep jobs
: 1504066034:0;more jobs.yaml.template
: 1504066053:0;kubectl get pods -o wide  --namespace kube-system
: 1504066087:0;kubectl --namespace kube-system logs kube-scheduler-ip-172-24-157-126.ec2.internal
: 1504066098:0;kubectl get pods -o wide  --namespace kube-system
: 1504066163:0;kops get cluster neywk.k8s.meyouhealth.com -o yaml > Documents/cluster-neywk.k8s.meyouhealth.com.1.7.4.upgrade.yaml
: 1504066168:0;cd ../
: 1504066171:0;kops get cluster neywk.k8s.meyouhealth.com -o yaml > Documents/cluster-neywk.k8s.meyouhealth.com.1.7.4.upgrade.yaml
: 1504066179:0;more Documents/cluster-neywk.k8s.meyouhealth.com.1.7.4.upgrade.yaml
: 1504066209:0;kubectl get pods -o wide  --namespace kube-system
: 1504066264:0;watch 'kubectl get pods -o wide  --namespace kube-system'
: 1504066268:0;which watch
: 1504066277:0;brew install watch
: 1504066299:0;which watch
: 1504066303:0;watch 'kubectl get pods -o wide  --namespace kube-system'
: 1504066341:0;kubectl get nodes -o wide 
: 1504066356:0;kubectl get nodes -o wide --show-labels | grep -i master
: 1504066605:0;date
: 1504066642:0;kubectl get nodes -o wide --show-labels | grep -i master
: 1504066678:0;kubectl get pods -o wide  --namespace kube-system
: 1504066701:0;kubectl get nodes -o wide --show-labels | grep -i ip-172-24-133-65.ec2.internal
: 1504066875:0;kubectl get pods -o wide  --all-namespaces | grep -v Running
: 1504066905:0;kubectl --namespace deis logs deis-builder-2994006370-sv6c3
: 1504066913:0;kubectl --namespace deis logs deis-builder-2994006370-sv6c3 --previous
: 1504066917:0;kubectl get pods -o wide  --all-namespaces | grep -v Running
: 1504066928:0;kubectl get events --all-namespaces
: 1504066930:0;kubectl get events 
: 1504066960:0;kubectl get pods -o wide  --all-namespaces | grep -v Running
: 1504066970:0;kubectl get nodes -o wide --show-labels | grep -i ip-172-24-133-65.ec2.internal
: 1504067139:0;kubectl --namespace deis logs deis-builder-2994006370-sv6c3
: 1504067177:0;kubectl get pods -o wide  --namespace kube-system
: 1504067276:0;kubectl get nodes -o wide 
: 1504067295:0;kubectl get pods -o wide  --all-namespaces --show-labels
: 1504067328:0;kubectl get nodes -o wide --show-labels | grep -i master
: 1504067361:0;kubectl get pods -o wide  --all-namespaces | grep -v Running
: 1504067411:0;kubectl get nodes -o wide --show-labels | grep -i master
: 1504067432:0;date
: 1504067438:0;kubectl get nodes -o wide --show-labels | grep -i master
: 1504067734:0;kubectl get pods -o wide  --all-namespaces | grep -v Running
: 1504067924:0;date
: 1504092756:0;kubectl get pods -o wide  --all-namespaces | grep -v Running
: 1504092808:0;kubectl get events --all-namespaces
: 1504092809:0;kubectl get events 
: 1504099994:0;cat datawarehouse-endpoint.tf
: 1504100949:0;cat providers.tf
: 1504101147:0;terraform plan -out=test.plan
: 1504101160:0;cat datawarehouse-endpoint.tf
: 1504101170:0;vi datawarehouse-endpoint.tf
: 1504101192:0;terraform plan -out=test.plan
: 1504101235:0;more terraform.tfstate
: 1504101508:0;terraform apply test.plan
: 1504102306:0;ls -ltra
: 1504102333:0;aws dms describe-endpoints > all-endpoints
: 1504102338:0;more all-endpoints
: 1504102373:0;more memcached.tf
: 1504102386:0;cat all-endpoints >> datawarehouse-endpoint.tf
: 1504102398:0;mv datawarehouse-endpoint.tf dms-endpoints.tf
: 1504102403:0;vi dms-endpoints.tf
: 1504103333:0;terraform plan -out=test.plan
: 1504103344:0;ls -ltra
: 1504103363:0;mv all-endpoints /tmp/
: 1504103367:0;terraform plan -out=test.plan
: 1504103374:0;rm terraform.tfstate
: 1504103375:0;ls -ltra
: 1504103380:0;rm test.plan
: 1504103390:0;terraform init
: 1504103405:0;terraform import aws_dms_endpoint.datawarehouse analysis-warehouse-production
: 1504103416:0;more dms-endpoints.tf
: 1504103423:0;rm terraform.tfstate
: 1504103427:0;vi dms-endpoints.tf
: 1504103455:0;terraform import aws_dms_endpoint.datawarehouse analysis-warehouse-production
: 1504103479:0;more dms-endpoints.tf
: 1504103499:0;terraform import aws_dms_endpoint.dailychallenge mysql-dc-production3
: 1504103504:0;terraform plan -out=test.plan
: 1504103517:0;terraform apply test.plan
: 1504103556:0;kubectl get pods -o wide  --all-namespaces | grep quitnet
: 1504103634:0;kubectl --namespace quitnet-production logs quitnet-production-clockwork-129032744-8f982
: 1504103665:0;kubectl exec quitnet-production-clockwork-129032744-8f982 -i -t /bin/sh --namespace quitnet-production 
: 1504103819:0;kubectl exec quitnet-production-clockwork-129032744-8f982 -i -t /bin/bash --namespace quitnet-production 
: 1504105255:0;ls -ltra
: 1504105261:0;vi dms-endpoints.tf
: 1504105639:0;rm terraform.tfstate
: 1504105641:0;ls -ltra
: 1504105665:0;terraform import aws_dms_endpoint.dailychallenge-production mysql-dc-production3
: 1504105670:0;terraform plan -out=test.plan
: 1504105702:0;terraform import aws_dms_endpoint.datawarehouse-production analysis-warehouse-production
: 1504105724:0;terraform import aws_dms_endpoint.wbt-production-read-replica mysql-wbt-production-read2
: 1504105729:0;terraform plan -out=test.plan
: 1504105752:0;vi dms-endpoints.tf
: 1504105830:0;terraform plan -out=test.plan
: 1504105843:0;terraform apply test.plan
: 1504105848:0;terraform plan -out=test.plan
: 1504105854:0;vi dms-endpoints.tf
: 1504106112:0;terraform plan -out=test.plan
: 1504106150:0;terraform import aws_dms_endpoint.wilder-production mysql-wilder-production2
: 1504106153:0;terraform plan -out=test.plan
: 1504106165:0;terraform apply test.plan
: 1504106169:0;ls -ltra
: 1504106277:0;terraform -V
: 1504106285:0;terraform -version
: 1504106462:0;ls -ltra
: 1504106467:0;vi dms-endpoints.tf
: 1504114195:0;kubectl get pods -o wide  --all-namespaces | grep rout
: 1504115515:0;terraform plan -out=test.plan
: 1504115549:0;terraform import aws_dms_endpoint.concierge-production pg-concierge-production
: 1504115555:0;terraform plan -out=test.plan
: 1504115581:0;terraform import aws_dms_endpoint.insight-production pg-insight-production
: 1504115599:0;terraform import aws_dms_endpoint.iris-production-research pg-iris-research
: 1504115604:0;terraform plan -out=test.plan
: 1504115632:0;vi dms-endpoints.tf
: 1504115659:0;terraform plan -out=test.plan
: 1504115672:0;terraform apply test.plan 
: 1504115679:0;ls -ltra
: 1504115685:0;more dms-endpoints.tf
: 1504115771:0;vi dms-endpoints.tf
: 1504116021:0;terraform plan -out=test.plan
: 1504116039:0;terraform import aws_dms_endpoint.quitnet-production pg-quitnet-production
: 1504116043:0;terraform plan -out=test.plan
: 1504116065:0;vi dms-endpoints.tf
: 1504116128:0;terraform plan -out=test.plan
: 1504116147:0;vi terraform.tfstate
: 1504116209:0;terraform plan -out=test.plan
: 1504116228:0;terraform import aws_dms_endpoint.quitnet-production-research pg-quitnet-production
: 1504116232:0;terraform plan -out=test.plan
: 1504116242:0;terraform apply test.plan
: 1504116898:0;vi dms-endpoints.tf
: 1504119508:0;terraform plan -out=test.plan
: 1504119516:0;vi dms-endpoints.tf
: 1504119558:0;terraform plan -out=test.plan
: 1504119631:0;terraform import aws_dms_endpoint.wellbeingid-production pg-wbid-migration
: 1504119651:0;cp dms-endpoints.tf keep-dms-endpoints.tf
: 1504119653:0;ls -ltra
: 1504119663:0;mv keep-dms-endpoints.tf keep-dms-endpoints
: 1504119667:0;vi dms-endpoints.tf
: 1504119704:0;terraform plan -out=test.plan
: 1504119714:0;terraform apply test.plan
: 1504119717:0;ls -ltra
: 1504119721:0;git status
: 1504119734:0;ls -l ../../
: 1504119745:0;git add dms-endpoints.tf
: 1504119749:0;git add terraform.tfstate
: 1504119751:0;git status
: 1504119753:0;git pull
: 1504119770:0;git commit
: 1504119801:0;git push origin master
: 1504119805:0;ls -ltra
: 1504119812:0;vi keep-dms-endpoints
: 1504207511:0;git pull
: 1504209234:0;cd ~/Documents/git
: 1504209239:0;cd dwh
: 1504209242:0;git pull
: 1504209252:0;more .ssh
: 1504209255:0;cd .ssh
: 1504209256:0;ls -ltra
: 1504209262:0;ls -l *git*
: 1504209878:0;more github_rsa
: 1504209884:0;more github_rsa.pub
: 1504210433:0;ls -ltra
: 1504210827:0;cd
: 1504211056:0;cd ~/Documents/git
: 1504211089:0;git clone git@github.com:heschmidt04/config.git
: 1504211095:0;cd
: 1504211098:0;ls .??*
: 1504211108:0;ls -ltra .zs*
: 1504211126:0;mkdir Documents/git/config/zsh
: 1504211139:0;cp .zsh* Documents/git/config/zsh/
